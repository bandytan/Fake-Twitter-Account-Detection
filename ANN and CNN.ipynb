{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-11-14 18:01:18.326660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/twitter_data_train_multiclass.csv')\n",
    "test_df = pd.read_csv('data/twitter_data_test_multiclass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() \n",
    "train_df[['statuses_count', 'favourites_count', 'followers_count', 'friends_count', 'number_of_mentions', 'listed_count', 'number_of_tags']] = scaler.fit_transform(train_df[['statuses_count', 'favourites_count', 'followers_count', 'friends_count', 'number_of_mentions', 'listed_count', 'number_of_tags']])\n",
    "test_df[['statuses_count', 'favourites_count', 'followers_count', 'friends_count', 'number_of_mentions', 'listed_count', 'number_of_tags']] = scaler.transform(test_df[['statuses_count', 'favourites_count', 'followers_count', 'friends_count', 'number_of_mentions', 'listed_count', 'number_of_tags']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Unnamed: 0', 'screen_name', 'url', 'profile_image_url', 'description',\n",
    "           'id', 'name', 'account_type', 'tweets_list', 'tweets_list_processed',\n",
    "          'description_processed', 'protected', 'verified', 'account_type_multi', \n",
    "          'profile_use_background_image', 'profile_background_tile']\n",
    "\n",
    "x_train, y_train = train_df.drop(to_drop, axis=1), train_df['account_type_multi']\n",
    "x_test, y_test = test_df.drop(to_drop, axis=1), test_df['account_type_multi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.drop(x_train.iloc[:, 8:20].columns, axis=1)\n",
    "x_test = x_test.drop(x_test.iloc[:, 8:20].columns, axis=1)\n",
    "x_train = x_train.drop(x_train.iloc[:, -2:].columns , axis=1)\n",
    "x_test = x_test.drop(x_test.iloc[:, -2:].columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   statuses_count  followers_count  friends_count  favourites_count  \\\n",
       "0        0.000240         0.000019       0.000429          0.000049   \n",
       "1        0.000052         0.000007       0.002113          0.000000   \n",
       "2        0.000060         0.000011       0.002652          0.000000   \n",
       "3        0.000128         0.000006       0.000451          0.000000   \n",
       "4        0.000093         0.000004       0.000396          0.000000   \n",
       "\n",
       "   listed_count  default_profile  default_profile_image  geo_enabled  \\\n",
       "0           0.0              0.0                    0.0          0.0   \n",
       "1           0.0              1.0                    0.0          0.0   \n",
       "2           0.0              1.0                    0.0          0.0   \n",
       "3           0.0              0.0                    0.0          0.0   \n",
       "4           0.0              0.0                    0.0          0.0   \n",
       "\n",
       "   tweets_0  tweets_1  tweets_2  tweets_3  tweets_4  tweets_5  tweets_6  \\\n",
       "0  0.091977  0.034554  0.000000  0.059238  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.213376  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.326747  0.000000  0.326068   \n",
       "3  0.000000  0.051097  0.099407  0.087598  0.000000  0.192483  0.049504   \n",
       "4  0.000000  0.135142  0.000000  0.000000  0.065601  0.254544  0.000000   \n",
       "\n",
       "   tweets_7  tweets_8  tweets_9  tweets_10  tweets_11  tweets_12  tweets_13  \\\n",
       "0  0.056452  0.048987  0.058207    0.11607   0.125471        0.0   0.000000   \n",
       "1  0.000000  0.000000  0.000000    0.00000   0.000000        0.0   0.000000   \n",
       "2  0.000000  0.000000  0.000000    0.00000   0.152764        0.0   0.000000   \n",
       "3  0.083478  0.000000  0.000000    0.00000   0.092771        0.0   0.157222   \n",
       "4  0.000000  0.000000  0.000000    0.00000   0.000000        0.0   0.138609   \n",
       "\n",
       "   tweets_14  tweets_15  tweets_16  tweets_17  tweets_18  tweets_19  \\\n",
       "0   0.151018   0.000000   0.000000   0.058498    0.15312   0.118353   \n",
       "1   0.000000   0.000000   0.193488   0.000000    0.00000   0.000000   \n",
       "2   0.367734   0.000000   0.296292   0.000000    0.00000   0.000000   \n",
       "3   0.000000   0.044817   0.000000   0.000000    0.00000   0.000000   \n",
       "4   0.000000   0.118533   0.118974   0.000000    0.00000   0.000000   \n",
       "\n",
       "   tweets_20  tweets_21  tweets_22  tweets_23  tweets_24  tweets_25  \\\n",
       "0   0.439767   0.000000   0.200249        0.0   0.000000   0.178702   \n",
       "1   0.000000   0.000000   0.212284        0.0   0.000000   0.000000   \n",
       "2   0.000000   0.000000   0.325076        0.0   0.000000   0.000000   \n",
       "3   0.000000   0.180025   0.000000        0.0   0.134194   0.000000   \n",
       "4   0.000000   0.119035   0.000000        0.0   0.118307   0.000000   \n",
       "\n",
       "   tweets_26  tweets_27  tweets_28  tweets_29  tweets_30  tweets_31  \\\n",
       "0   0.030877   0.000000   0.000000   0.000000   0.000000        0.0   \n",
       "1   0.000000   0.000000   0.000000   0.213317   0.000000        0.0   \n",
       "2   0.000000   0.165395   0.151137   0.163328   0.165001        0.0   \n",
       "3   0.273960   0.150662   0.183566   0.148780   0.000000        0.0   \n",
       "4   0.241527   0.132826   0.121376   0.000000   0.000000        0.0   \n",
       "\n",
       "   tweets_32  tweets_33  tweets_34  tweets_35  tweets_36  tweets_37  \\\n",
       "0   0.030396        0.0        0.0   0.386811   0.028850        0.0   \n",
       "1   0.000000        0.0        0.0   0.000000   0.183501        0.0   \n",
       "2   0.000000        0.0        0.0   0.000000   0.000000        0.0   \n",
       "3   0.134846        0.0        0.0   0.081714   0.255969        0.0   \n",
       "4   0.000000        0.0        0.0   0.000000   0.338499        0.0   \n",
       "\n",
       "   tweets_38  tweets_39  tweets_40  tweets_41  tweets_42  tweets_43  \\\n",
       "0   0.111074   0.000000   0.069019   0.071754   0.000000   0.000000   \n",
       "1   0.000000   0.000000   0.658502   0.000000   0.000000   0.199179   \n",
       "2   0.000000   0.155325   0.000000   0.000000   0.000000   0.000000   \n",
       "3   0.000000   0.047163   0.000000   0.159160   0.000000   0.185226   \n",
       "4   0.000000   0.062370   0.000000   0.000000   0.091928   0.367420   \n",
       "\n",
       "   tweets_44  tweets_45  tweets_46  tweets_47  tweets_48  tweets_49  \\\n",
       "0   0.000000        0.0   0.000000   0.000000   0.000000        0.0   \n",
       "1   0.187112        0.0   0.000000   0.000000   0.187065        0.0   \n",
       "2   0.000000        0.0   0.000000   0.000000   0.000000        0.0   \n",
       "3   0.130503        0.0   0.146159   0.054831   0.130471        0.0   \n",
       "4   0.115053        0.0   0.193284   0.072510   0.172537        0.0   \n",
       "\n",
       "   tweets_50  tweets_51  tweets_52  tweets_53  tweets_54  tweets_55  \\\n",
       "0   0.031558   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "1   0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
       "2   0.000000   0.000000   0.000000   0.000000        0.0   0.155387   \n",
       "3   0.186667   0.196652   0.064543   0.132027        0.0   0.000000   \n",
       "4   0.123426   0.000000   0.000000   0.000000        0.0   0.062394   \n",
       "\n",
       "   tweets_56  tweets_57  tweets_58  tweets_59  tweets_60  tweets_61  \\\n",
       "0   0.063091   0.000000   0.000000   0.161387   0.059978   0.000000   \n",
       "1   0.000000   0.000000   0.000000   0.000000   0.190748   0.000000   \n",
       "2   0.000000   0.319592   0.000000   0.000000   0.146048   0.000000   \n",
       "3   0.093296   0.097041   0.049041   0.000000   0.088693   0.093235   \n",
       "4   0.185066   0.256659   0.000000   0.000000   0.000000   0.123296   \n",
       "\n",
       "   tweets_62  tweets_63  tweets_64  tweets_65  tweets_66  tweets_67  \\\n",
       "0   0.294536        0.0        0.0        0.0        0.0        0.0   \n",
       "1   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "2   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "3   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "4   0.000000        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "   tweets_68  tweets_69  tweets_70  tweets_71  tweets_72  tweets_73  \\\n",
       "0   0.000000   0.000000   0.000000   0.083316   0.000000        0.0   \n",
       "1   0.000000   0.000000   0.222132   0.000000   0.212284        0.0   \n",
       "2   0.000000   0.000000   0.170078   0.000000   0.162538        0.0   \n",
       "3   0.052269   0.160015   0.051643   0.000000   0.246766        0.0   \n",
       "4   0.069122   0.070536   0.000000   0.000000   0.130532        0.0   \n",
       "\n",
       "   tweets_74  tweets_75  tweets_76  tweets_77  tweets_78  tweets_79  \\\n",
       "0   0.000000   0.136500        0.0   0.000000   0.114215   0.374189   \n",
       "1   0.000000   0.289408        0.0   0.000000   0.000000   0.000000   \n",
       "2   0.000000   0.000000        0.0   0.000000   0.000000   0.000000   \n",
       "3   0.080587   0.000000        0.0   0.000000   0.000000   0.000000   \n",
       "4   0.106569   0.000000        0.0   0.071715   0.000000   0.000000   \n",
       "\n",
       "   tweets_80  tweets_81  tweets_82  tweets_83  tweets_84  tweets_85  \\\n",
       "0   0.000000        0.0   0.000000   0.000000        0.0   0.029388   \n",
       "1   0.000000        0.0   0.000000   0.000000        0.0   0.186925   \n",
       "2   0.000000        0.0   0.161666   0.161688        0.0   0.143121   \n",
       "3   0.203775        0.0   0.294531   0.049095        0.0   0.086915   \n",
       "4   0.134738        0.0   0.129831   0.000000        0.0   0.114938   \n",
       "\n",
       "   tweets_86  tweets_87  tweets_88  tweets_89  tweets_90  tweets_91  \\\n",
       "0   0.000000   0.177164   0.180592   0.000000   0.267742        0.0   \n",
       "1   0.000000   0.000000   0.000000   0.000000   0.000000        0.0   \n",
       "2   0.000000   0.000000   0.175899   0.000000   0.000000        0.0   \n",
       "3   0.202677   0.000000   0.053410   0.135054   0.065987        0.0   \n",
       "4   0.201018   0.000000   0.000000   0.059533   0.000000        0.0   \n",
       "\n",
       "   tweets_92  tweets_93  tweets_94  tweets_95  tweets_96  tweets_97  \\\n",
       "0   0.091636   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "1   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "2   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3   0.000000   0.046821   0.188003   0.000000   0.000000   0.000000   \n",
       "4   0.000000   0.123833   0.248619   0.067321   0.137761   0.068264   \n",
       "\n",
       "   tweets_98  tweets_99  description_0  description_1  description_2  \\\n",
       "0   0.000000   0.000000            0.0            0.0            0.0   \n",
       "1   0.000000   0.000000            0.0            0.0            0.0   \n",
       "2   0.000000   0.000000            0.0            0.0            0.0   \n",
       "3   0.000000   0.102325            0.0            0.0            0.0   \n",
       "4   0.072532   0.135316            0.0            0.0            0.0   \n",
       "\n",
       "   description_3  description_4  description_5  description_6  description_7  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   description_8  description_9  description_10  description_11  \\\n",
       "0            0.0            0.0             0.0             0.0   \n",
       "1            0.0            0.0             0.0             0.0   \n",
       "2            0.0            0.0             0.0             0.0   \n",
       "3            0.0            0.0             0.0             0.0   \n",
       "4            0.0            0.0             0.0             0.0   \n",
       "\n",
       "   description_12  description_13  description_14  description_15  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_16  description_17  description_18  description_19  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_20  description_21  description_22  description_23  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_24  description_25  description_26  description_27  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_28  description_29  description_30  description_31  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_32  description_33  description_34  description_35  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_36  description_37  description_38  description_39  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_40  description_41  description_42  description_43  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_44  description_45  description_46  description_47  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_48  description_49  description_50  description_51  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_52  description_53  description_54  description_55  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             1.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_56  description_57  description_58  description_59  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_60  description_61  description_62  description_63  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_64  description_65  description_66  description_67  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_68  description_69  description_70  description_71  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_72  description_73  description_74  description_75  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_76  description_77  description_78  description_79  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_80  description_81  description_82  description_83  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_84  description_85  description_86  description_87  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_88  description_89  description_90  description_91  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_92  description_93  description_94  description_95  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   description_96  description_97  description_98  description_99  \n",
       "0             0.0             0.0             0.0             0.0  \n",
       "1             0.0             0.0             0.0             0.0  \n",
       "2             0.0             0.0             0.0             0.0  \n",
       "3             0.0             0.0             0.0             0.0  \n",
       "4             0.0             0.0             0.0             0.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>statuses_count</th>\n      <th>followers_count</th>\n      <th>friends_count</th>\n      <th>favourites_count</th>\n      <th>listed_count</th>\n      <th>default_profile</th>\n      <th>default_profile_image</th>\n      <th>geo_enabled</th>\n      <th>tweets_0</th>\n      <th>tweets_1</th>\n      <th>tweets_2</th>\n      <th>tweets_3</th>\n      <th>tweets_4</th>\n      <th>tweets_5</th>\n      <th>tweets_6</th>\n      <th>tweets_7</th>\n      <th>tweets_8</th>\n      <th>tweets_9</th>\n      <th>tweets_10</th>\n      <th>tweets_11</th>\n      <th>tweets_12</th>\n      <th>tweets_13</th>\n      <th>tweets_14</th>\n      <th>tweets_15</th>\n      <th>tweets_16</th>\n      <th>tweets_17</th>\n      <th>tweets_18</th>\n      <th>tweets_19</th>\n      <th>tweets_20</th>\n      <th>tweets_21</th>\n      <th>tweets_22</th>\n      <th>tweets_23</th>\n      <th>tweets_24</th>\n      <th>tweets_25</th>\n      <th>tweets_26</th>\n      <th>tweets_27</th>\n      <th>tweets_28</th>\n      <th>tweets_29</th>\n      <th>tweets_30</th>\n      <th>tweets_31</th>\n      <th>tweets_32</th>\n      <th>tweets_33</th>\n      <th>tweets_34</th>\n      <th>tweets_35</th>\n      <th>tweets_36</th>\n      <th>tweets_37</th>\n      <th>tweets_38</th>\n      <th>tweets_39</th>\n      <th>tweets_40</th>\n      <th>tweets_41</th>\n      <th>tweets_42</th>\n      <th>tweets_43</th>\n      <th>tweets_44</th>\n      <th>tweets_45</th>\n      <th>tweets_46</th>\n      <th>tweets_47</th>\n      <th>tweets_48</th>\n      <th>tweets_49</th>\n      <th>tweets_50</th>\n      <th>tweets_51</th>\n      <th>tweets_52</th>\n      <th>tweets_53</th>\n      <th>tweets_54</th>\n      <th>tweets_55</th>\n      <th>tweets_56</th>\n      <th>tweets_57</th>\n      <th>tweets_58</th>\n      <th>tweets_59</th>\n      <th>tweets_60</th>\n      <th>tweets_61</th>\n      <th>tweets_62</th>\n      <th>tweets_63</th>\n      <th>tweets_64</th>\n      <th>tweets_65</th>\n      <th>tweets_66</th>\n      <th>tweets_67</th>\n      <th>tweets_68</th>\n      <th>tweets_69</th>\n      <th>tweets_70</th>\n      <th>tweets_71</th>\n      <th>tweets_72</th>\n      <th>tweets_73</th>\n      <th>tweets_74</th>\n      <th>tweets_75</th>\n      <th>tweets_76</th>\n      <th>tweets_77</th>\n      <th>tweets_78</th>\n      <th>tweets_79</th>\n      <th>tweets_80</th>\n      <th>tweets_81</th>\n      <th>tweets_82</th>\n      <th>tweets_83</th>\n      <th>tweets_84</th>\n      <th>tweets_85</th>\n      <th>tweets_86</th>\n      <th>tweets_87</th>\n      <th>tweets_88</th>\n      <th>tweets_89</th>\n      <th>tweets_90</th>\n      <th>tweets_91</th>\n      <th>tweets_92</th>\n      <th>tweets_93</th>\n      <th>tweets_94</th>\n      <th>tweets_95</th>\n      <th>tweets_96</th>\n      <th>tweets_97</th>\n      <th>tweets_98</th>\n      <th>tweets_99</th>\n      <th>description_0</th>\n      <th>description_1</th>\n      <th>description_2</th>\n      <th>description_3</th>\n      <th>description_4</th>\n      <th>description_5</th>\n      <th>description_6</th>\n      <th>description_7</th>\n      <th>description_8</th>\n      <th>description_9</th>\n      <th>description_10</th>\n      <th>description_11</th>\n      <th>description_12</th>\n      <th>description_13</th>\n      <th>description_14</th>\n      <th>description_15</th>\n      <th>description_16</th>\n      <th>description_17</th>\n      <th>description_18</th>\n      <th>description_19</th>\n      <th>description_20</th>\n      <th>description_21</th>\n      <th>description_22</th>\n      <th>description_23</th>\n      <th>description_24</th>\n      <th>description_25</th>\n      <th>description_26</th>\n      <th>description_27</th>\n      <th>description_28</th>\n      <th>description_29</th>\n      <th>description_30</th>\n      <th>description_31</th>\n      <th>description_32</th>\n      <th>description_33</th>\n      <th>description_34</th>\n      <th>description_35</th>\n      <th>description_36</th>\n      <th>description_37</th>\n      <th>description_38</th>\n      <th>description_39</th>\n      <th>description_40</th>\n      <th>description_41</th>\n      <th>description_42</th>\n      <th>description_43</th>\n      <th>description_44</th>\n      <th>description_45</th>\n      <th>description_46</th>\n      <th>description_47</th>\n      <th>description_48</th>\n      <th>description_49</th>\n      <th>description_50</th>\n      <th>description_51</th>\n      <th>description_52</th>\n      <th>description_53</th>\n      <th>description_54</th>\n      <th>description_55</th>\n      <th>description_56</th>\n      <th>description_57</th>\n      <th>description_58</th>\n      <th>description_59</th>\n      <th>description_60</th>\n      <th>description_61</th>\n      <th>description_62</th>\n      <th>description_63</th>\n      <th>description_64</th>\n      <th>description_65</th>\n      <th>description_66</th>\n      <th>description_67</th>\n      <th>description_68</th>\n      <th>description_69</th>\n      <th>description_70</th>\n      <th>description_71</th>\n      <th>description_72</th>\n      <th>description_73</th>\n      <th>description_74</th>\n      <th>description_75</th>\n      <th>description_76</th>\n      <th>description_77</th>\n      <th>description_78</th>\n      <th>description_79</th>\n      <th>description_80</th>\n      <th>description_81</th>\n      <th>description_82</th>\n      <th>description_83</th>\n      <th>description_84</th>\n      <th>description_85</th>\n      <th>description_86</th>\n      <th>description_87</th>\n      <th>description_88</th>\n      <th>description_89</th>\n      <th>description_90</th>\n      <th>description_91</th>\n      <th>description_92</th>\n      <th>description_93</th>\n      <th>description_94</th>\n      <th>description_95</th>\n      <th>description_96</th>\n      <th>description_97</th>\n      <th>description_98</th>\n      <th>description_99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000240</td>\n      <td>0.000019</td>\n      <td>0.000429</td>\n      <td>0.000049</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.091977</td>\n      <td>0.034554</td>\n      <td>0.000000</td>\n      <td>0.059238</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.056452</td>\n      <td>0.048987</td>\n      <td>0.058207</td>\n      <td>0.11607</td>\n      <td>0.125471</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.151018</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.058498</td>\n      <td>0.15312</td>\n      <td>0.118353</td>\n      <td>0.439767</td>\n      <td>0.000000</td>\n      <td>0.200249</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.178702</td>\n      <td>0.030877</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.030396</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.386811</td>\n      <td>0.028850</td>\n      <td>0.0</td>\n      <td>0.111074</td>\n      <td>0.000000</td>\n      <td>0.069019</td>\n      <td>0.071754</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.031558</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.063091</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.161387</td>\n      <td>0.059978</td>\n      <td>0.000000</td>\n      <td>0.294536</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.083316</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.136500</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.114215</td>\n      <td>0.374189</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.029388</td>\n      <td>0.000000</td>\n      <td>0.177164</td>\n      <td>0.180592</td>\n      <td>0.000000</td>\n      <td>0.267742</td>\n      <td>0.0</td>\n      <td>0.091636</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000052</td>\n      <td>0.000007</td>\n      <td>0.002113</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.213376</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.193488</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.212284</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.213317</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.183501</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.658502</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.199179</td>\n      <td>0.187112</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.187065</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.190748</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.222132</td>\n      <td>0.000000</td>\n      <td>0.212284</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.289408</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.186925</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000060</td>\n      <td>0.000011</td>\n      <td>0.002652</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.326747</td>\n      <td>0.000000</td>\n      <td>0.326068</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.152764</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.367734</td>\n      <td>0.000000</td>\n      <td>0.296292</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.325076</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.165395</td>\n      <td>0.151137</td>\n      <td>0.163328</td>\n      <td>0.165001</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.155325</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.155387</td>\n      <td>0.000000</td>\n      <td>0.319592</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.146048</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.170078</td>\n      <td>0.000000</td>\n      <td>0.162538</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.161666</td>\n      <td>0.161688</td>\n      <td>0.0</td>\n      <td>0.143121</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.175899</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000128</td>\n      <td>0.000006</td>\n      <td>0.000451</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.051097</td>\n      <td>0.099407</td>\n      <td>0.087598</td>\n      <td>0.000000</td>\n      <td>0.192483</td>\n      <td>0.049504</td>\n      <td>0.083478</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.092771</td>\n      <td>0.0</td>\n      <td>0.157222</td>\n      <td>0.000000</td>\n      <td>0.044817</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.180025</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.134194</td>\n      <td>0.000000</td>\n      <td>0.273960</td>\n      <td>0.150662</td>\n      <td>0.183566</td>\n      <td>0.148780</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.134846</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.081714</td>\n      <td>0.255969</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.047163</td>\n      <td>0.000000</td>\n      <td>0.159160</td>\n      <td>0.000000</td>\n      <td>0.185226</td>\n      <td>0.130503</td>\n      <td>0.0</td>\n      <td>0.146159</td>\n      <td>0.054831</td>\n      <td>0.130471</td>\n      <td>0.0</td>\n      <td>0.186667</td>\n      <td>0.196652</td>\n      <td>0.064543</td>\n      <td>0.132027</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.093296</td>\n      <td>0.097041</td>\n      <td>0.049041</td>\n      <td>0.000000</td>\n      <td>0.088693</td>\n      <td>0.093235</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.052269</td>\n      <td>0.160015</td>\n      <td>0.051643</td>\n      <td>0.000000</td>\n      <td>0.246766</td>\n      <td>0.0</td>\n      <td>0.080587</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.203775</td>\n      <td>0.0</td>\n      <td>0.294531</td>\n      <td>0.049095</td>\n      <td>0.0</td>\n      <td>0.086915</td>\n      <td>0.202677</td>\n      <td>0.000000</td>\n      <td>0.053410</td>\n      <td>0.135054</td>\n      <td>0.065987</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.046821</td>\n      <td>0.188003</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.102325</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000093</td>\n      <td>0.000004</td>\n      <td>0.000396</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.135142</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.065601</td>\n      <td>0.254544</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.138609</td>\n      <td>0.000000</td>\n      <td>0.118533</td>\n      <td>0.118974</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.119035</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.118307</td>\n      <td>0.000000</td>\n      <td>0.241527</td>\n      <td>0.132826</td>\n      <td>0.121376</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.338499</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.062370</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.091928</td>\n      <td>0.367420</td>\n      <td>0.115053</td>\n      <td>0.0</td>\n      <td>0.193284</td>\n      <td>0.072510</td>\n      <td>0.172537</td>\n      <td>0.0</td>\n      <td>0.123426</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.062394</td>\n      <td>0.185066</td>\n      <td>0.256659</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.123296</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.069122</td>\n      <td>0.070536</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.130532</td>\n      <td>0.0</td>\n      <td>0.106569</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.071715</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.134738</td>\n      <td>0.0</td>\n      <td>0.129831</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.114938</td>\n      <td>0.201018</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.059533</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.123833</td>\n      <td>0.248619</td>\n      <td>0.067321</td>\n      <td>0.137761</td>\n      <td>0.068264</td>\n      <td>0.072532</td>\n      <td>0.135316</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 64)                13376     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,470\n",
      "Trainable params: 22,470\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ann_model = Sequential()\n",
    "ann_model.add(Dense(64, activation = 'relu', input_dim = 208))\n",
    "ann_model.add(Dropout(.1))\n",
    "ann_model.add(Dense(128, activation='relu'))\n",
    "ann_model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "ann_model.summary()\n",
    "# configure the model\n",
    "ann_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.6408 - accuracy: 0.8210 - val_loss: 0.2898 - val_accuracy: 0.8998\n",
      "Epoch 2/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1964 - accuracy: 0.9331 - val_loss: 0.2154 - val_accuracy: 0.9139\n",
      "Epoch 3/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1598 - accuracy: 0.9426 - val_loss: 0.1919 - val_accuracy: 0.9330\n",
      "Epoch 4/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1433 - accuracy: 0.9486 - val_loss: 0.1887 - val_accuracy: 0.9506\n",
      "Epoch 5/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1291 - accuracy: 0.9549 - val_loss: 0.1787 - val_accuracy: 0.9358\n",
      "Epoch 6/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1193 - accuracy: 0.9565 - val_loss: 0.1759 - val_accuracy: 0.9428\n",
      "Epoch 7/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1099 - accuracy: 0.9620 - val_loss: 0.1668 - val_accuracy: 0.9386\n",
      "Epoch 8/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1021 - accuracy: 0.9636 - val_loss: 0.1701 - val_accuracy: 0.9478\n",
      "Epoch 9/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0980 - accuracy: 0.9666 - val_loss: 0.1591 - val_accuracy: 0.9541\n",
      "Epoch 10/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0927 - accuracy: 0.9657 - val_loss: 0.1623 - val_accuracy: 0.9555\n",
      "Epoch 11/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0895 - accuracy: 0.9676 - val_loss: 0.1580 - val_accuracy: 0.9513\n",
      "Epoch 12/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0846 - accuracy: 0.9676 - val_loss: 0.1610 - val_accuracy: 0.9520\n",
      "Epoch 13/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0809 - accuracy: 0.9702 - val_loss: 0.1655 - val_accuracy: 0.9534\n",
      "Epoch 14/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0764 - accuracy: 0.9694 - val_loss: 0.1650 - val_accuracy: 0.9520\n",
      "Epoch 15/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.9730 - val_loss: 0.1618 - val_accuracy: 0.9506\n",
      "Epoch 16/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0685 - accuracy: 0.9745 - val_loss: 0.1671 - val_accuracy: 0.9534\n",
      "Epoch 17/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0681 - accuracy: 0.9746 - val_loss: 0.1724 - val_accuracy: 0.9464\n",
      "Epoch 18/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0634 - accuracy: 0.9770 - val_loss: 0.1680 - val_accuracy: 0.9570\n",
      "Epoch 19/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9777 - val_loss: 0.1716 - val_accuracy: 0.9548\n",
      "Epoch 20/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0610 - accuracy: 0.9772 - val_loss: 0.1725 - val_accuracy: 0.9534\n",
      "Total time taken for the program execution 7.449026107788086\n",
      "Train loss: 0.046613141894340515 / Train accuracy: 0.982687771320343\n",
      "Test loss: 0.21222521364688873 / Test accuracy: 0.9562087655067444\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "history = ann_model.fit(x_train, y_train, epochs=20, batch_size=50, validation_data=(x_val, y_val))\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Total time taken for the program execution\", time_taken) # seconds\n",
    "\n",
    "score = ann_model.evaluate(x_train, y_train, verbose=0)\n",
    "print(f'Train loss: {score[0]} / Train accuracy: {score[1]}')\n",
    "\n",
    "score = ann_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "53/53 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.93046   0.93947   0.93494       413\n",
      "           1    0.94255   0.96304   0.95269       460\n",
      "           2    0.96992   0.93478   0.95203       276\n",
      "           3    0.98249   0.97490   0.97868       518\n",
      "\n",
      "    accuracy                        0.95621      1667\n",
      "   macro avg    0.95636   0.95305   0.95458      1667\n",
      "weighted avg    0.95650   0.95621   0.95626      1667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = ann_model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(classification_report(y_test, pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN\n",
    "training a ANN with categorical and numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Unnamed: 0', 'screen_name', 'url', 'profile_image_url', 'description',\n",
    "           'id', 'name', 'account_type', 'tweets_list', 'tweets_list_processed',\n",
    "          'description_processed', 'protected', 'verified', 'account_type_multi', \n",
    "          'profile_use_background_image', 'profile_background_tile']\n",
    "\n",
    "x_train, y_train = train_df.drop(to_drop, axis=1), train_df['account_type_multi']\n",
    "x_test, y_test = test_df.drop(to_drop, axis=1), test_df['account_type_multi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_26 (Dense)            (None, 64)                14272     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,366\n",
      "Trainable params: 23,366\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "ann_model = Sequential()\n",
    "ann_model.add(Dense(64, activation = 'relu', input_dim = 222))\n",
    "ann_model.add(Dropout(.1))\n",
    "ann_model.add(Dense(128, activation='relu'))\n",
    "ann_model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "ann_model.summary()\n",
    "# configure the model\n",
    "ann_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.5580 - accuracy: 0.8250 - val_loss: 0.2441 - val_accuracy: 0.9358\n",
      "Epoch 2/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2441 - accuracy: 0.9248 - val_loss: 0.2012 - val_accuracy: 0.9492\n",
      "Epoch 3/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1615 - accuracy: 0.9539 - val_loss: 0.1680 - val_accuracy: 0.9661\n",
      "Epoch 4/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1306 - accuracy: 0.9655 - val_loss: 0.1317 - val_accuracy: 0.9732\n",
      "Epoch 5/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1088 - accuracy: 0.9701 - val_loss: 0.1301 - val_accuracy: 0.9753\n",
      "Epoch 6/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0827 - accuracy: 0.9765 - val_loss: 0.1092 - val_accuracy: 0.9682\n",
      "Epoch 7/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0849 - accuracy: 0.9762 - val_loss: 0.1267 - val_accuracy: 0.9760\n",
      "Epoch 8/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0786 - accuracy: 0.9801 - val_loss: 0.2178 - val_accuracy: 0.9661\n",
      "Epoch 9/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0781 - accuracy: 0.9787 - val_loss: 0.1045 - val_accuracy: 0.9774\n",
      "Epoch 10/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0709 - accuracy: 0.9826 - val_loss: 0.1774 - val_accuracy: 0.9647\n",
      "Epoch 11/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0629 - accuracy: 0.9822 - val_loss: 0.0921 - val_accuracy: 0.9774\n",
      "Epoch 12/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0529 - accuracy: 0.9844 - val_loss: 0.0987 - val_accuracy: 0.9781\n",
      "Epoch 13/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0559 - accuracy: 0.9831 - val_loss: 0.1105 - val_accuracy: 0.9788\n",
      "Epoch 14/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0531 - accuracy: 0.9852 - val_loss: 0.0845 - val_accuracy: 0.9774\n",
      "Epoch 15/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0465 - accuracy: 0.9867 - val_loss: 0.0831 - val_accuracy: 0.9817\n",
      "Epoch 16/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0459 - accuracy: 0.9874 - val_loss: 0.0908 - val_accuracy: 0.9788\n",
      "Epoch 17/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0423 - accuracy: 0.9878 - val_loss: 0.0863 - val_accuracy: 0.9788\n",
      "Epoch 18/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 0.9874 - val_loss: 0.0879 - val_accuracy: 0.9767\n",
      "Epoch 19/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0429 - accuracy: 0.9865 - val_loss: 0.1121 - val_accuracy: 0.9781\n",
      "Epoch 20/20\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0416 - accuracy: 0.9872 - val_loss: 0.0893 - val_accuracy: 0.9795\n",
      "Total time taken for the program execution 6.7363951206207275\n",
      "Train loss: 0.03321533277630806 / Train accuracy: 0.990036129951477\n",
      "Test loss: 0.10333868116140366 / Test accuracy: 0.9772045612335205\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "history = ann_model.fit(x_train, y_train, epochs=20, batch_size=50, validation_data=(x_val, y_val))\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Total time taken for the program execution\", time_taken) # seconds\n",
    "\n",
    "score = ann_model.evaluate(x_train, y_train, verbose=0)\n",
    "print(f'Train loss: {score[0]} / Train accuracy: {score[1]}')\n",
    "\n",
    "score = ann_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1200x600 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"378.83625pt\" version=\"1.1\" viewBox=\"0 0 713.265625 378.83625\" width=\"713.265625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-11-15T20:49:16.562811</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 378.83625 \nL 713.265625 378.83625 \nL 713.265625 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 173.518125 \nL 706.065625 173.518125 \nL 706.065625 22.318125 \nL 36.465625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mfce5398858\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.901989\" xlink:href=\"#mfce5398858\" y=\"173.518125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <g transform=\"translate(58.950426 188.116562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.997682\" xlink:href=\"#mfce5398858\" y=\"173.518125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2.5 -->\n      <g transform=\"translate(139.04612 188.116562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"227.093376\" xlink:href=\"#mfce5398858\" y=\"173.518125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 5.0 -->\n      <g transform=\"translate(219.141814 188.116562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"307.18907\" xlink:href=\"#mfce5398858\" y=\"173.518125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 7.5 -->\n      <g transform=\"translate(299.237507 188.116562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"387.284764\" xlink:href=\"#mfce5398858\" y=\"173.518125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10.0 -->\n      <g transform=\"translate(376.151951 188.116562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"467.380458\" xlink:href=\"#mfce5398858\" y=\"173.518125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 12.5 -->\n      <g transform=\"translate(456.247645 188.116562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"547.476151\" xlink:href=\"#mfce5398858\" y=\"173.518125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 15.0 -->\n      <g transform=\"translate(536.343339 188.116562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"627.571845\" xlink:href=\"#mfce5398858\" y=\"173.518125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 17.5 -->\n      <g transform=\"translate(616.439033 188.116562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m6a77a1aedf\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m6a77a1aedf\" y=\"145.543443\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.85 -->\n      <g transform=\"translate(7.2 149.342662)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m6a77a1aedf\" y=\"103.323751\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.90 -->\n      <g transform=\"translate(7.2 107.12297)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m6a77a1aedf\" y=\"61.104059\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.95 -->\n      <g transform=\"translate(7.2 64.903278)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_12\">\n    <path clip-path=\"url(#p6319ceea97)\" d=\"M 66.901989 166.645398 \nL 98.940266 82.405853 \nL 130.978544 57.796546 \nL 163.016821 48.015907 \nL 195.055099 44.124709 \nL 227.093376 38.761113 \nL 259.131654 38.971441 \nL 291.169931 35.711228 \nL 323.208209 36.868109 \nL 355.246486 33.607896 \nL 387.284764 33.923413 \nL 419.323041 32.030359 \nL 451.361319 33.187239 \nL 483.399596 31.399374 \nL 515.437874 30.137354 \nL 547.476151 29.506319 \nL 579.514429 29.190852 \nL 611.552706 29.506319 \nL 643.590984 30.242493 \nL 675.629261 29.716648 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p6319ceea97)\" d=\"M 66.901989 73.1115 \nL 98.940266 61.78936 \nL 130.978544 47.487696 \nL 163.016821 41.528648 \nL 195.055099 39.740934 \nL 227.093376 45.699981 \nL 259.131654 39.145029 \nL 291.169931 47.487696 \nL 323.208209 37.95327 \nL 355.246486 48.679505 \nL 387.284764 37.95327 \nL 419.323041 37.357365 \nL 451.361319 36.76146 \nL 483.399596 37.95327 \nL 515.437874 34.377841 \nL 547.476151 36.76146 \nL 579.514429 36.76146 \nL 611.552706 38.549174 \nL 643.590984 37.357365 \nL 675.629261 36.165555 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 173.518125 \nL 36.465625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 706.065625 173.518125 \nL 706.065625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 173.518125 \nL 706.065625 173.518125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 22.318125 \nL 706.065625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_12\">\n    <!-- Train and Validation Accuracy -->\n    <g transform=\"translate(282.841562 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 28.609375 0 \nL 0.78125 72.90625 \nL 11.078125 72.90625 \nL 34.1875 11.53125 \nL 57.328125 72.90625 \nL 67.578125 72.90625 \nL 39.796875 0 \nz\n\" id=\"DejaVuSans-86\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"271.675781\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"332.955078\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"396.333984\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"459.810547\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"491.597656\" xlink:href=\"#DejaVuSans-86\"/>\n     <use x=\"552.255859\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"613.535156\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"641.318359\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"669.101562\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"732.578125\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"793.857422\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"833.066406\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"860.849609\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"922.03125\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"985.410156\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1017.197266\" xlink:href=\"#DejaVuSans-65\"/>\n     <use x=\"1083.855469\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1138.835938\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1193.816406\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"1257.195312\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1298.308594\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1359.587891\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1414.568359\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 623.4875 168.518125 \nL 699.065625 168.518125 \nQ 701.065625 168.518125 701.065625 166.518125 \nL 701.065625 138.161875 \nQ 701.065625 136.161875 699.065625 136.161875 \nL 623.4875 136.161875 \nQ 621.4875 136.161875 621.4875 138.161875 \nL 621.4875 166.518125 \nQ 621.4875 168.518125 623.4875 168.518125 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_14\">\n     <path d=\"M 625.4875 144.260312 \nL 645.4875 144.260312 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_15\"/>\n    <g id=\"text_13\">\n     <!-- train acc -->\n     <g transform=\"translate(653.4875 147.760312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"325.830078\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"380.810547\" xlink:href=\"#DejaVuSans-99\"/>\n     </g>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 625.4875 158.938437 \nL 645.4875 158.938437 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_14\">\n     <!-- val acc -->\n     <g transform=\"translate(653.4875 162.438437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"180.029297\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"241.308594\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"296.289062\" xlink:href=\"#DejaVuSans-99\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_8\">\n    <path d=\"M 36.465625 354.958125 \nL 706.065625 354.958125 \nL 706.065625 203.758125 \nL 36.465625 203.758125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_9\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.901989\" xlink:href=\"#mfce5398858\" y=\"354.958125\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.0 -->\n      <g transform=\"translate(58.950426 369.556562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.997682\" xlink:href=\"#mfce5398858\" y=\"354.958125\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 2.5 -->\n      <g transform=\"translate(139.04612 369.556562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"227.093376\" xlink:href=\"#mfce5398858\" y=\"354.958125\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 5.0 -->\n      <g transform=\"translate(219.141814 369.556562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"307.18907\" xlink:href=\"#mfce5398858\" y=\"354.958125\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 7.5 -->\n      <g transform=\"translate(299.237507 369.556562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"387.284764\" xlink:href=\"#mfce5398858\" y=\"354.958125\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 10.0 -->\n      <g transform=\"translate(376.151951 369.556562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"467.380458\" xlink:href=\"#mfce5398858\" y=\"354.958125\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 12.5 -->\n      <g transform=\"translate(456.247645 369.556562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"547.476151\" xlink:href=\"#mfce5398858\" y=\"354.958125\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 15.0 -->\n      <g transform=\"translate(536.343339 369.556562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"627.571845\" xlink:href=\"#mfce5398858\" y=\"354.958125\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 17.5 -->\n      <g transform=\"translate(616.439033 369.556562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_4\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m6a77a1aedf\" y=\"332.539135\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 0.1 -->\n      <g transform=\"translate(13.5625 336.338354)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m6a77a1aedf\" y=\"305.920226\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 0.2 -->\n      <g transform=\"translate(13.5625 309.719445)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m6a77a1aedf\" y=\"279.301316\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 0.3 -->\n      <g transform=\"translate(13.5625 283.100535)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m6a77a1aedf\" y=\"252.682406\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 0.4 -->\n      <g transform=\"translate(13.5625 256.481625)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m6a77a1aedf\" y=\"226.063497\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 0.5 -->\n      <g transform=\"translate(13.5625 229.862716)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_31\">\n    <path clip-path=\"url(#p2ad95e2356)\" d=\"M 66.901989 210.630852 \nL 98.940266 294.190718 \nL 130.978544 316.168976 \nL 163.016821 324.384153 \nL 195.055099 330.197044 \nL 227.093376 337.135907 \nL 259.131654 336.559706 \nL 291.169931 338.23327 \nL 323.208209 338.370855 \nL 355.246486 340.27241 \nL 387.284764 342.425391 \nL 419.323041 345.076107 \nL 451.361319 344.275441 \nL 483.399596 345.011801 \nL 515.437874 346.785488 \nL 547.476151 346.947386 \nL 579.514429 347.893395 \nL 611.552706 346.896777 \nL 643.590984 347.735552 \nL 675.629261 348.085398 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path clip-path=\"url(#p2ad95e2356)\" d=\"M 66.901989 294.190456 \nL 98.940266 305.606639 \nL 130.978544 314.432609 \nL 163.016821 324.08967 \nL 195.055099 324.514786 \nL 227.093376 330.090904 \nL 259.131654 325.428138 \nL 291.169931 301.181911 \nL 323.208209 331.335112 \nL 355.246486 311.933146 \nL 387.284764 334.645139 \nL 419.323041 332.887765 \nL 451.361319 329.751936 \nL 483.399596 336.669263 \nL 515.437874 337.044863 \nL 547.476151 334.989144 \nL 579.514429 336.199015 \nL 611.552706 335.763413 \nL 643.590984 329.322498 \nL 675.629261 335.398979 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 36.465625 354.958125 \nL 36.465625 203.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 706.065625 354.958125 \nL 706.065625 203.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 36.465625 354.958125 \nL 706.065625 354.958125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 36.465625 203.758125 \nL 706.065625 203.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_28\">\n    <!-- Train and Validation Loss -->\n    <g transform=\"translate(297.074687 197.758125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-84\"/>\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"271.675781\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"332.955078\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"396.333984\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"459.810547\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"491.597656\" xlink:href=\"#DejaVuSans-86\"/>\n     <use x=\"552.255859\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"613.535156\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"641.318359\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"669.101562\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"732.578125\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"793.857422\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"833.066406\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"860.849609\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"922.03125\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"985.410156\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1017.197266\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"1071.160156\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1132.341797\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1184.441406\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_2\">\n    <g id=\"patch_13\">\n     <path d=\"M 621.296875 241.114375 \nL 699.065625 241.114375 \nQ 701.065625 241.114375 701.065625 239.114375 \nL 701.065625 210.758125 \nQ 701.065625 208.758125 699.065625 208.758125 \nL 621.296875 208.758125 \nQ 619.296875 208.758125 619.296875 210.758125 \nL 619.296875 239.114375 \nQ 619.296875 241.114375 621.296875 241.114375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_33\">\n     <path d=\"M 623.296875 216.856563 \nL 643.296875 216.856563 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_34\"/>\n    <g id=\"text_29\">\n     <!-- train loss -->\n     <g transform=\"translate(651.296875 220.356563)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"232.763672\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"264.550781\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"292.333984\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"353.515625\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"405.615234\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_35\">\n     <path d=\"M 623.296875 231.534688 \nL 643.296875 231.534688 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_36\"/>\n    <g id=\"text_30\">\n     <!-- val loss -->\n     <g transform=\"translate(651.296875 235.034688)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"180.029297\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"207.8125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"268.994141\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"321.09375\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6319ceea97\">\n   <rect height=\"151.2\" width=\"669.6\" x=\"36.465625\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p2ad95e2356\">\n   <rect height=\"151.2\" width=\"669.6\" x=\"36.465625\" y=\"203.758125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAAIOCAYAAABOJNWwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAA9hAAAPYQGoP6dpAACsp0lEQVR4nOzdeVhUZf8G8HvYd0REwA033FfEBVzTBEXR1BIVFxQrKyuzMv21uNSbZmVar0uZirtobpim4k5pigpq7pkICkiggoJsw/n98bwzMLINyHBm4P5c17mYOefMOd+ZYZR7nuc8j0KSJAlEREREREREpBNGchdAREREREREVJUxeBMRERERERHpEIM3ERERERERkQ4xeBMRERERERHpEIM3ERERERERkQ4xeBMRERERERHpEIM3ERERERERkQ4xeBMRERERERHpEIM3ERERERERkQ4xeBMRUbEUCoVWy7Fjx57rPHPmzIFCoaiYoitZSEgIFAoFYmJiit2nY8eOqFu3LpRKZbH7dO/eHbVq1UJ2drZW542JiYFCoUBISEiZalHp06cP+vTpo9W5nvXll19i165dhdYfO3asQn4fntfw4cOhUCgwdepUWesgIiJSYfAmIqJinTp1SmPx8/ODpaVlofUeHh7PdZ7Jkyfj1KlTFVS1/gkODkZ8fDwOHDhQ5PYbN27g5MmTGDduHMzMzMp9nkGDBuHUqVNwdXUt9zG0UVzw9vDwqJDfh+eRlJSEX3/9FQCwceNGZGZmylYLERGRioncBRARkf7q1q2bxn0nJycYGRkVWv+sjIwMWFlZaX2eevXqoV69euWq0RAEBgbiww8/xOrVq+Hn51do++rVqwEAkyZNeq7zODk5wcnJ6bmO8Tzs7OxK/d3QtXXr1iEnJweDBg3C3r17sWPHDowZM0bWmorz9OlTWFpayl0GERFVArZ4ExHRc+nTpw/atGmDEydOwNvbG1ZWVuoAGRoaCh8fH7i6usLS0hItW7bEzJkzkZ6ernGMorqaN2zYEIMHD8b+/fvh4eEBS0tLtGjRQh1SSzN37lx07doVNWvWhJ2dHTw8PLBq1SpIklTu8/z555/o3r07LCwsUKdOHcyaNQs5OTml1uLg4IBhw4Zhz549SElJ0dimVCqxfv16dO7cGW3btsXff/+NiRMnwt3dHVZWVqhbty78/f1x6dKlUs9TVFdzSZKwcOFCuLm5wcLCAh4eHvjtt98KPTYzMxPvv/8+OnToAHt7e9SsWRNeXl7YvXu3xn4KhQLp6elYu3at+lIDVZf14rqah4WFwcvLC1ZWVrC1tUX//v0L9XBQ/Q5cvnwZo0ePhr29PZydnTFp0iSkpqaW+txVVq9eDWdnZ6xduxaWlpbF/r6cPn0a/v7+cHR0hIWFBZo0aYJp06Zp7HPt2jWMHj0azs7OMDc3R4MGDTB+/HhkZWVp1Pysot4H1e/Zjh070LFjR1hYWGDu3LkAgKVLl6JXr16oXbs2rK2t0bZtWyxcuLDI3639+/ejX79+sLe3h5WVFVq2bIn58+cDANavXw+FQlFk75F58+bB1NQU8fHxWr2ORERUsdjiTUREzy0hIQFjx47FjBkz8OWXX8LISHyve/PmTfj5+WHatGmwtrbGtWvX8NVXX+HMmTM4cuRIqce9cOEC3n//fcycORPOzs74+eefERwcjKZNm6JXr14lPjYmJgavv/46GjRoAECE5rfffhv37t3DZ599VubzXLlyBf369UPDhg0REhICKysrLFu2DJs2bdLqNQoODsbmzZuxYcMGvPvuu+r1Bw4cQHx8vLqm+Ph4ODo6YsGCBXBycsKDBw+wdu1adO3aFVFRUWjevLlW51OZO3cu5s6di+DgYLz88suIi4vDq6++CqVSqXGsrKwsPHjwAB988AHq1q2L7OxsHDp0CMOHD8eaNWswfvx4AOLyg759++KFF17Ap59+CkC0dBdn06ZNCAwMhI+PDzZv3oysrCwsXLgQffr0weHDh9GjRw+N/UeMGIGAgAAEBwfj0qVLmDVrFgBo9YXLyZMncfXqVXz44YdwdHTEiBEjsHHjRty+fRuNGjVS73fgwAH4+/ujZcuWWLRoERo0aICYmBgcPHhQvc+FCxfQo0cP1KpVC/PmzYO7uzsSEhIQFhaG7OxsmJuba/Hqazp//jyuXr2KTz75BI0aNYK1tTUA4NatWxgzZgwaNWoEMzMzXLhwAf/5z39w7do1jee9atUqvPrqq+jduzdWrFiB2rVr48aNG/jrr78AAAEBAZgxYwaWLl0KLy8v9eNyc3Px448/YtiwYahTp06Z6yYiogogERERaWnChAmStbW1xrrevXtLAKTDhw+X+Ni8vDwpJydHOn78uARAunDhgnrb7NmzpWf/S3Jzc5MsLCykO3fuqNc9ffpUqlmzpvT666+XqW6lUinl5ORI8+bNkxwdHaW8vLwynycgIECytLSUEhMT1etyc3OlFi1aSACk27dvl/r8GzVqJLVr105j/YgRIyQrKyspNTW1yMfl5uZK2dnZkru7u/Tee++p19++fVsCIK1Zs0a9bs2aNRq1PHz4ULKwsJCGDRumccw//vhDAiD17t272Hpzc3OlnJwcKTg4WOrYsaPGNmtra2nChAmFHnP06FEJgHT06FFJksTrXqdOHalt27aSUqlU7/f48WOpdu3akre3t3qd6ndg4cKFGsd88803JQsLC433rDiTJk2SAEhXr17VqOfTTz/V2K9JkyZSkyZNpKdPnxZ7rL59+0o1atSQkpKSit2nqN9bSSr8PkiS+D0zNjaWrl+/XuJzUP2urlu3TjI2NpYePHggSZJ4zezs7KQePXqU+FrMnj1bMjMzk+7fv69eFxoaKgGQjh8/XuK5iYhId9jVnIiInpuDgwP69u1baP0///yDMWPGwMXFBcbGxjA1NUXv3r0BAFevXi31uB06dFC3WAOAhYUFmjVrhjt37pT62CNHjuDFF1+Evb29+tyfffYZUlJSkJSUVObzHD16FP369YOzs7N6nbGxMQICAkqtBRBdtCdOnIiLFy/i3LlzAICUlBTs2bMHI0aMULca5+bm4ssvv0SrVq1gZmYGExMTmJmZ4ebNm1q9ZgWdOnUKmZmZCAwM1Fjv7e0NNze3Qvtv27YN3bt3h42NDUxMTGBqaopVq1aV+bwq169fR3x8PMaNG6fuBQEANjY2GDFiBP78809kZGRoPGbIkCEa99u1a4fMzMxC79mznjx5gq1bt8Lb2xstWrQAAPTu3RtNmjRBSEgI8vLyAIiB7G7duoXg4GBYWFgUeayMjAwcP34cI0eOrNBr5tu1a4dmzZoVWh8VFYUhQ4bA0dFR/bs6fvx4KJVK3LhxA4BozU9LS8Obb75Z4gwAb7zxBgBg5cqV6nX//e9/0bZt21J7iRARke4weBMR0XMrahTtJ0+eoGfPnjh9+jS++OILHDt2DJGRkdixYwcAMbBUaRwdHQutMzc3L/WxZ86cgY+PDwARQP744w9ERkbi448/LvLc2pwnJSUFLi4uhfYral1xJk6cCCMjI6xZswaAGHU7OzsbwcHB6n2mT5+OTz/9FC+99BL27NmD06dPIzIyEu3bt9fqNStIdT25NnXv2LEDI0eORN26dbFhwwacOnUKkZGRmDRpUrlHBledv6jfjzp16iAvLw8PHz7UWP/se6Hq0l3acw8NDcWTJ08wcuRIPHr0CI8ePUJqaipGjhyJuLg4hIeHAwD+/fdfAChxML+HDx9CqVRW+IB/Rb0OsbGx6NmzJ+7du4clS5YgIiICkZGRWLp0KYD8561N3QDg7OyMgIAA/Pjjj1Aqlbh48SIiIiI4tRoRkcx4jTcRET23olrgjhw5gvj4eBw7dkzdyg0Ajx490nk9W7ZsgampKX799VeNVs2ipsDSlqOjIxITEwutL2pdcerVqwcfHx9s2rQJ3377LdasWVPoevUNGzZg/Pjx+PLLLzUem5ycjBo1apS55uJqTExMRMOGDTXO26hRI4SGhmq8n6qBxMpDdf6EhIRC2+Lj42FkZAQHB4dyH7+gVatWAQCmTZtWaJA01XZfX191C/bdu3eLPVbNmjVhbGxc4j4A1L9bWVlZGtd8JycnF7l/UZ+TXbt2IT09HTt27NDohRAdHa2xnzZ1q7z77rtYv349du/ejf3796NGjRqFej0QEVHlYos3ERHphCpkPDsI1Y8//lgp5zYxMYGxsbF63dOnT7F+/fpyH/OFF17A4cOHcf/+ffU6pVKJ0NDQMh0nODgYDx8+xGeffYbo6GhMnDhRI5ApFIpCr9nevXtx7969MtfcrVs3WFhYYOPGjRrrT548Wai7vkKhgJmZmUYtiYmJhUY1B7TrdQAAzZs3R926dbFp0yaN0eTT09Oxfft29Ujnz+vq1as4deoURowYgaNHjxZa+vXrh927dyMlJQXNmjVDkyZNsHr16mK/VLC0tETv3r2xbdu2YkM0APUXFxcvXtRYv2fPHq1rL+pzIkmSRldxQFweYG9vjxUrVhQamf9ZnTp1gre3N7766its3LgRQUFB6oHciIhIHgzeRESkE97e3nBwcMCUKVOwc+dO/Prrrxg9ejQuXLig83MPGjQIT548wZgxYxAeHo4tW7agZ8+e5RqJWuWTTz4BAPTt2xehoaHYs2cPBg0aVGhqtNIMGTIEtWrVwtdffw1jY2NMmDBBY/vgwYMREhKCxYsX48iRI/j6668xceLEcnV7dnBwwAcffICdO3di8uTJOHDgAH7++WeMHDmyUFfzwYMH4/r163jzzTdx5MgRrF27Fj169Ciye3Tbtm1x7Ngx7NmzB2fPnsX169eLPL+RkREWLlyI6OhoDB48GGFhYdi2bRteeOEFPHr0CAsWLCjzcyqKqrV7xowZ6NOnT6Hl3XffRXZ2NjZs2ABATN91584ddOvWDevWrcOxY8ewbt06jVbhRYsWIScnB127dsXKlStx9OhRbNmyBWPGjMHjx48BAH5+fqhZsyaCg4Oxa9cu/Prrr+qR47XVv39/mJmZYfTo0fjtt9+wc+dO+Pr6FuqCb2Njg2+//RYnTpzAiy++iC1btuDo0aNYuXJlkd3I3333XZw5cwZPnz7Fm2++WebXlIiIKhaDNxER6YSjoyP27t0LKysrjB07FpMmTYKNjU2ZW4jLo2/fvli9ejUuXboEf39/fPzxx3j55Zcxc+bMch+zTZs2OHToEOzs7DBhwgS89tpraNeunXpKLW2ZmZlh3LhxkCQJvr6+qFu3rsb2JUuWYOzYsZg/fz78/f0RFhaGHTt2oEmTJuWqe968eZg/fz4OHjyIIUOG4IcffsCKFSsKTUs2ceJELFiwAL/99hv8/Pzw1VdfYebMmRgzZkyhYy5ZsgTu7u4YNWoUOnfujNdff73Y848ZMwa7du1CSkoKAgICMHHiRNjZ2eHo0aOFphIrj5ycHKxfvx4dOnRAly5ditzHz88P9erVUwd0X19fnDhxAq6urnjnnXcwYMAAzJs3T2PgvPbt2+PMmTPo1KkTZs2ahQEDBuCjjz6Cubk5zMzMAIhp1Pbv3w9bW1uMHTsWU6ZMQZs2bdRjCWijRYsW2L59Ox4+fIjhw4fj7bffRocOHfD9998X2jc4OBj79u2DUqnE5MmTMXjwYCxevFhjYECVl156Cebm5vD19YW7u7vW9RARkW4opNL6KxERERGRQdmzZw+GDBmCvXv3ws/PT+5yiIiqPQZvIiIioiriypUruHPnDt59911YW1vj/PnzJU4/RkRElYNdzYmIiIiqiDfffBNDhgyBg4MDNm/ezNBNRKQn2OJNREREREREpENs8SYiIiIiIiLSIQZvIiIiIiIiIh1i8CYiIiIiIiLSIRO5C6goeXl5iI+Ph62tLQcSISIiIiIiIp2TJAmPHz9GnTp1YGRUfLt2lQne8fHxqF+/vtxlEBERERERUTUTFxeHevXqFbu9ygRvW1tbAOIJ29nZyVwNERERERERVXVpaWmoX7++Oo8Wp8oEb1X3cjs7OwZvIiIiIiIiqjSlXe7MwdWIiIiIiIiIdIjBm4iIiIiIiEiHGLyJiIiIiIiIdIjBm4iIiIiIiEiHGLyJiIiIiIiIdKjKjGpOREREREQ6JElAXp5YlMqy/yxpm0IBGBkBxsYV+9PISBybSGYM3kREREREZSFJIizm5BS9ZGcXv62sS3mPVZ7wW9pPSZL7lS8fhUIE8YoM9cbGgKkpYGZW+qLNfuXdx9RU918sqH7fc3Pzl5wczfu6WtenD9Crl26fXyVh8CYiIiIiw5eTA2RkiCU9Pf92aYs2+2ZmFg62VLyyBlij/139Wp5Wc21IUn6Yq4oKBvKSAjxQvgAs5+s2bx6DNxERERGRViRJBNjUVODxY92EY7lDlUIhQs/zLKrgVBFLSSG3PMFY230ru1t3SV3fK7o7fMGfqpCanZ3fK0F1+9mlvNuK2p6VVfg1UH0ZlJ5eua+9sTFgYqK5mJoWXlfcem3WeXhU7nPSIQZvIiIiqp4kSfyhmpqav6Sni/BjZQVYWuYvqvuV0a1Tn0gS8PSpeG3S0vKXgve13aZt6+TzMjICrK3Fe1ZwKWqdtouFhXZBlyqf6jpuk2oSa1Tdvssa6LOyxL9dFRWIC/ZUIK1Uk99QIiIiqlIKtqAWtzx6VPK2tDTxB2xZGBlpBvFng3lJ68p639Ky/GFCkkT36PKG5IL3y/oalcTYGLC1zQ/B5QnDpT2mun05QtVLwfBMBoXvGBERkSFQKkVYBEofwVffqUKhtgG5uG0V1bXY2BiwtxeLjY1oHcrIEC29qkXVWpuXJ1rFK6tLp6lp6UFdqSw6QFdk12sjI8DOLn+xty/9dlHbLC0ZiomoWmLwJiIikpMkASkpQHx8yUtiovYtj89zPefzjPBb1LasrKIDdEUNTmVklB+aCy41ahS9vqjtVlYlh0FJEmG8YBB/NpiXdr8sj8nMzD+36trNtLTyvT4KhWYQLm9otrautoFZNZB3NX36RFRBGLyJiIh0QZJEWCouSN+7J34mJIhQV5FUgw3p+8jLqlZUbcJxcdsqIxAqFIC5uVhq1NDtuQDx3mVlaR/UC3758GxotrY2jF4QMlF97xUXJ5a7dwv/vHtXvCU1awKOjtovNWuKDgtERACDNxERPS9JAh4+BJKT87vFFhyIqipKTy+9hTo+XgQjbdWqBdSpA9StK34WtdSuLULU88zD+zyPLc+5zM2LD882NgyFRVFdR25pKXclBk2SgAcPNIN0UaG6YAeDkty/L5aysLcvW1h3dKzWnQuIqjQGbyIiKt7jx0W30j67FDW9CSC6G1fUQFTa7PO8owpnZYkW6NKec1m6/daoUXyQVi0uLiKgaothlao51fd9RbVSFwzVT59qdzxnZ6BePaB+/aJ/mpiIlnFtl4cPxXFVV1b884/2z83MrHAYr1Wr5LDu4MBB1Ul/5ebmd9Ap6WdR63x9gX795H4GFYPBm4ioOnr6VLsW2ydPtD+mnZ343/Xp0/yLIpVKcYyyHOd5mJmVLaxnZGg+35QU7c9lZaXZOl1US7Wrq9iPqgRV7/2Ci2oqX13d12YfY+P8X+vy/Cz4kdCHTiqSJIYEKK7rt+qnth1KatcuPlTXry8+qtp871W/vvbPQakU4btgGE9OLj2wq2Z+SkgQi7YUCvEdnyEHcXNz7cfos7MTg+Mb2nPUJ0plyYG3rD9L2vY84zza2DB4ExGRPsrOFoNwFXc9sWpRjY6tDXt77VpsLSzE/pIkWo6fZyCqstwv2Nqu+qs1NbXUpyUBKLI3p7l56c+3Th3xVx/7g5ZI9b1LRYdVuY6h+j6pKjMxeb4Qr81PExMRKku6rlrbQeOdnEoP1ap/miqTsbFopa5VS/vHqKaVL0vLekqK+OdO1QPg4UPg779197z0jbW1diG9pPs2NvoT4PPyyhZon+dnRQ8toq2y/nvRrZs8deoCgzcRkSHIzQWSkkpvof73X+2PaWlZ8vXEqhZbG5uy1apQiL90LSxEs4uuqf5S0SKop/ybh9+iXPDrxQbYf6MRTI3zMLzzXYwckIbeAyxh0qCOqJmButxycoDDh4GtW4GdO8v2HY8hMjYWrcSqxcRE835R68p6v7R9KqLlSvWlQm5u/qxkcqtVq+RQXbeuPKFaVxQK8c+tjQ3g5qb943JyCreuP3xoWF8Uqb6v1XZ6edX3raqZ/eLjn+/8NjbahfSCt62s8r9jrqgwXNxVW7pmYaFdr5jn6VFjZSW+167O/70qJMmQPpbFS0tLg729PVJTU2FnZyd3OUQkF0kSrbuXLwNXroifly8Dt26JgGZkJP7VV813XPB2Sdu0uV0Bj5egwN3MWjj3qAnMMtPQI/sI7BJviBF9VPMIl8bUVDM8Fxeu7eyq/P+AkiR+DX79VSwnTxb/Mjo5AcOHAyNHAr16iWBD2snJAY4cyQ/bqutbCzIyqtgwqouAW9ZzmJhUjY+QKvToqpvps9+FAaI7dGmhmmPLUXGyssQQJEWF8uLuF7VNrlZfbZibV0wvk4KXkhQVoi0sOGzI89I2hzJ4E5GaJIk/ikr7zystTfSybd8e6NBB/JFU6X98SpL4ivvZgH3lilbdjPXFfdRGJDrjLDxxFp6IRGckwVm93Ri56IIz6Isj6Gd0DF4ut2FR17HkVmpHx6qRBsopKws4diw/bMfEaG5v1w7w9wcGDRItJVu3Ajt2aF7e7eQEjBiRH8L1pRuiPsnJAY4ezQ/bDx7kb6tdG3j5ZfH6desmQir/sCMgf8B7fbiWnEjVyl6eAP/kiQitFdk6rPppYcH/dwwJgzdRNSJJYjqU5/3mNy1N/EFUVg4O+SFc9bNVKzHOVYU8uYQEzXCtCtjF9WE1NgaaNQNatxaFtG4NNG8uClLNbyxJZbtdAY958NgUZ/+pibO3HXE2phYiY2rh7sPC3biNjfLQxiUZT3LMcetfe41tFhZA9+5ioJF+/QAPD7bMAuKy9n37RNA+eFDz+lBzc/FaDR4swnaDBoUfrwqR27aJEF4wRDo757eE9+xZvf8Yys3VDNsFv6yoXTv/y4rq/joREVH1weBNZCAkSYx0+uhR2QLys/efZ8TIZykUJQ9QYmsrao6OFvm3qHObmgItW4oQrgrk7duLxthiX4j79zXDtSpgF9VvFRB/2bu754dr1dKsWQWl/vJLSwPOnwfOngUiI8XPoqaTUSjE6+TpCXTuLH62b5/fxfLOHXG97JEj4mdioubj7eyAPn1EsOzbVzz96tDYLUni92/PHhG2IyM1t7u6iqA9eLB4bayttT+2qtu0KoQX/PVzdhYtua+8AvToUT3CZW6u6EGgej2Sk/O3sWcAERFVdwzeRHooJwe4dk0Ehuho4MIF8bMsMxiVRKEQobik0KzN6J/W1tqHt6ws4OrVws+puMboevUkdGiZjQ7O8WhvehUdnp5C47jjMLp6WbOZsSAjI6BpU81wrQrYZZn7WEcyMsRzLhiyr18vemCbpk01Q3bHjuI904YkiddaFcKPHSv8Ojs7iwDet68InI0aPeeT0yMZGeJ5q7qQPzuYTufO+WG7Y8eK+QJCNVDYtm2Fr112cckP4d27V63QmZsLHD8unvf27Zphu1YtEbZfeQXo3Zs9LoiIqHpj8CaS2aNH+SFU9fPy5eIH8igqMJc1NFtb68d1lJIkpoSJPvYI0Ucf4kJUHqJv2+GfNKci97fBY7TDRXTABbSvnYAOLbPQpqs1rDo0yw/YejJ0bVYWcOlSfsA+e1a8r0V10W/QID9ge3oCnTpV7CDfSiUQFSWC4eHDwO+/5w9cpNKoUX4If+EFERYNSVwcsHevCNqHD4tLKlSsrYH+/UXQ9vMTrdy6lJ2tGcILfunh6pp/TbO3t358DstKqQROnBDdyLdv1xwg39ExP2z36cOwTUREpMLgTVRJJEkM3vRsi++dO0Xvb2ureS10+/YiWxr06K3//lu4e/jly5rNZADSYIuLaIdodMQFux6INvLApccNkaUsPMqOkZHI2wVfqw4dKjc45uaKp1IwZF+4IFpBn+XiohmyPT3FNa+VKSsL+PPP/K7pp08Xvgygdev8bum9ewM1alRujaXJywPOnMlv1b5wQXO7m5sI2v7+on65vo/JzgYOHRIhddcuzfH86tTJD+FeXvodwpVKICIiP2wnJeVvc3QEhg0Tz6NPHw6GRUREVBQGbyIdyMwUebJgyL5wofj5Tt3cNENj+/ZAw4b6/Yd4sR49Et8m3Lkjvmm4cSM/aBc3d7RCATRuXPga7BYt1N805OaKQxX80iI6WjMAFFS7duEw3qzZ87fA5eWJ7uGqgB0ZKep4tgUZAGrW1AzZnTuLsKVv11Y/fixawVUt4tHRmtuNjET9qhbx7t3l+QIoLQ0IDxdBe+9ezV8nIyMRXlVdyPXxGvasrPwQvnu3ZgivW1dzdG99+OwrleL3QhW279/P31azZn7YfuEFhm0iIqLSMHgTPaekJM0geOGCuD67qC7FZmYiEBQMg+3aVWy3Yp1SDWymCtZFLcV9u6DSqFHha7BbtBDzYpRDYmLhXgQ3bhQ9B7OFBdCmTeHXv7h/CiRJDHRWMGSfPy+C6rPs7EQX8YLXZTdsqH/hTxvJyeK6cFWL+I0bmtvNzEQ3aVWLeOfOugtet27lt2ofP67Zi8DODhgwQATtgQPFNcWGIitLfImgCuEFPzb16uWH8K5dKzeEK5XAH3/kh+2Cg/Q5OOSH7b59GbaJiIjKgsGbSEtKJXDzZuHrsRMSit7f0bFwi2uLFnr+x2puLnDvXuEwHRMjfsbGisRQmlq1RDO+m5vmYGctWpRt2OhyysgA/vpL8326cEFz6qiCGjfOf58aNxbdxlVhu6iB0i0txfRcBUO2u7t+tFLqQlycCOCqwdru3dPcbmMjunOrWsTbti3/a5GbC5w8mR+2r17V3O7uLrqPDx4sRgvX68+TlrKyxNRmqhBe8Iud+vU1Q7guvsjJy9MM2wX/TatRQzNsyzwJABERkcFi8CYqwpMnYmCsgi2pFy8W3Z1YoRDZ8tnrsevW1cPWzqdPRXgurrX63r3SJ+hWKMSTUwXrZ5cGDSolXJdVXp5ovX62q/rduyU/zsxMvJ8Fu4y3bFl9B42SJNECrgrhR48WHmS+Vi3R/Vg1h3iTJiV/Fh48APbvF0H7t980ByMzMRFzPau6kDdrppOnpTcyMzVD+JMn+dsaNBCDlr3yCtCly/P9+5KXJ77g2LYN+OUXzZHf7e3zw3a/fgzbREREFYHBm6o1SRJZ89mu4n//XfQUT5aWomtywZDdtq1o8dMLqakldwMveJFmcUxNxV/4xQXrevWq1F/iKSn51+BHRwO3bwPNm+e3ZrdpU6WeboXLyxOvnapb+okThXsWNGiQ3xret68Y2fvatfxW7T/+0Py+p2ZNMfr44MGAr6/+DexWWTIzgQMHRAgPC9MM4W5u+SG8c2ftQnheHnDqVH7YLthzwd4eeOklcbz+/fk7T0REVNEYvKnakSQxOu/334vrWIubG9vVtXBX8aZNZZ6DNzlZJMNnu4CrloKjNRXH2lozSDdsqHnfxaXq9pkmncvOFqONq1rET50qPLq7o2Phz12bNvmt2t26Va25rivC06eiV8C2bSKEF/xyo2FDEZhHjhTjDBQM4Xl5YtT6rVvFYwuGbTs7zbCtB1PdExERVVkM3lRt5OSIPzwXLQLOnctfb2wsLj1+tqt4ZU/xVKS8PHGhcViYWC5dKv0xjo7Ft1a7uYnmRL3rA09VVXq6aNFWjZh+/rz48svMTHRHV4Xthg3lrtRwPH0quuRv2wbs2aMZwhs1EkG6Vy/xev/yi7hGX8XWFhg6VIR0Hx+GbSIiosrC4E1V3qNHwMqVooVbdT2vhQUwYQIwcaII2XLN8Vukp09Fc+Hu3eKv6oLDCisUoim+pGCtN/3eiQp7+FAMmNauHX9VK0JGhmYIz8govI+tLTBkSH7Y1qt/74iIiKoJBm+qsm7fBhYvBlatym8RcnYGpk4FpkzRs6mHkpLExMRhYWJkpYJ/PdvaijmThgwRF77WrClfnUSktzIygH37RLfyyEgx5dvIkeI6eYZtIiIieTF4U5Vz8qToTr5zZ/5czm3aANOnA6NH68kfoJIEXL8uWrXDwsSFsAU/YvXri6A9ZIiYp4n9QYmIiIiIDJa2ObSaTpxDhiI3VwTtRYuAP//MX+/rKwJ3//56cFmzaoJi1fXaN29qbvfwEEF76FDR/132gomIiIiIqDIxeJNeSksTXcmXLBGDegNi0KaxY4H33hMt3bJ6/Fh0Hd+9W3QlLzjhsZmZmFtpyBAxulT9+vLVSUREREREsmPwJr0SGysGS1u5UoRvQFyz/eabYnF2lrG4u3fFKEdhYWKQtOzs/G01awKDBomw7esrrt8mIiIiIiICgzfpichI4NtvxRQ5SqVY16KF6E4+dixgaSlDUZIEXLgggvbu3WK+pIKaNhXdx4cMEaMdmfDjREREREREhTEpkGyUSpFpFy0Cfv89f32/fiJwDxgAGBlVclHZ2cCxY/nXaxecKFehALy88gdHa9GC12sTEREREVGpGLyp0j15AoSEiCnBbt0S60xNxcjk06eL8ccq1YMHYsLcsDDx8/Hj/G2WlmKC3KFDRVfy2rUruTgiIiIiIjJ0DN5Uae7eBf77X+DHH4FHj8Q6BwfgjTeAt94C6tSpxGJu3cpv1Y6IyO/fDgAuLoC/v2jV7tdPpn7uRERERERUVTB4k85FRYnu5Fu2iJm3AHF59HvvARMmANbWlVBEXh5w5kx+2L58WXN7mzb512t7esrQx52IiIiIiKoqBm/Sibw8YN8+MWDasWP563v1At5/X8yypfNsm5EBHDokgvavvwL37+dvMzYGevcWQdvfH2jcWMfFEBERERFRdcXgTRUqIwNYtw747jvgxg2xztgYCAgQLdyenjouQJKAHTtEEeHhwNOn+dvs7ICBA0XL9oABop87ERERERGRjjF4U4VITBTXby9fLsYqAwB7e+C114C33wbq16+EIjIyxMXiISH569zc8kch79ULMDOrhEKIiIiIiIjylauz77Jly9CoUSNYWFigU6dOiIiIKHH/pUuXomXLlrC0tETz5s2xbt06je0hISFQKBSFlszMzPKUR5Xo4kVg4kSRb//zHxG6GzUCliwRM3EtXFhJofvmTTHVV0iI6MP+wQdiDu7bt4HvvwdefJGhm4iIiIiIZFHmFu/Q0FBMmzYNy5YtQ/fu3fHjjz9i4MCBuHLlCho0aFBo/+XLl2PWrFlYuXIlOnfujDNnzuDVV1+Fg4MD/P391fvZ2dnh+vXrGo+1sLAox1MiXZMk4MABcf32oUP56729xXRgL70kupdXmh07RPpPSxPTfW3eDPTtW4kFEBERERERFU8hSZJUlgd07doVHh4eWL58uXpdy5Yt8dJLL2H+/PmF9vf29kb37t3x9ddfq9dNmzYNZ8+exe+//w5AtHhPmzYNj1RzTJVDWloa7O3tkZqaCjs7u3Ifh4qXmQls2CCu375yRawzMgJGjBCBu1u3Si4oJweYNUt8AwAA3bsDoaFA3bqVXAgREREREVVH2ubQMnU1z87Oxrlz5+Dj46Ox3sfHBydPnizyMVlZWYVari0tLXHmzBnk5OSo1z158gRubm6oV68eBg8ejKioqBJrycrKQlpamsZCupGUBMydCzRoALz6qgjdtrZisLRbt4CtW2UI3fHxolVbFbqnTweOHmXoJiIiIiIivVOm4J2cnAylUglnZ2eN9c7OzkhMTCzyMb6+vvj5559x7tw5SJKEs2fPYvXq1cjJyUFycjIAoEWLFggJCUFYWBg2b94MCwsLdO/eHTdv3iy2lvnz58Pe3l691K+UC4mrj+xsYPduMRp5gwbAnDnAv/+K67W/+UZcv71oEdCwoQzFHT0KdOwI/P67GKl8+3YRwE1NZSiGiIiIiIioZOUa1VyhUGjclySp0DqVTz/9FImJiejWrRskSYKzszOCgoKwcOFCGP/vQuBu3bqhW4Em0+7du8PDwwM//PADvv/++yKPO2vWLEyfPl19Py0tjeH7OeXlARERwMaNwC+/AA8f5m/r3FnMvz1iBGAi11j4eXnAggXAp5+K2+3aiULd3WUqiIiIiIiIqHRlilC1atWCsbFxodbtpKSkQq3gKpaWlli9ejV+/PFH3L9/H66urvjpp59ga2uLWrVqFfkYIyMjdO7cucQWb3Nzc5ibm5elfCqCJInBvzdtEmOS3b2bv83VFRg9GhgzBvDwAIr5bqVyPHgAjB8P7N0r7gcFAUuXAlZWMhZFRERERERUujIFbzMzM3Tq1Anh4eEYNmyYen14eDiGDh1a4mNNTU1Rr149AMCWLVswePBgGBkV3dNdkiRER0ejbdu2ZSmPyuD2bRG2N23KHygNEHNvjxgBBAYCvXtX8ujkxTl7FnjlFSAmBjA3F4F70iSZvwkgIiIiIiLSTpk7DU+fPh3jxo2Dp6cnvLy88NNPPyE2NhZTpkwBILqA37t3Tz1X940bN3DmzBl07doVDx8+xKJFi/DXX39h7dq16mPOnTsX3bp1g7u7O9LS0vD9998jOjoaS5curaCnSYAYJG3rVhG2T53KX29uDgweLFq2/fwAvZnFTZKAn34C3nlHXHTeuLHoWt6xo9yVERERERERaa3MwTsgIAApKSmYN28eEhIS0KZNG+zbtw9ubm4AgISEBMTGxqr3VyqV+Pbbb3H9+nWYmprihRdewMmTJ9GwwKhcjx49wmuvvYbExETY29ujY8eOOHHiBLp06fL8z7Cae/xYDJK2cSMQHg4olWK9QiEGBQ8MBIYPFy3deiU9HXjjDWD9enF/yBBg7VqgRg1ZyyIiIiIiIiqrMs/jra84j3e+7GzgwAHRsr17N/D0af42T08RtgMCxDXceun6ddHf/fJl0df9yy+BDz9k13IiIiIiItIr2uZQucanpgqWlwf88Ydo2d62TYxFptK0qQjbY8YAzZrJV6NWtm0T128/eQK4uABbtoiLzYmIiIiIiAwUg7eBu3gxf0TyAj384eICjBolwranpwE0FmdnAzNmAEuWiPu9eonQrbfN8kRERERERNph8DZAMTEiaG/aBPz1V/56W9v8EclfeEFPRiTXxt27wMiR+SO+ffQR8MUXMk4YTkREREREVHGYbAxEcnL+iOR//JG/3swMGDRItGwPGgRYWspXY7mEh4vik5PFCG/r1omB1IiIiIiIiKoIBm89lp6ePyL5wYNAbq5Yr1CIFu0xY8SI5A4O8tZZLnl5wH/+A8yeLaYN69BBTBXWpInclREREREREVUoBm89k5MjQvamTcCuXUBGRv42D4/8Ecnr1pWtxOeXkgKMHQvs3y/uv/qquLbb4JrriYiIiIiISsfgrQfy8sTlzRs3iu7kKSn525o0ES3bY8YALVrIV2OFOXMGeOUVMRKchQWwYgUwYYLcVREREREREekMg7eM/vpLtGxv2gTcuZO/vnbt/BHJu3QxgBHJtSFJwLJlwHvviWb9pk2B7duBdu3kroyIiIiIiEinGLwrWWxs/ojkFy/mr7exEddrBwYCfftWsQG9nzwBXntNPHFAPNHVq8VgakRERERERFVcVYp3eu/iRaB9+/z7pqaAn59o2fb3r6KXOF+9KuY4u3pVzG+2cKFo9a4SzfhERERERESlY/CuRG3bAo0bAw0aiLD98ssGOiK5tjZvFgOnpacDdeoAoaFAjx5yV0VERERERFSpGLwrkUIBXLoEWFnJXYmOZWUB778PLF0q7vftK/rWOzvLWxcREREREZEMjOQuoLqp8qE7Nhbo1Ss/dH/8sZgfjaGbiIiIiIiqKbZ4U8XZv1+MDvfggehDv349MGiQ3FURERERERHJii3e9PyUSmD2bDFS3IMHgKcncP48QzcRERERERHY4k3P699/RSt3eLi4P2UKsHgxYG4ua1lERERERET6gsGbyu/UKWDkSODuXXHx+o8/AmPHyl0VERERERGRXmFXcyo7SQK+/14Monb3LtC8OXD6NEM3ERERERFREdjiTWXz+DEweTKwdau4P3Ik8PPPgK2tvHURERERERHpKQZv0t7ly8CIEcD164CJCfDtt8Dbb4sJyomIiIiIiKhIDN6knQ0bgNdfBzIygHr1RIu3l5fcVREREREREek9XuNNJcvKAt54Axg3ToTu/v3FVGEM3URERERERFph8KaSffEFsGKF6E4+ezbw22+Ak5PcVRERERERERkMdjWn4uXmioHTAGDVKmDiRHnrISIiIiIiMkBs8abi7d8PJCaKFm5OFUZERERERFQuDN5UvDVrxM9x4wBTU3lrISIiIiIiMlAM3lS0f/8FwsLEbXYxJyIiIiIiKjcGbyraxo3iGm9PT6BNG7mrISIiIiIiMlgM3lSYJAGrV4vbkybJWwsREREREZGBY/Cmws6fBy5dAszNgVGj5K6GiIiIiIjIoDF4U2GqQdWGDQMcHOSthYiIiIiIyMAxeJOmzExg0yZxm93MiYiIiIiInhuDN2navRt4+BCoXx/o21fuaoiIiIiIiAwegzdpUnUzDwoCjI1lLYWIiIiIiKgqYPCmfHFxwMGD4nZQkKylEBERERERVRUM3pRv3ToxlVjv3kDjxnJXQ0REREREVCUweJMgSfndzDmoGhERERERUYVh8CYhIgK4dQuwtQVGjJC7GiIiIiIioiqDwZsEVWv3yJGAtbW8tRAREREREVUhDN4EPH4MbNsmbrObORERERERUYVi8CYRutPTgebNAS8vuashIiIiIiKqUhi8SXPuboVC1lKIiIiIiIiqGgbv6u7GDeD33wEjI2D8eLmrISIiIiIiqnIYvKu7kBDxc8AAoE4dWUshIiIiIiKqihi8qzOlEli3TtyeOFHeWoiIiIiIiKooBu/qLDwcuHcPcHQE/P3lroaIiIiIiKhKYvCuzlavFj8DAwFzc3lrISIiIiIiqqLKFbyXLVuGRo0awcLCAp06dUJERESJ+y9duhQtW7aEpaUlmjdvjnWq7s0FbN++Ha1atYK5uTlatWqFnTt3lqc00lZKCrB7t7jNubuJiIiIiIh0pszBOzQ0FNOmTcPHH3+MqKgo9OzZEwMHDkRsbGyR+y9fvhyzZs3CnDlzcPnyZcydOxdvvfUW9uzZo97n1KlTCAgIwLhx43DhwgWMGzcOI0eOxOnTp8v/zKhkmzYB2dlAx45A+/ZyV0NERERERFRlKSRJksrygK5du8LDwwPLly9Xr2vZsiVeeuklzJ8/v9D+3t7e6N69O77++mv1umnTpuHs2bP4/fffAQABAQFIS0vDb7/9pt5nwIABcHBwwObNm7WqKy0tDfb29khNTYWdnV1ZnlL15OEBREUB338PvP223NUQEREREREZHG1zaJlavLOzs3Hu3Dn4+PhorPfx8cHJkyeLfExWVhYsLCw01llaWuLMmTPIyckBIFq8nz2mr69vscek5xQdLUK3mRkwZozc1RAREREREVVpZQreycnJUCqVcHZ21ljv7OyMxMTEIh/j6+uLn3/+GefOnYMkSTh79ixWr16NnJwcJCcnAwASExPLdExABPq0tDSNhbS0Zo34OXSoGNGciIiIiIiIdKZcg6spFAqN+5IkFVqn8umnn2LgwIHo1q0bTE1NMXToUAQFBQEAjI2Ny3VMAJg/fz7s7e3VS/369cvzVKqf7Gxg40Zxm3N3ExERERER6VyZgnetWrVgbGxcqCU6KSmpUIu1iqWlJVavXo2MjAzExMQgNjYWDRs2hK2tLWrVqgUAcHFxKdMxAWDWrFlITU1VL3FxcWV5KtXXnj1iRPO6dYFnuvcTERERERFRxStT8DYzM0OnTp0QHh6usT48PBze3t4lPtbU1BT16tWDsbExtmzZgsGDB8PISJzey8ur0DEPHjxY4jHNzc1hZ2ensZAWVHN3jx8PFOhxQERERERERLphUtYHTJ8+HePGjYOnpye8vLzw008/ITY2FlOmTAEgWqLv3bunnqv7xo0bOHPmDLp27YqHDx9i0aJF+Ouvv7B27Vr1Md9991306tULX331FYYOHYrdu3fj0KFD6lHPqYLExwP794vb/+vuT0RERERERLpV5uAdEBCAlJQUzJs3DwkJCWjTpg327dsHNzc3AEBCQoLGnN5KpRLffvstrl+/DlNTU7zwwgs4efIkGjZsqN7H29sbW7ZswSeffIJPP/0UTZo0QWhoKLp27fr8z5DyrV8P5OUBPXoAzZrJXQ0REREREVG1UOZ5vPUV5/EuhSQBLVoAN24Aq1YBkybJXREREREREZFB08k83mTATp0SodvaGnjlFbmrISIiIiIiqjYYvKsL1aBqr7wC2NrKWwsREREREVE1wuBdHaSnA6Gh4jbn7iYiIiIiIqpUDN7VwfbtwJMnQNOmQM+ecldDRERERERUrTB4VweqbuZBQYBCIWspRERERERE1Q2Dd1X3zz/A8eMicI8fL3c1RERERERE1Q6Dd1UXEiJ++vgA9evLWgoREREREVF1xOBdlSmV+cGbg6oRERERERHJgsG7KjtyBIiLA2rUAIYOlbsaIiIiIiKiaonBuypbs0b8DAwELCzkrYWIiIiIiKiaYvCuqh4+BHbsELfZzZyIiIiIiEg2DN5V1ZYtQFYW0LYt4OEhdzVERERERETVFoN3VaXqZj5pEufuJiIiIiIikhGDd1X0119AZCRgYiKu7yYiIiIiIiLZMHhXRarW7iFDACcneWshIiIiIiKq5hi8q5qcHGD9enGbg6oRERERERHJjsG7qtm7F/j3X8DFBRgwQO5qiIiIiIiIqj0G76pG1c18/HhxjTcRERERERHJisG7KklMFC3eALuZExERERER6QkG76pkwwZAqQS6dQNatJC7GiIiIiIiIgKDd9UhSZpzdxMREREREZFeYPCuKs6cAa5cASwtgYAAuashIiIiIiKi/2HwripUrd0jRgB2dvLWQkRERERERGoM3lVBRgawebO4zW7mREREREREeoXBuyrYuRNISwMaNgR695a7GiIiIiIiIiqAwbsqUHUzDwoCjPiWEhERERER6ROmNEMXEwMcPgwoFCJ4ExERERERkV5h8DZ0a9eKn337Am5u8tZCREREREREhTB4G7K8PCAkRNzmoGpERERERER6icHbkB07Jrqa29sDw4bJXQ0REREREREVgcHbkKkGVRs1CrC0lLcWIiIiIiIiKhKDt6FKTQW2bxe32c2ciIiIiIhIbzF4G6rQUODpU6BVK6BzZ7mrISIiIiIiomIweBsqVTfziRPFVGJERERERESklxi8DdHVq8CffwLGxsC4cXJXQ0RERERERCVg8DZEqtbuQYMAZ2d5ayEiIiIiIqISMXgbmpwcYN06cXviRHlrISIiIiIiolKZyF0AldH+/cD9+0Dt2qLFm4iIiIiIDIpSqUROTo7cZZAWTE1NYWxs/NzHYfA2NKpu5mPHAqam8tZCRERERERakyQJiYmJePTokdylUBnUqFEDLi4uUDzHoNYM3obk33+BPXvEbXYzJyIiIiIyKKrQXbt2bVhZWT1XkCPdkyQJGRkZSEpKAgC4urqW+1gM3oZkwwYgN1fM292mjdzVEBERERGRlpRKpTp0Ozo6yl0OacnS0hIAkJSUhNq1a5e72zkHVzMUkqQ5dzcRERERERkM1TXdVlZWMldCZaV6z57nunwGb0Nx/jxw6RJgYQGMHi13NUREREREVA7sXm54KuI9Y/A2FKtXi5/DhgE1ashaChEREREREWmPwdsQZGYCmzaJ2+xmTkREREREBqphw4ZYvHix3GVUOg6uZgh27wYePQIaNAD69pW7GiIiIiIiqib69OmDDh06VFhYjoyMhLW1dYUcy5AweBsCVTfzCROACpi8nYiIiIiIqKJIkgSlUgkTk9LjpZOTUyVUpH/Y1VzfxcUB4eHidlCQrKUQEREREVH1ERQUhOPHj2PJkiVQKBRQKBSIiYnBsWPHoFAocODAAXh6esLc3BwRERG4desWhg4dCmdnZ9jY2KBz5844dOiQxjGf7WquUCjw888/Y9iwYbCysoK7uzvCwsJKrGvDhg3w9PSEra0tXFxcMGbMGPVc2yqXL1/GoEGDYGdnB1tbW/Ts2RO3bt1Sb1+9ejVat24Nc3NzuLq6YurUqc//gpWgXMF72bJlaNSoESwsLNCpUydERESUuP/GjRvRvn17WFlZwdXVFRMnTkRKSop6e0hIiPqNLLhkZmaWp7yqZe1aMZVYnz5A48ZyV0NERERERBVFkoD09MpfJEmr8pYsWQIvLy+8+uqrSEhIQEJCAurXr6/ePmPGDMyfPx9Xr15Fu3bt8OTJE/j5+eHQoUOIioqCr68v/P39ERsbW+J55s6di5EjR+LixYvw8/NDYGAgHjx4UOz+2dnZ+Pzzz3HhwgXs2rULt2/fRlCBRsp79+6hV69esLCwwJEjR3Du3DlMmjQJubm5AIDly5fjrbfewmuvvYZLly4hLCwMTZs21eo1Ka8ydzUPDQ3FtGnTsGzZMnTv3h0//vgjBg4ciCtXrqBBgwaF9v/9998xfvx4fPfdd/D398e9e/cwZcoUTJ48GTt37lTvZ2dnh+vXr2s81sLCohxPqQqRJCAkRNzmoGpERERERFVLRgZgY1P5533yBNDiOmt7e3uYmZnBysoKLi4uhbbPmzcP/fv3V993dHRE+/bt1fe/+OIL7Ny5E2FhYSW2KAcFBWH0/6ZM/vLLL/HDDz/gzJkzGDBgQJH7T5o0SX27cePG+P7779GlSxc8efIENjY2WLp0Kezt7bFlyxaYmpoCAJo1a6ZR1/vvv493331Xva5z586lvRzPpcwt3osWLUJwcDAmT56Mli1bYvHixahfvz6WL19e5P5//vknGjZsiHfeeQeNGjVCjx498Prrr+Ps2bMa+ykUCri4uGgs1V5EBHDrFmBrC4wYIXc1REREREREap6enhr309PTMWPGDLRq1Qo1atSAjY0Nrl27VmqLd7t27dS3ra2tYWtrW6jreEFRUVEYOnQo3NzcYGtriz59+gCA+jzR0dHo2bOnOnQXlJSUhPj4ePTr10/bp1khyhS8s7Ozce7cOfj4+Gis9/HxwcmTJ4t8jLe3N+7evYt9+/ZBkiTcv38fv/zyCwYNGqSx35MnT+Dm5oZ69eph8ODBiIqKKrGWrKwspKWlaSxVjmpQtYAArb6RIiIiIiIiA2JlJVqfK3uxsqqQ8p8dnfzDDz/E9u3b8Z///AcRERGIjo5G27ZtkZ2dXeJxng3ICoUCeXl5Re6bnp4OHx8f2NjYYMOGDYiMjFT3pFadx9LSsthzlbRNl8rU1Tw5ORlKpRLOzs4a652dnZGYmFjkY7y9vbFx40YEBAQgMzMTubm5GDJkCH744Qf1Pi1atEBISAjatm2LtLQ0LFmyBN27d8eFCxfg7u5e5HHnz5+PuXPnlqV8w/L4MbBtm7jNbuZERERERFWPQqH3DWxmZmZQKpVa7RsREYGgoCAMGzYMgGhcjYmJqdB6rl27huTkZCxYsEB9vfmzvanbtWuHtWvXIicnp1Cot7W1RcOGDXH48GG88MILFVpbSco1uJpCodC4L0lSoXUqV65cwTvvvIPPPvsM586dw/79+3H79m1MmTJFvU+3bt0wduxYtG/fHj179sTWrVvRrFkzjXD+rFmzZiE1NVW9xMXFleep6K9t28Q1H82bA15ecldDRERERETVUMOGDXH69GnExMQgOTm52JZoAGjatCl27NiB6OhoXLhwAWPGjClx//Jo0KABzMzM8MMPP+Cff/5BWFgYPv/8c419pk6dirS0NIwaNQpnz57FzZs3sX79evWYYnPmzMG3336L77//Hjdv3sT58+dLzJ4VoUzBu1atWjA2Ni7Uup2UlFSoFVxl/vz56N69Oz788EO0a9cOvr6+WLZsGVavXo2EhISiizIyQufOnXHz5s1iazE3N4ednZ3GUqWouplPnCi+CSMiIiIiIqpkH3zwAYyNjdGqVSs4OTmVeL32d999BwcHB3h7e8Pf3x++vr7w8PCo0HqcnJwQEhKCbdu2oVWrVliwYAG++eYbjX0cHR1x5MgRPHnyBL1790anTp2wcuVKdev3hAkTsHjxYixbtgytW7fG4MGDS8yeFUEhSVqOJf8/Xbt2RadOnbBs2TL1ulatWmHo0KGYP39+of1HjBgBExMThIaGqtedOnUK3t7euHfvHurUqVPoMZIkoUuXLmjbti1WqwJoKdLS0mBvb4/U1FTDD+E3boiWbiMjMY93Ea8REREREREZjszMTNy+fVs9LTMZjpLeO21zaJmnE5s+fTrGjRsHT09PeHl54aeffkJsbKy66/isWbNw7949rFu3DgDg7++PV199FcuXL4evry8SEhIwbdo0dOnSRR26586di27dusHd3R1paWn4/vvvER0djaVLl5a1vKpBNYXYwIEM3URERERERAauzME7ICAAKSkpmDdvHhISEtCmTRvs27cPbm5uAICEhASN7gdBQUF4/Pgx/vvf/+L9999HjRo10LdvX3z11VfqfR49eoTXXnsNiYmJsLe3R8eOHXHixAl06dKlAp6igVEqgbVrxW0OqkZERERERGTwytzVXF9Vma7mv/0G+PkBjo5AfDxgZiZ3RURERERE9JzY1dxwVURX83KNak46tGaN+Dl2LEM3ERERERFRFcDgrU9SUoDdu8VtdjMnIiIiIiKqEhi89cmmTUB2NtCxI9C+vdzVEBERERERUQVg8NYnqqnTJk2Stw4iIiIiIiKqMAze+iI6WixmZsCYMXJXQ0RERERERBWEwVtfqAZVGzoUqFlT3lqIiIiIiIiowjB464OsLGDDBnGb3cyJiIiIiKgKadiwIRYvXix3GbJi8NYHe/YADx4AdesC/fvLXQ0RERERERFVIAZvfaDqZj5hAmBsLG8tREREREREVKEYvOV27x6wf7+4HRQkaylEREREREQqP/74I+rWrYu8vDyN9UOGDMGECRMAALdu3cLQoUPh7OwMGxsbdO7cGYcOHSrTeSIjI9G/f3/UqlUL9vb26N27N86fP6+xz6NHj/Daa6/B2dkZFhYWaNOmDX799Vf19j/++AO9e/eGlZUVHBwc4Ovri4cPH5bzmVc8Bm+5rV8P5OUBPXoA7u5yV0NERERERJVEkoD09MpfJEm7+l555RUkJyfj6NGj6nUPHz7EgQMHEBgYCAB48uQJ/Pz8cOjQIURFRcHX1xf+/v6IjY3V+nV4/PgxJkyYgIiICPz5559wd3eHn58fHj9+DADIy8vDwIEDcfLkSWzYsAFXrlzBggULYPy/3sLR0dHo168fWrdujVOnTuH333+Hv78/lEql1jXomoncBVRrkpTfzZyDqhERERERVSsZGYCNTeWf98kTwNq69P1q1qyJAQMGYNOmTejXrx8AYNu2bahZs6b6fvv27dG+fXv1Y7744gvs3LkTYWFhmDp1qlb19O3bV+P+jz/+CAcHBxw/fhyDBw/GoUOHcObMGVy9ehXNmjUDADRu3Fi9/8KFC+Hp6Ylly5ap17Vu3Vqrc1cWtnjL6eRJ4MYN8Vv/yityV0NERERERKQhMDAQ27dvR1ZWFgBg48aNGDVqlLq1OT09HTNmzECrVq1Qo0YN2NjY4Nq1a2Vq8U5KSsKUKVPQrFkz2Nvbw97eHk+ePFEfIzo6GvXq1VOH7mepWrz1GVu85aRq7X7lFXm+6iIiIiIiItlYWYnWZznOqy1/f3/k5eVh79696Ny5MyIiIrBo0SL19g8//BAHDhzAN998g6ZNm8LS0hIvv/wysrOztT5HUFAQ/v33XyxevBhubm4wNzeHl5eX+hiWlpYlPr607fqAwVsu6elAaKi4zW7mRERERETVjkKhXZdvOVlaWmL48OHYuHEj/v77bzRr1gydOnVSb4+IiEBQUBCGDRsGQFzzHRMTU6ZzREREYNmyZfDz8wMAxMXFITk5Wb29Xbt2uHv3Lm7cuFFkq3e7du1w+PBhzJ07txzPsHKwq7lcfvlFfL3VtKkYWI2IiIiIiEgPBQYGYu/evVi9ejXGjh2rsa1p06bYsWMHoqOjceHCBYwZM6bQKOiladq0KdavX4+rV6/i9OnTCAwM1GjF7t27N3r16oURI0YgPDwct2/fxm+//Yb9/5sdatasWYiMjMSbb76Jixcv4tq1a1i+fLlGeJcbg7dcVN3Mg4LEV11ERERERER6qG/fvqhZsyauX7+OMWPGaGz77rvv4ODgAG9vb/j7+8PX1xceHh5lOv7q1avx8OFDdOzYEePGjcM777yD2rVra+yzfft2dO7cGaNHj0arVq0wY8YM9ajlzZo1w8GDB3HhwgV06dIFXl5e2L17N0xM9KeDt0KStB1MXr+lpaXB3t4eqampsLOzk7uckt26JVq6FQogNhaoV0/uioiIiIiISIcyMzNx+/ZtNGrUCBYWFnKXQ2VQ0nunbQ5li7ccQkLETx8fhm4iIiIiIqIqjsG7simVwNq14vbEifLWQkRERERERDrH4F3ZDh8G4uIABwdg6FC5qyEiIiIiIiIdY/CubKpB1caMAXhtBxERERERUZXH4F2ZHj4Edu4Utzl3NxERERERUbWgP+OrVwdpaaJ7eUwM0LGj3NUQEREREVElK+sc1yS/injPGLwrk5sbEBoqBljj3N1ERERERNWGmZkZjIyMEB8fDycnJ5iZmUHBTKDXJElCdnY2/v33XxgZGcHMzKzcx2LwloOxsdwVEBERERFRJTIyMkKjRo2QkJCA+Ph4ucuhMrCyskKDBg1gZFT+K7UZvImIiIiIiCqBmZkZGjRogNzcXCiVSrnLIS0YGxvDxMTkuXsnMHgTERERERFVEoVCAVNTU5iamspdClUijmpOREREREREpEMM3kREREREREQ6xOBNREREREREpENV5hpvSZIAAGlpaTJXQkRERERERNWBKn+q8mhxqkzwfvz4MQCgfv36MldCRERERERE1cnjx49hb29f7HaFVFo0NxB5eXmIj4+Hra2tXk9En5aWhvr16yMuLg52dnZyl0NlwPfOcPG9M2x8/wwX3zvDxffOcPG9M2x8/wyPJEl4/Pgx6tSpU+I831WmxdvIyAj16tWTuwyt2dnZ8cNkoPjeGS6+d4aN75/h4ntnuPjeGS6+d4aN759hKamlW4WDqxERERERERHpEIM3ERERERERkQ4xeFcyc3NzzJ49G+bm5nKXQmXE985w8b0zbHz/DBffO8PF985w8b0zbHz/qq4qM7gaERERERERkT5iizcRERERERGRDjF4ExEREREREekQgzcRERERERGRDjF4ExEREREREekQg7cOLFu2DI0aNYKFhQU6deqEiIiIEvc/fvw4OnXqBAsLCzRu3BgrVqyopEpJZf78+ejcuTNsbW1Ru3ZtvPTSS7h+/XqJjzl27BgUCkWh5dq1a5VUNQHAnDlzCr0HLi4uJT6Gnzn90bBhwyI/R2+99VaR+/NzJ58TJ07A398fderUgUKhwK5duzS2S5KEOXPmoE6dOrC0tESfPn1w+fLlUo+7fft2tGrVCubm5mjVqhV27typo2dQfZX03uXk5OCjjz5C27ZtYW1tjTp16mD8+PGIj48v8ZghISFFfhYzMzN1/Gyqn9I+e0FBQYXeh27dupV6XH72dK+0966oz5BCocDXX39d7DH52TNcDN4VLDQ0FNOmTcPHH3+MqKgo9OzZEwMHDkRsbGyR+9++fRt+fn7o2bMnoqKi8H//93945513sH379kquvHo7fvw43nrrLfz5558IDw9Hbm4ufHx8kJ6eXupjr1+/joSEBPXi7u5eCRVTQa1bt9Z4Dy5dulTsvvzM6ZfIyEiN9y48PBwA8Morr5T4OH7uKl96ejrat2+P//73v0VuX7hwIRYtWoT//ve/iIyMhIuLC/r374/Hjx8Xe8xTp04hICAA48aNw4ULFzBu3DiMHDkSp0+f1tXTqJZKeu8yMjJw/vx5fPrppzh//jx27NiBGzduYMiQIaUe187OTuNzmJCQAAsLC108hWqttM8eAAwYMEDjfdi3b1+Jx+Rnr3KU9t49+/lZvXo1FAoFRowYUeJx+dkzUBJVqC5dukhTpkzRWNeiRQtp5syZRe4/Y8YMqUWLFhrrXn/9dalbt246q5FKl5SUJAGQjh8/Xuw+R48elQBIDx8+rLzCqJDZs2dL7du313p/fub027vvvis1adJEysvLK3I7P3f6AYC0c+dO9f28vDzJxcVFWrBggXpdZmamZG9vL61YsaLY44wcOVIaMGCAxjpfX19p1KhRFV4zCc++d0U5c+aMBEC6c+dOsfusWbNGsre3r9jiqFRFvX8TJkyQhg4dWqbj8LNX+bT57A0dOlTq27dvifvws2e42OJdgbKzs3Hu3Dn4+PhorPfx8cHJkyeLfMypU6cK7e/r64uzZ88iJydHZ7VSyVJTUwEANWvWLHXfjh07wtXVFf369cPRo0d1XRoV4ebNm6hTpw4aNWqEUaNG4Z9//il2X37m9Fd2djY2bNiASZMmQaFQlLgvP3f65fbt20hMTNT4bJmbm6N3797F/v8HFP95LOkxpHupqalQKBSoUaNGifs9efIEbm5uqFevHgYPHoyoqKjKKZAKOXbsGGrXro1mzZrh1VdfRVJSUon787Onf+7fv4+9e/ciODi41H352TNMDN4VKDk5GUqlEs7OzhrrnZ2dkZiYWORjEhMTi9w/NzcXycnJOquViidJEqZPn44ePXqgTZs2xe7n6uqKn376Cdu3b8eOHTvQvHlz9OvXDydOnKjEaqlr165Yt24dDhw4gJUrVyIxMRHe3t5ISUkpcn9+5vTXrl278OjRIwQFBRW7Dz93+kn1f1xZ/v9TPa6sjyHdyszMxMyZMzFmzBjY2dkVu1+LFi0QEhKCsLAwbN68GRYWFujevTtu3rxZidUSAAwcOBAbN27EkSNH8O233yIyMhJ9+/ZFVlZWsY/hZ0//rF27Fra2thg+fHiJ+/GzZ7hM5C6gKnq2pUaSpBJbb4rav6j1VDmmTp2Kixcv4vfffy9xv+bNm6N58+bq+15eXoiLi8M333yDXr166bpM+p+BAweqb7dt2xZeXl5o0qQJ1q5di+nTpxf5GH7m9NOqVaswcOBA1KlTp9h9+LnTb2X9/6+8jyHdyMnJwahRo5CXl4dly5aVuG+3bt00BvDq3r07PDw88MMPP+D777/XdalUQEBAgPp2mzZt4OnpCTc3N+zdu7fEEMfPnn5ZvXo1AgMDS71Wm589w8UW7wpUq1YtGBsbF/q2MCkpqdC3iiouLi5F7m9iYgJHR0ed1UpFe/vttxEWFoajR4+iXr16ZX58t27d+I2jzKytrdG2bdti3wd+5vTTnTt3cOjQIUyePLnMj+XnTn6qmQTK8v+f6nFlfQzpRk5ODkaOHInbt28jPDy8xNbuohgZGaFz5878LOoBV1dXuLm5lfhe8LOnXyIiInD9+vVy/R/Iz57hYPCuQGZmZujUqZN6VF6V8PBweHt7F/kYLy+vQvsfPHgQnp6eMDU11VmtpEmSJEydOhU7duzAkSNH0KhRo3IdJyoqCq6urhVcHZVFVlYWrl69Wuz7wM+cflqzZg1q166NQYMGlfmx/NzJr1GjRnBxcdH4bGVnZ+P48ePF/v8HFP95LOkxVPFUofvmzZs4dOhQub6ElCQJ0dHR/CzqgZSUFMTFxZX4XvCzp19WrVqFTp06oX379mV+LD97BkSuUd2qqi1btkimpqbSqlWrpCtXrkjTpk2TrK2tpZiYGEmSJGnmzJnSuHHj1Pv/888/kpWVlfTee+9JV65ckVatWiWZmppKv/zyi1xPoVp64403JHt7e+nYsWNSQkKCesnIyFDv8+x7991330k7d+6Ubty4If3111/SzJkzJQDS9u3b5XgK1db7778vHTt2TPrnn3+kP//8Uxo8eLBka2vLz5wBUSqVUoMGDaSPPvqo0DZ+7vTH48ePpaioKCkqKkoCIC1atEiKiopSj3y9YMECyd7eXtqxY4d06dIlafTo0ZKrq6uUlpamPsa4ceM0Zvn4448/JGNjY2nBggXS1atXpQULFkgmJibSn3/+WenPryor6b3LycmRhgwZItWrV0+Kjo7W+D8wKytLfYxn37s5c+ZI+/fvl27duiVFRUVJEydOlExMTKTTp0/L8RSrtJLev8ePH0vvv/++dPLkSen27dvS0aNHJS8vL6lu3br87OmB0v7dlCRJSk1NlaysrKTly5cXeQx+9qoOBm8dWLp0qeTm5iaZmZlJHh4eGlNSTZgwQerdu7fG/seOHZM6duwomZmZSQ0bNiz2g0e6A6DIZc2aNep9nn3vvvrqK6lJkyaShYWF5ODgIPXo0UPau3dv5RdfzQUEBEiurq6SqampVKdOHWn48OHS5cuX1dv5mdN/Bw4ckABI169fL7SNnzv9oZrK7dllwoQJkiSJKcVmz54tubi4SObm5lKvXr2kS5cuaRyjd+/e6v1Vtm3bJjVv3lwyNTWVWrRowS9RdKCk9+727dvF/h949OhR9TGefe+mTZsmNWjQQDIzM5OcnJwkHx8f6eTJk5X/5KqBkt6/jIwMycfHR3JycpJMTU2lBg0aSBMmTJBiY2M1jsHPnjxK+3dTkiTpxx9/lCwtLaVHjx4VeQx+9qoOhST9b1QhIiIiIiIiIqpwvMabiIgqjUKh0Go5duzYc51nzpw5Bjs6b0hICBQKBWJiYordp2PHjqhbty6USmWx+3Tv3h21atVCdna2VueNiYmBQqFASEhImWpR6dOnD/r06aPVuZ715ZdfYteuXYXWHzt2rEJ+H8ojKCgINjY2lX5eIiKqmhi8iYio0pw6dUpj8fPzg6WlZaH1Hh4ez3WeyZMn49SpUxVUtf4JDg5GfHw8Dhw4UOT2Gzdu4OTJkxg3bhzMzMzKfZ5Bgwbh1KlTOh+0p7jg7eHhUSG/D0RERHLjPN5ERFRpCs49CgBOTk4wMjIqtP5ZGRkZsLKy0vo89erVK9eUgIYiMDAQH374IVavXg0/P79C21evXg0AmDRp0nOdx8nJCU5OTs91jOdhZ2dX6u8GERGRIWCLNxER6ZU+ffqgTZs2OHHiBLy9vWFlZaUOkKGhofDx8YGrqyssLS3RsmVLzJw5E+np6RrHKKqrecOGDTF48GDs378fHh4esLS0RIsWLdQhtTRz585F165dUbNmTdjZ2cHDwwOrVq3Cs0OllOU8f/75J7p37w4LCwvUqVMHs2bNQk5OTqm1ODg4YNiwYdizZw9SUlI0timVSqxfvx6dO3dG27Zt8ffff2PixIlwd3eHlZUV6tatC39/f1y6dKnU8xTV1VySJCxcuBBubm6wsLCAh4cHfvvtt0KPzczMxPvvv48OHTrA3t4eNWvWhJeXF3bv3q2xn0KhQHp6OtauXau+1EDVZb24ruZhYWHw8vKClZUVbG1t0b9//0I9HFS/A5cvX8bo0aNhb28PZ2dnTJo0CampqaU+d22tXr0a7du3h4WFBWrWrIlhw4bh6tWrGvv8888/GDVqFOrUqQNzc3M4OzujX79+iI6OVu9z5MgR9OnTB46OjrC0tESDBg0wYsQIZGRkVFitREQkHwZvIiLSOwkJCRg7dizGjBmDffv24c033wQA3Lx5E35+fli1ahX279+PadOmYevWrfD399fquBcuXMD777+P9957D7t370a7du0QHByMEydOlPrYmJgYvP7669i6dSt27NiB4cOH4+2338bnn39ervNcuXIF/fr1w6NHjxASEoIVK1YgKioKX3zxhVbPJTg4GNnZ2diwYYPG+gMHDiA+Ph7BwcEAgPj4eDg6OmLBggXYv38/li5dChMTE3Tt2hXXr1/X6lwFzZ07Fx999BH69++PXbt24Y033sCrr75a6FhZWVl48OABPvjgA+zatQubN29Gjx49MHz4cKxbt06936lTp2BpaQk/Pz/1pQbLli0r9vybNm3C0KFDYWdnh82bN2PVqlV4+PAh+vTpg99//73Q/iNGjECzZs2wfft2zJw5E5s2bcJ7771X5uddlPnz5yM4OBitW7fGjh07sGTJEly8eBFeXl64efOmej8/Pz+cO3cOCxcuRHh4OJYvX46OHTvi0aNHAMTv1qBBg2BmZobVq1dj//79WLBgAaytrbW+Rp+IiPScvIOqExFRdTZhwgTJ2tpaY13v3r0lANLhw4dLfGxeXp6Uk5MjHT9+XAIgXbhwQb1t9uzZ0rP/xbm5uUkWFhYa86c+ffpUqlmzpvT666+XqW6lUinl5ORI8+bNkxwdHaW8vLwynycgIECytLSUEhMT1etyc3OlFi1aSACk27dvl/r8GzVqJLVr105j/YgRIyQrKyspNTW1yMfl5uZK2dnZkru7u/Tee++p16umlSo4jeKaNWs0ann48KFkYWEhDRs2TOOYf/zxhwSg0NR9z543JydHCg4Oljp27KixzdrautBUR5KUPxWPalorpVIp1alTR2rbtq2kVCrV+z1+/FiqXbu25O3trV6n+h1YuHChxjHffPNNycLCQuM9K0pRv5sFPXz4ULK0tJT8/Pw01sfGxkrm5ubSmDFjJEmSpOTkZAmAtHjx4mKP9csvv0gApOjo6BJrIiIiw8UWbyIi0jsODg7o27dvofX//PMPxowZAxcXFxgbG8PU1BS9e/cGgELde4vSoUMHNGjQQH3fwsICzZo1w507d0p97JEjR/Diiy/C3t5efe7PPvsMKSkpSEpKKvN5jh49in79+sHZ2Vm9ztjYGAEBAaXWAogu2hMnTsTFixdx7tw5AEBKSgr27NmDESNGwM7ODgCQm5uLL7/8Eq1atYKZmRlMTExgZmaGmzdvavWaFXTq1ClkZmYiMDBQY723tzfc3NwK7b9t2zZ0794dNjY2MDExgampKVatWlXm86pcv34d8fHxGDduHIyM8v+EsbGxwYgRI/Dnn38W6po9ZMgQjfvt2rVDZmZmofesrE6dOoWnT58iKChIY339+vXRt29fHD58GABQs2ZNNGnSBF9//TUWLVqEqKgo5OXlaTymQ4cOMDMzw2uvvYa1a9fin3/+ea7aiIhI/zB4ExGR3ilqFO0nT56gZ8+eOH36NL744gscO3YMkZGR2LFjBwDg6dOnpR7X0dGx0Dpzc/NSH3vmzBn4+PgAAFauXIk//vgDkZGR+Pjjj4s8tzbnSUlJgYuLS6H9ilpXnIkTJ8LIyAhr1qwBAGzcuBHZ2dnqbuYAMH36dHz66ad46aWXsGfPHpw+fRqRkZFo3769Vq9ZQarrybWpe8eOHRg5ciTq1q2LDRs24NSpU4iMjMSkSZOQmZlZpvM+e/6ifj/q1KmDvLw8PHz4UGP9s++Fubk5AO1+X56nFtV2hUKBw4cPw9fXFwsXLoSHhwecnJzwzjvv4PHjxwCAJk2a4NChQ6hduzbeeustNGnSBE2aNMGSJUueq0YiItIfHNWciIj0TlFzcB85cgTx8fE4duyYupUbgPo6WV3asmULTE1N8euvv8LCwkK9vqgpsLTl6OiIxMTEQuuLWlecevXqwcfHB5s2bcK3336LNWvWoGnTpujVq5d6nw0bNmD8+PH48ssvNR6bnJyMGjVqlLnm4mpMTExEw4YNNc7bqFEjhIaGaryfWVlZZTpnUedPSEgotC0+Ph5GRkZwcHAo9/ErspZatWqp77u5uWHVqlUAxFRvW7duxZw5c5CdnY0VK1YAAHr27ImePXtCqVTi7Nmz+OGHHzBt2jQ4Oztj1KhRlfCMiIhIl9jiTUREBkEV3lQtlio//vhjpZzbxMQExsbG6nVPnz7F+vXry33MF154AYcPH8b9+/fV65RKJUJDQ8t0nODgYDx8+BCfffYZoqOjMXHiRI2gq1AoCr1me/fuxb1798pcc7du3WBhYYGNGzdqrD958mSh7voKhQJmZmYatSQmJhYa1RzQrtcBADRv3hx169bFpk2bNEaTT09Px/bt29UjnVcGLy8vWFpaFhrc7u7duzhy5Aj69etX5OOaNWuGTz75BG3btsX58+cLbTc2NkbXrl2xdOlSAChyHyIiMjxs8SYiIoPg7e0NBwcHTJkyBbNnz4apqSk2btyICxcu6PzcgwYNwqJFizBmzBi89tprSElJwTfffFMo0JbFJ598grCwMPTt2xefffYZrKyssHTp0kJTo5VmyJAhqFWrFr7++msYGxtjwoQJGtsHDx6MkJAQtGjRAu3atcO5c+fw9ddfl2uecwcHB3zwwQf44osvMHnyZLzyyiuIi4vDnDlzCnU1Hzx4MHbs2IE333wTL7/8MuLi4vD555/D1dVVY8RvAGjbti2OHTuGPXv2wNXVFba2tmjevHmh8xsZGWHhwoUIDAzE4MGD8frrryMrKwtff/01Hj16hAULFpT5OZVEqVTil19+KbTe2toaAwcOxKeffor/+7//w/jx4zF69GikpKRg7ty5sLCwwOzZswEAFy9exNSpU/HKK6/A3d0dZmZmOHLkCC5evIiZM2cCAFasWIEjR45g0KBBaNCgATIzM9XTz7344osV+pyIiEgeDN5ERGQQHB0dsXfvXrz//vsYO3YsrK2tMXToUISGhsLDw0On5+7bty9Wr16Nr776Cv7+/qhbty5effVV1K5dW+N66rJo06YNDh06hPfffx8TJkyAg4MDxo0bhxEjRuC1117T+jhmZmYYN24cvvvuO/j6+qJu3boa25csWQJTU1PMnz8fT548gYeHB3bs2IFPPvmkXHXPmzcP1tbWWLZsGdavX48WLVpgxYoV+OabbzT2mzhxIpKSkrBixQqsXr0ajRs3xsyZM3H37l3MnTu3UI1vvfUWRo0ahYyMDPTu3bvQ3N0qY8aMgbW1NebPn4+AgAAYGxujW7duOHr0KLy9vcv1nIqTmZmJV155pdB6Nzc3xMTEYNasWahduza+//57hIaGwtLSEn369MGXX34Jd3d3AOLa9yZNmmDZsmWIi4uDQqFA48aN8e233+Ltt98GIAZXO3jwIGbPno3ExETY2NigTZs2CAsLU48tQEREhk0hFeyrRUREREREREQVitd4ExEREREREekQgzcRERERERGRDjF4ExEREREREekQgzcRERERERGRDjF4ExEREREREekQgzcRERERERGRDlWZebzz8vIQHx8PW1tbKBQKucshIiIiIiKiKk6SJDx+/Bh16tSBkVHx7dpVJnjHx8ejfv36cpdBRERERERE1UxcXBzq1atX7PYqE7xtbW0BiCdsZ2cnczVERERERERU1aWlpaF+/frqPFqcKhO8Vd3L7ezsGLyJiIiIiIio0pR2uTMHVyMiIiIiIiLSIQZvIiIiIiIiIh1i8CYiIiIiIiLSoSpzjTcREREREZG+UiqVyMnJkbsMKiNTU1MYGxs/93EYvImIiIiIiHREkiQkJibi0aNHcpdC5VSjRg24uLiUOoBaSRi8K1tsLHDxIjB4sNyVEBERERGRjqlCd+3atWFlZfVc4Y0qlyRJyMjIQFJSEgDA1dW13Mdi8K5MFy4AHTsCNjZAQgJgbS13RUREREREpCNKpVIduh0dHeUuh8rB0tISAJCUlITatWuXu9s5B1erTO3aAY0bA48fA9u3y10NERERERHpkOqabisrK5kroeehev+e5xp9Bu/KpFAAEyeK26tXy1sLERERERFVCnYvN2wV8f4xeFe2CRNEAD9+HPj7b7mrISIiIiIiIh1j8K5s9eoBvr7i9po18tZCRERERERUCRo2bIjFixfLfgy5MHjLYdIk8TMkBFAqZS2FiIiIiIjoWX369MG0adMq7HiRkZF47bXXKux4hobBWw5DhgCOjkB8PHDwoNzVEBERERERlZkkScjNzdVqXycnp2o9yByDtxzMzYGxY8VtDrJGRERERER6JCgoCMePH8eSJUugUCigUCgQExODY8eOQaFQ4MCBA/D09IS5uTkiIiJw69YtDB06FM7OzrCxsUHnzp1x6NAhjWM+201coVDg559/xrBhw2BlZQV3d3eEhYWVqc7Y2FgMHToUNjY2sLOzw8iRI3H//n319gsXLuCFF16Ara0t7Ozs0KlTJ5w9exYAcOfOHfj7+8PBwQHW1tZo3bo19u3bV/4XrRQM3nJRjW6+ezeQnCxvLUREREREVDkkCUhPl2eRJK1KXLJkCby8vPDqq68iISEBCQkJqF+/vnr7jBkzMH/+fFy9ehXt2rXDkydP4Ofnh0OHDiEqKgq+vr7w9/dHbGxsieeZO3cuRo4ciYsXL8LPzw+BgYF48OCBli+jhJdeegkPHjzA8ePHER4ejlu3biEgIEC9T2BgIOrVq4fIyEicO3cOM2fOhKmpKQDgrbfeQlZWFk6cOIFLly7hq6++go2NjVbnLg8TnR2ZSta+PdCpE3DuHLBxI/Duu3JXREREREREupaRAegw4JXoyRPA2rrU3ezt7WFmZgYrKyu4uLgU2j5v3jz0799ffd/R0RHt27dX3//iiy+wc+dOhIWFYerUqcWeJygoCKNHjwYAfPnll/jhhx9w5swZDBgwoNQaDx06hIsXL+L27dvqLwXWr1+P1q1bIzIyEp07d0ZsbCw+/PBDtGjRAgDg7u6ufnxsbCxGjBiBtm3bAgAaN25c6jmfB1u85aQaZG3VKq2/fSIiIiIiIpKTp6enxv309HTMmDEDrVq1Qo0aNWBjY4Nr166V2uLdrl079W1ra2vY2toiKSlJqxquXr2K+vXra7TEq85/9epVAMD06dMxefJkvPjii1iwYAFu3bql3vedd97BF198ge7du2P27Nm4ePGiVuctLwZvOY0eLa73vnRJtHwTEREREVHVZmUlWp7lWCpocDPrZ1rNP/zwQ2zfvh3/+c9/EBERgejoaLRt2xbZ2dklHkfV7VtFoVAgLy9PqxokSYJCoShx/Zw5c3D58mUMGjQIR44cQatWrbBz504AwOTJk/HPP/9g3LhxuHTpEjw9PfHDDz9ode7yYPCWk4MDMGKEuM1B1oiIiIiIqj6FQnT3lmMpIqgWx8zMDEotpz6OiIhAUFAQhg0bhrZt28LFxQUxMTHlfIG006pVK8TGxiIuLk697sqVK0hNTUXLli3V65o1a4b33nsPBw8exPDhw7FmzRr1tvr162PKlCnYsWMH3n//faxcuVJn9TJ4y03V3XzTJuDpU3lrISIiIiIighiF/PTp04iJiUFycnKJLdFNmzbFjh07EB0djQsXLmDMmDFat1yX14svvoh27dohMDAQ58+fx5kzZzB+/Hj07t0bnp6eePr0KaZOnYpjx47hzp07+OOPPxAZGakO5dOmTcOBAwdw+/ZtnD9/HkeOHNEI7BWNwVtuL7wAuLkBqanA/7o9EBERERERyemDDz6AsbExWrVqBScnpxKv1/7uu+/g4OAAb29v+Pv7w9fXFx4eHjqtT6FQYNeuXXBwcECvXr3w4osvonHjxggNDQUAGBsbIyUlBePHj0ezZs0wcuRIDBw4EHPnzgUAKJVKvPXWW2jZsiUGDBiA5s2bY9myZbqrV5KqxqheaWlpsLe3R2pqKuzs7OQup2zmzgXmzAH69QOeme+OiIiIiIgMU2ZmJm7fvo1GjRrBwsJC7nKonEp6H7XNoWzx1gdBQeJ6i8OHAR1fC0FERERERESVi8FbH7i5idZuAChwsT8REREREREZPp0F72XLlqmb4jt16oSIiIhi9z127BgUCkWh5dq1a7oqT/+oBllbswbQcvRAIiIiIiIi0n86Cd6hoaGYNm0aPv74Y0RFRaFnz54YOHBgqROoX79+HQkJCerF3d1dF+Xpp2HDgBo1gLg44MgRuashIiIiIiKiCqKT4L1o0SIEBwdj8uTJaNmyJRYvXoz69etj+fLlJT6udu3acHFxUS/Gxsa6KE8/WVgAgYHiNuf0JiIiIiIiqjIqPHhnZ2fj3Llz8PHx0Vjv4+ODkydPlvjYjh07wtXVFf369cPRo0dL3DcrKwtpaWkai8FTdTffuRN48EDeWoiIiIiIiKhCVHjwTk5OhlKphLOzs8Z6Z2dnJCYmFvkYV1dX/PTTT9i+fTt27NiB5s2bo1+/fjhx4kSx55k/fz7s7e3VS/369Sv0eciiY0egfXsgKwvYvFnuaoiIiIiIiKgC6GxwNYVCoXFfkqRC61SaN2+OV199FR4eHvDy8sKyZcswaNAgfPPNN8Uef9asWUhNTVUvcXFxFVq/LBSK/FbvVavkrYWIiIiIiIgqRIUH71q1asHY2LhQ63ZSUlKhVvCSdOvWDTdv3ix2u7m5Oezs7DSWKiEwEDAzA6KixEJEREREREQGrcKDt5mZGTp16oTw8HCN9eHh4fD29tb6OFFRUXB1da3o8vSfoyPw0kviNuf0JiIiIiIiA9WwYUMsXry42O1BQUF4SZV9qjgTXRx0+vTpGDduHDw9PeHl5YWffvoJsbGxmDJlCgDRTfzevXtYt24dAGDx4sVo2LAhWrdujezsbGzYsAHbt2/H9u3bdVGe/ps0Cdi6FdiwAVi4UIx4TkRERERERAZJJ8E7ICAAKSkpmDdvHhISEtCmTRvs27cPbm5uAICEhASNOb2zs7PxwQcf4N69e7C0tETr1q2xd+9e+Pn56aI8/ffii0C9esDdu0BYGDBypNwVERERERERUTnpbHC1N998EzExMcjKysK5c+fQq1cv9baQkBAcO3ZMfX/GjBn4+++/8fTpUzx48AARERHVN3QDgLExEBQkbnNObyIiIiIiqkQ//vgj6tati7y8PI31Q4YMwYQJEwAAt27dwtChQ+Hs7AwbGxt07twZhw4deq7zZmVl4Z133kHt2rVhYWGBHj16IDIyUr394cOHCAwMhJOTEywtLeHu7o41/7s8Nzs7G1OnToWrqyssLCzQsGFDzJ8//7nqqUg6C970nFTB++BBoCqM2E5ERERERJAkID1dnkWStKvxlVdeQXJyMo4ePape9/DhQxw4cACBgYEAgCdPnsDPzw+HDh1CVFQUfH194e/vr9GzuaxmzJiB7du3Y+3atTh//jyaNm0KX19fPHjwAADw6aef4sqVK/jtt99w9epVLF++HLVq1QIAfP/99wgLC8PWrVtx/fp1bNiwAQ0bNix3LRVNJ13NqQI0aQL06QMcOwaEhACffipzQURERERE9LwyMgAbG3nO/eQJYG1d+n41a9bEgAEDsGnTJvTr1w8AsG3bNtSsWVN9v3379mjfvr36MV988QV27tyJsLAwTJ06tcy1paenY/ny5QgJCcHAgQMBACtXrkR4eDhWrVqFDz/8ELGxsejYsSM8PT0BQCNYx8bGwt3dHT169IBCoVBf5qwv2OKtz1Rzeq9ZAzzTzYOIiIiIiEhXAgMDsX37dmRlZQEANm7ciFGjRsHY2BiACMozZsxAq1atUKNGDdjY2ODatWvlbvG+desWcnJy0L17d/U6U1NTdOnSBVevXgUAvPHGG9iyZQs6dOiAGTNm4OTJk+p9g4KCEB0djebNm+Odd97BwYMHy/vUdYLBW5+NGAHY2QG3bwPHj8tdDRERERERPScrK9HyLMdiZaV9nf7+/sjLy8PevXsRFxeHiIgIjB07Vr39ww8/xPbt2/Gf//wHERERiI6ORtu2bZGdnV2u10X6Xz94hUJRaL1q3cCBA3Hnzh1MmzYN8fHx6NevHz744AMAgIeHB27fvo3PP/8cT58+xciRI/Hyyy+XqxZdYPDWZ1ZWwOjR4jYHWSMiIiIiMngKhejuLcfyTKYtkaWlJYYPH46NGzdi8+bNaNasGTp16qTeHhERgaCgIAwbNgxt27aFi4sLYmJiyv26NG3aFGZmZvj999/V63JycnD27Fm0bNlSvc7JyQlBQUHYsGEDFi9ejJ9++km9zc7ODgEBAVi5ciVCQ0Oxfft29fXhcuM13vpu0iTgxx+BX34B/vtfwN5e7oqIiIiIiKgaCAwMhL+/Py5fvqzR2g2IoLxjxw74+/tDoVDg008/LTQKellYW1vjjTfewIcffoiaNWuiQYMGWLhwITIyMhAcHAwA+Oyzz9CpUye0bt0aWVlZ+PXXX9Wh/LvvvoOrqys6dOgAIyMjbNu2DS4uLqhRo0a5a6pIDN76rnNnoHVr4PJlYMsW4PXX5a6IiIiIiIiqgb59+6JmzZq4fv06xowZo7Htu+++w6RJk+Dt7Y1atWrho48+Qlpa2nOdb8GCBcjLy8O4cePw+PFjeHp64sCBA3BwcAAAmJmZYdasWYiJiYGlpSV69uyJLVu2AABsbGzw1Vdf4ebNmzA2Nkbnzp2xb98+GBnpRydvhSRpO6i8fktLS4O9vT1SU1NhZ2cndzkVa9Ei4P33RQg/c0buaoiIiIiISAuZmZm4ffs2GjVqBAsLC7nLoXIq6X3UNofqR/ynko0dC5iYAJGRwKVLcldDREREREREZcDgbQhq1waGDBG316yRtxYiIiIiIiIqEwZvQ6Ga03v9eqCcQ/QTERERERFR5WPwNhS+voCrK5CcDPz6q9zVEBERERERkZYYvA2FiQkwYYK4zTm9iYiIiIiIDAaDtyGZOFH8/O03ID5e3lqIiIiIiEgrzzO/NcmvIt4/zuNtSJo1A3r0AH7/HVi7Fpg1S+6KiIiIiIioGGZmZjAyMkJ8fDycnJxgZmYGhUIhd1mkJUmSkJ2djX///RdGRkYwMzMr97E4j7ehCQkRLd9NmwI3bgD84BIRERER6a3s7GwkJCQgIyND7lKonKysrODq6lpk8NY2hzJ4G5onT8Qga0+eACdOAD17yl0RERERERGVQJIk5ObmQqlUyl0KlZGxsTFMTEyK7amgbQ5lV3NDY2MDBAQAq1aJQdYYvImIiIiI9JpCoYCpqSlMTU3lLoVkwsHVDJFqTu+tW4HHj+WthYiIiIiIiErE4G2IvLyA5s2BjAwRvomIiIiIiEhvMXgbIoUiv9V71Sp5ayEiIiIiIqISMXgbqvHjAWNj4NQp4OpVuashIiIiIiKiYjB4GyoXF2DQIHF7zRp5ayEiIiIiIqJiMXgbMlV383XrgJwceWshIiIiIiKiIjF4GzI/P6B2beD+feC33+SuhoiIiIiIiIrA4G3ITE3Ftd6AmNObiIiIiIiI9I7OgveyZcvQqFEjWFhYoFOnToiIiNDqcX/88QdMTEzQoUMHXZVWtUycKH7++iuQmChvLURERERERFSIToJ3aGgopk2bho8//hhRUVHo2bMnBg4ciNjY2BIfl5qaivHjx6Nfv366KKtqatUK6NYNUCqB9evlroaIiIiIiIieoZPgvWjRIgQHB2Py5Mlo2bIlFi9ejPr162P58uUlPu7111/HmDFj4OXlpYuyqq7gYPFz9WpAkuSthYiIiIiIiDRUePDOzs7GuXPn4OPjo7Hex8cHJ0+eLPZxa9aswa1btzB79mytzpOVlYW0tDSNpdoaORKwsgKuXQP+/FPuaoiIiIiIiKiACg/eycnJUCqVcHZ21ljv7OyMxGKuQb558yZmzpyJjRs3wsTERKvzzJ8/H/b29uqlfv36z127wbKzA155RdzmIGtERERERER6RWeDqykUCo37kiQVWgcASqUSY8aMwdy5c9GsWTOtjz9r1iykpqaql7i4uOeu2aCp5vTesgVIT5e3FiIiIiIiIlLTrnm5DGrVqgVjY+NCrdtJSUmFWsEB4PHjxzh79iyioqIwdepUAEBeXh4kSYKJiQkOHjyIvn37Fnqcubk5zM3NK7p8w9WzJ9C0KfD338AvvwATJshdEREREREREUEHLd5mZmbo1KkTwsPDNdaHh4fD29u70P52dna4dOkSoqOj1cuUKVPQvHlzREdHo2vXrhVdYtWkUORPLcbu5kRERERERHqjwlu8AWD69OkYN24cPD094eXlhZ9++gmxsbGYMmUKANFN/N69e1i3bh2MjIzQpk0bjcfXrl0bFhYWhdZTKSZMAD79FDhxArh5E3B3l7siIiIiIiKiak8nwTsgIAApKSmYN28eEhIS0KZNG+zbtw9ubm4AgISEhFLn9KZyqFsXGDAA2LcPWLMG+PJLuSsiIiIiIiKq9hSSVDUmfk5LS4O9vT1SU1NhZ2cndzny2b4dePlloE4d4M4dQMtR4omIiIiIiKhstM2hOhvVnGTi7w/UqgXExwMHD8pdDRERERERUbXH4F3VmJkBY8eK2xxkjYiIiIiISHYM3lWRak7vsDDg33/lrYWIiIiIiKiaY/Cuitq2BTw9gZwcYMMGuashIiIiIiKq1hi8q6rgYPFz1SqgaoyfR0REREREZJAYvKuqUaMACwvg8mXg7Fm5qyEiIiIiIqq2GLyrqho1gBEjxG0OskZERERERCQbBu+qTDXI2qZNQEaGvLUQERERERFVUwzeVVmfPkDDhkBaGrBzp9zVEBERERERVUsM3lWZkREwcaK4ze7mREREREREsmDwruqCggCFAjhyBPjnH7mrISIiIiIiqnYYvKu6Bg2A/v3F7ZAQWUshIiIiIiKqjhi8qwPVIGshIYBSKWspRERERERE1Q2Dd3UwdCjg4ADExQGHD8tdDRERERERUbXC4F0dWFgAgYHiNgdZIyIiIiIiqlQM3tWFqrv5zp3Agwfy1kJERERERFSNMHhXFx07Ah06ANnZwMaNcldDRERERERUbTB4VyfBweInu5sTERERERFVGgbv6mTMGMDMDIiOBqKi5K6GiIiIiIioWmDwrk5q1gSGDRO32epNRERERERUKRi8qxvVIGsbNwKZmfLWQkREREREVA0weFc3/foB9esDDx8Cu3fLXQ0REREREVGVx+Bd3RgbA0FB4ja7mxMREREREekcg3d1NHGi+BkeDty5I28tREREREREVRyDd3XUqBHQty8gScDatXJXQ0REREREVKUxeFdXqkHW1qwB8vLkrYWIiIiIiKgK01nwXrZsGRo1agQLCwt06tQJERERxe77+++/o3v37nB0dISlpSVatGiB7777TlelEQAMHw7Y2wMxMcCxY3JXQ0REREREVGXpJHiHhoZi2rRp+PjjjxEVFYWePXti4MCBiI2NLXJ/a2trTJ06FSdOnMDVq1fxySef4JNPPsFPP/2ki/IIACwtgdGjxW0OskZERERERKQzCkmSpIo+aNeuXeHh4YHly5er17Vs2RIvvfQS5s+fr9Uxhg8fDmtra6xfv16r/dPS0mBvb4/U1FTY2dmVq+5qJzIS6NIFsLAAEhKAGjXkroiIiIiIiMhgaJtDK7zFOzs7G+fOnYOPj4/Geh8fH5w8eVKrY0RFReHkyZPo3bt3sftkZWUhLS1NY6Ey8vQE2rYFMjOBzZvlroaIiIiIiKhKqvDgnZycDKVSCWdnZ431zs7OSExMLPGx9erVg7m5OTw9PfHWW29h8uTJxe47f/582Nvbq5f69etXSP3VikKRP8gau5sTERERERHphM4GV1MoFBr3JUkqtO5ZEREROHv2LFasWIHFixdjcwmtsLNmzUJqaqp6iYuLq5C6q53AQMDUFDh7Frh4Ue5qiIiIiIiIqhyTij5grVq1YGxsXKh1OykpqVAr+LMaNWoEAGjbti3u37+POXPmYLRqALBnmJubw9zcvGKKrs6cnIAhQ4Dt28XUYhxNnoiIiIiIqEJVeIu3mZkZOnXqhPDwcI314eHh8Pb21vo4kiQhKyurosujoqi6m69fD2Rny1sLERERERFRFVPhLd4AMH36dIwbNw6enp7w8vLCTz/9hNjYWEyZMgWA6CZ+7949rFu3DgCwdOlSNGjQAC1atAAg5vX+5ptv8Pbbb+uiPHqWjw9Qpw4QHw/s2QOMGCF3RURERERERFWGToJ3QEAAUlJSMG/ePCQkJKBNmzbYt28f3NzcAAAJCQkac3rn5eVh1qxZuH37NkxMTNCkSRMsWLAAr7/+ui7Ko2eZmAATJgDz54tB1hi8iYiIiIiIKoxO5vGWA+fxfk5//w24uwNGRkBsLFC3rtwVERERERER6TXZ5vEmA9W0KdCrF5CXB6xdK3c1REREREREVQaDN+UrOKd31egIQUREREREJDsGb8r38suAjQ1w6xYQESF3NURERERERFUCgzfls7YGRo0St1evlrcWIiIiIiKiKoLBmzSpuptv2wakpclbCxERERERURXA4E2aunUDWrYEMjKArVvlroaIiIiIiMjgMXiTJoUiv9V71Sp5ayEiIiIiIqoCGLypsHHjAGNj4M8/gStX5K6GiIiIiIjIoDF4U2HOzsDgweL2mjXy1kJERERERGTgGLypaKru5uvWATk58tZCRERERERkwBi8qWgDB4qW76QkYN8+uashIiIiIiIyWAzeVDRTU2D8eHGbc3oTERERERGVG4N3JTt6FEhJkbsKLam6m+/dCyQkyFsLERERERGRgWLwrkRPnwJDhwKursCQIUBoqJguW2+1aAF4ewNKpQjh9+/LXREREREREZHBYfCuRHFxQJMmYqyyPXuAUaPEZdRBQUB4uMi3euezzwAzM2D/fqBNG2D7drkrIiIiIiIiMigM3pWoWTMgKgr46y9g1izAzQ148gRYuxbw8QHq1QOmTwfOnQMkSe5q/8fXF4iMBNq1A5KTgZdfFvN8P3okd2VEREREREQGQSFJehPxnktaWhrs7e2RmpoKOzs7ucvRSl4ecPIksHEjsHUr8OBB/rbmzYHAQLE0bixfjWrZ2cDcucCCBaLwevXEoGv9+8tdGRERERERkSy0zaEM3noiO1v05t64EQgLAzIz87d5eYkAPnIk4OQkX40AgFOngAkTgJs3xf233gK++gqwtpa3LiIiIiIiokrG4G3A0tKAnTtFCD98WDQwA4CJieiSHhgoBmmTLeumpwMffQQsXSruN20KrFsnviEgIiIiIiKqJhi8q4iEBGDLFhHCz53LX29tDQwbJkL4iy+KUF7pwsOBiROBe/cAIyNg5kxg9mwxGBsREREREVEVx+BdBV27JgL4pk3AP//kr69dW4yQHhgIdO4MKBSVWNTDh8A77wAbNoj77dsD69cDbdtWYhFERERERESVj8G7CpMk4M8/RQgPDRWDjas0bZo/KJu7eyUWtX078PrrQEqKaPH+/HPg/fcBY+NKLIKIiIiIiKjyMHhXEzk5osf3hg3Arl3A06f527p0EQE8IEDMF65z9+8Dr74qJikHgO7dxVxpTZpUwsmJSE7//isGiBwyBLC3l7saIiIiosqhbQ7lPN4GztQU8PMT3c+TkkQv7wEDREPzmTPAu+8CdeuKdRs2iHnDdcbZGdi9W0wzZmsL/PGH6Hq+YoUeTUxORBXtxAnxUR8/HujVS3wHR0RERET52OJdRd2/L7qhb9woAriKlZUYET0wUIyQbmqqowJiYoCgIOD4cXF/wADg55/FtwBEVCXk5QELFwIff5w/+wIgLnM5dAho0EC+2oiIiIgqA1u8qzlnZzHm2enTwI0bwJw54vrvjAxg82Zg8GCgTh1g6lQxNXeFf/3SsCFw5Ajw3XeAhYXog9q2rRiinYgMXkoK4O8PzJolQve4cUB0NODmBty8CfToIf7tISIiIiIdBu9ly5ahUaNGsLCwQKdOnRAREVHsvjt27ED//v3h5OQEOzs7eHl54cCBA7oqrdpxdxezfN24IYL4O++IkdCTk8VU3N7eIpR/+qkYOb3CGBkB06YB588Dnp5iBPTRo8VF5ykpFXgiIqpMf/4JdOwI7NsnvldbuVIM59C+PfD770CLFkBcHNCzJ3DhgtzVEhEREclPJ8E7NDQU06ZNw8cff4yoqCj07NkTAwcORGxsbJH7nzhxAv3798e+fftw7tw5vPDCC/D390dUVJQuyqu2FAox4NqSJWLq7f37RSuVtbWYnuyLL4CWLUVG/u47MYd4hWjZEjh5UjS7GxsDW7cCbdqIv9qJyGBIkvi3oWdPEazd3UUInzw5fxrDevXENd8dO4pxJ/r0Eb1qiIiIiKoznVzj3bVrV3h4eGD58uXqdS1btsRLL72E+fPna3WM1q1bIyAgAJ999plW+/Ma7/JLTwfCwsT14AcOALm5Yr2REdC3r7gefNAgwMmpAk529qwYgenqVXH/1VeBb78Vg7ERkd569AiYOFHMngAAI0eKlu7i/rl99Ehc0vLHH+LLvV27gBdfrJxaiYiIiCqLbNd4Z2dn49y5c/Dx8dFY7+Pjg5MnT2p1jLy8PDx+/Bg1a9Ysdp+srCykpaVpLFQ+1taiB/ivvwLx8aL7uZeXuG7z0CHxx7azs2gJ/7//E+OlZWeX82SensC5c8B774kmspUrRf/UEi5FICJ5nTsHeHiI8GxmJv6N2LKl+NANADVqiC/yfHzEl3uDBolJD4iIiIiqowoP3snJyVAqlXB+ZuJoZ2dnJCYmanWMb7/9Funp6Rg5cmSx+8yfPx/29vbqpX79+s9VNwlOTsCbb4qe4bduAZ9/LnKxJIk/vufPF11HHR3FfL1Ll4qBlMrUb8LSEli0CDh6VIzEdPs20Ls38OGHQGamrp4aEZWRJAHLlolxIG7fFmMm/vGH+DdC1bW8JNbWojfN8OHiy7oRI8S0hkRERETVjc4GV1M881eZJEmF1hVl8+bNmDNnDkJDQ1G7du1i95s1axZSU1PVS1xc3HPXTJoaNwY++USMVBwfD6xbJ7qdOzmJ+cD37BGjojdrBjRpAkyZAuzcCaSmanmC3r2BixeB4GDxF/4334gW8fPndfm0iEgLjx+LnjBvvSVC89Ch+eMkloW5uZjacMIEQKkU40osW6abmomIiIj0VYUH71q1asHY2LhQ63ZSUlKhVvBnhYaGIjg4GFu3bsWLpVwMaG5uDjs7O42FdMfVVfzBvGEDkJgo/gCfPx944QUxF/jt28CPP4qWLUdHMZXQvHliFHWlsoQD29mJ+b3DwkR/9suXga5dxUhvqovNiahSXbwoAnZoKGBiIjqo7NwJODiU73gmJsDq1cDbb4v7b70FLFhQcfUSERER6bsKD95mZmbo1KkTwsPDNdaHh4fD29u72Mdt3rwZQUFB2LRpEwYNGlTRZVEFMjISIxbPnCmm6n7wQFwf/vbbQPPmImj/8YeYwqxbN9FCPnIksGqVGAm5SP7+wF9/ib6oublibrPu3YHr1yv1uRFVZ5IkPqddu4rpB1UjlKuGZHgeRkZiRoVPPhH3Z80S/4ZU/PCeRERERPpHJ6Oah4aGYty4cVixYgW8vLzw008/YeXKlbh8+TLc3Nwwa9Ys3Lt3D+vWrQMgQvf48eOxZMkSDB8+XH0cS0tL2Nvba3VOjmquP2JigPBwMbDSoUOFu563bCkGXPL1BXr1EteBqkkSsGmT6MP+6JG4Hvyrr0QTmZHOrowgqvbS08W12//7ZxkDB4rbtWpV/Lm++UYM6QAAb7wB/Pe//HgTERGRYdI2h+okeAPAsmXLsHDhQiQkJKBNmzb47rvv0KtXLwBAUFAQYmJicOzYMQBAnz59cPz48ULHmDBhAkJCQrQ6H4O3fsrNBSIjgYMHRRA/fVqMlq5iZibmBFYF8Xbt/teydvcuMGmSSPCAmNdszRqgQQNZngdRVXb1KvDyy8CVKyIAf/EF8NFHug3DP/0kxoWQJDF2xJo14rIVIiIiIkMie/CubAzehuHRI+DwYRHCDxwAYmM1tzs754fw/i9KqL1jBfDBB0BGhrge/PvvxTzgz9vvlYgAiHEbXn9dfMRcXYHNm8W4h5Vh82bxcc7NFYO3bdkCWFhUzrmJiIiIKgKDN+k9SRLXkapaw48eFX/8F9SxI+Dj+QC+f3wG7ysrYY5s4KWXxEhuJYx6T0Qle/oUePddYOVKcb9fP2DjRvHlV2X69VfR2p6VJWrYtQuwsancGgzF33+LudBHjwbq1JG7GiIiIgIYvOUuh8ohK0vMH64K4lFRmtutzbLRJyccPtIB+DpEotmqj6AY9pIstRIZsps3gVdeAS5cEJ1HZs8Wg54ZG8tTz9GjwJAhYprCbt2AffvKP4J6VfT0qZhF4quvxNRuNWuKL0wKDIlCREREMmHwJoOXlCQu8T54UCzPzFCHBrgDX/fb8PmkM/r5W/MPdSItbNsGBAeLebqdnMRYhqXM3lgpzpwBBgwAHj4UYz0cPFj5re/66NdfgXfeEVM2AmKwu+RkcTs4GFi8mD0EiIiI5MTgTVWKJAGXLomW8IP783DieB6ylSbq7UZGErp0UcDXV1wj3qWLmDuYiISsLDFcwn//K+736iWusdanLst//QX07y++ZHN3F7MiVNfxFGNixKUAYWHift26ImQPGQLMmSPmQZckoGlTcYlAly4yFktERFSNMXhTlZaRARxffgUHPz+NA6ldcRWtNLY7OAB+fmJ68AEDAC1npSOqkm7fBkaOBM6eFfdnzQLmzdPPL6f+/lu0wN+5A9SvL8J3s2ZyV1V5srLEdGv/+Y/oYm5iIuZR/+wzzZbt48eBceOAuDhxicCcOeJ9letyASIiouqKwZuqhydPgBkzELd8Dw7CBwdtRuCQkQ8epOUnChMT0brn7y+WJk1krJeoku3eDQQFiRkFatYE1q8XX0rps7t3Rcv3tWtiDMWDB4H27eWuSvfCw4GpU8Wgk4AYXX7pUqB166L3f/hQzIMeGirud+8uRqlv2LBSyiUiIiIweMtdzv+3d+fhUZX3HsC/k2UmC0lYBJKwBEQgIGsCkqiAhQpiQdS2gLUUatHa60bhWrG2D9RrK3XDFXFBxOpjrQW8UFzASwKUTYQEwiIgi6AkrNkIJpnJvPePX09myZktmT3fz/OcZ2bOnDN5h8OZme95Nwq2zz6Teb9Pn0ZDTDy2TV6INbG3Yk1JFg4ecqwC6t/fFsLz8lhDRNHJbJYa0Geflcd5eRLQIqXp9rlzMq1gURHQtq0MuJafH+pSBca33wJz5kj/e0D6tj/7LPCzn3meOVEpCdv33Sf99lNTgcWLZW50IiIiCjwGb2p9ysuBBx6QDo+auDgcHXgr1lwxE2vO52NTSTtYLLZfsldcYWuSPn48kJISgnIT+dmpU8DUqcC2bfJ4zhwZFdtoDG25fFVRAUycCGzZAiQny1Rj4TAQnL+YzcALL0gz8ZoaICZGarwff9z37jHHj0vT8y1b5PEdd0gAb9vW36UmIiIiewze1Hp98gmwYgWwYYNtKOD/qIjviE+vuh9r4m7DJyeyUV4d3/ic0QjccIOtNjwrK8jlJvKDTz6RAHbhgoS3ZcuA224Ldamar6ZGps1at07O0X/8A5g8OdSlarlNm4D/+i9g/355nJ8vQXnIkOa/psUiF1j+9CegoUFaN/ztb9LVhoiIiAKDwZsIkKGBCwokhG/YAJw+3fiUBbHYYhqLNel3Y3XNGBw5395h14EDJYDfcgswfLjURhGFK4tF5uP+y1/kcW6uhNQrrwxtufyhrk6aXa9cKV1D3n4b+PnPQ12q5ikrAx5+WJqHA9Lq5q9/lX74/vqM2bFDmpofPSpN1R99VGrV4+M97kpEREQ+YvAmcqYUcOSIBPCCAlnOnWt8+hD6YI3pp1iTPA3/Lu8Pq7L9Cu7cGfjRjySI33ijNHslChelpdK0eONGeXzffdJH2GQKbbn8yWIBZs0Cli+Xx6+8IjXGkaKhAXj1VeCxx4CqKgnE99wjF0rat/e8v6+qq4HZs4G33pLHw4ZJL5zWNEI8ERFRMDB4E3litUo7T61GvLAQqKwEAFxAe3yCCVhj/Ak+td6IKostaZtMwJgxUhM+cSLQtWuIyk8E+a97xx3A2bMy3dSbb0r/7mhktUqYfOklefzkk8C8eSEtkle2b5eLBEVF8jg3V5qVB2Pu7RUrgLvvliEwkpJkLvBZszwP2kZERETeYfAm8lVDA1BcbKsR37QJqKlBPeKxGSOxBpOwJvZWHGvo4bDb0KG2fuE5OWySTsHR0CBzPS9YII05Bg2SUbGjvUZTKZnT+okn5PEjj0gAD8cgef68NPN+80153Lat1HDfc09wZ1P49ltgxgz5aAOkj/ybb0ozdyIiImoZBm+iljKbgZ07bTXiW7ZA1dXhIPphNW7BGkzCNuRDwZa0MzJsIXzsWCAxMYTlp6h19qz0cV6/Xh7PmgW8+GLr+v/2zDPSVxqQuaxffjl8LnpZrcDSpVIbf/GirJs5U/pyd+oUujItWiQXAsxmID1dmu2PGxea8hAREUULBm8if6utlTajWo349u04Z2mLj3Ez1mASPsN4XIJtPrLERIUf/tCASZOkSXpGRgjLTlFj82Zg2jQZJzApSfoN/+IXoS5VaLz+OnDvvVILfuedMoJ7qAcQ271bmpXv2CGPBw6UZuXXXx/acmmKi2WguoMH5fHs2dJiICEhlKUiIiKKXAzeRIF26ZJMmvufGvG6L0tQqEZJk3RMwkk4zkc2PMeCSbfGYdIkYPDg8GwaS+HLagWefloG52poAPr1k6blV18d6pKF1vvvy4UHi0WaUP/976EJkRUVwB/+IBdCrFYgJUWm9XrgASAuLvjlcefyZeB3v5MB6gC5OPDee3JLREREvmHwJgq2igqpjtywAer/NqCkRDU2Sf8CIxw27dbxe0y8JRaTfmzED37A2iZy78IF6aO7dq08nj5dalHbtAltucLFv/4F/OQnMu3Y2LHARx8F799GKZkr++GHpQsAIIPdPfMMkJkZnDI018cfA7/8pZTbZJKm8A88ED5N9omIiCIBgzdRqJ0/LyOlFxSgbN1erP26D9ZgEtbjRlyGbZT05Pg6/CC3Ch16piI20YS4OBl4Sbu1v6+3rqXPN3ef5OTgDhDVWm3fDkyZApw6JRdoXnoJ+NWv2GLCWUGBzDRw6RKQlyehsl27wP7NffukWfnmzfI4O1tqkceMCezf9aezZ4G77rJd1Bk3TuZJZ9cYIiIi7zB4E4Wb06eBwkJ8v24zCj6tw+oz1+BfmIjvEJnzkSUkAP37AwMGSBNV7TYzk6HQH5QCXnhBalItFqB3b2laPnhwqEsWvr74ArjpJpk6a9AgYN06oHNn//+d6moZTf6FF6TZf1KSjLT+298CRqP//16gKQUsWQLMmSNDWXToIIPDTZ4c6pIRERGFPwZvonB34gTUhgIUrTiGf2+LRV15DSyIQwNi0YDYxvuW2AQ0XNEZDVd0gqV9JzS06whLWns0xCWgoUFCmf1tINdZrZ7fVrt2TcP4gAEylRLps1iAb74BDh8GjhyRZdcuYNs2eX7KFOCNNwB+tHm2bx9w441AWZlcrPj8c6B7d/+8tlLAP/4hAfX0aVl3220yN7a//kYoHTwoA68VF8vje+4BnntOWrcQERGRPgZvokhTUQGUlAB79wJ79shtSYmMhKSna1ep/hw0yHbbu3dAR3JSyhbEv/1Wirdvn+328GF53lVxnQN5v36tp3+71SrNxbVgbR+yjx2Tf1NnRqNMAfWb37AVgS++/hr44Q/lYka3bhK+Wzq/+aFDwP33y2sBQK9e0ux/woSWlzec1NUBf/yj9FFXSv7d3nsPGDYs1CUjIiIKTwzeRNHAagWOHnUM43v2ACdO6G+fkCDDXDsH8vbtg1Lc2loJKCUljqH81Cn97WNi5FqBc+14r16R2X9cKaC0tGmwPnJEwmBdnet9ExOBq66Sf4/evSXwjBol68h3334rNd9ffSVzZ69b17xm+pcvA088IUHUbJZByB59FHjkkei+aLRhg4wW/913ci3v8cdlJPRIPC+JiIgCicGbKJpVVtpqx7UwXlIC1NTob9+lS9Mw3qdP0OY5qqyUEG5fO15SAly8qL+91n984EDHUJ6REfqaX6WAc+ccQ7UWtL/+2vUhAGSO6V69bMFaC9m9e8sh4mjS/nXuHDB+PFBUJF0dPv4YyM/3bl+lgP/9X+Chh4CTJ2XdzTcDL74ox7A1uHgR+PWvgX/+Ux6PGiUjuEdDs3oiIiJ/YfAmam2sVmmz7Fw7fvy4/vYmk37teIcOQSmuVjvsHMYPHAC+/15/n3btmtaOB6r/eHm5frPwI0fkQoIrsbFAjx5Ng3WfPhJYWGMYXBUVwMSJwJYt0lf5o4+kGbo7x44BDz5oG+m7e3cZSG3y5NBf+Ak2pYDly2WasUuXgLQ0mav8jjtCXTIiIqLwwOBNRKKqqmnf8b17XVfNZmY2DeN9+watdryhQYKPFsTt+4+7GtytW7em/cezsz03Ba6ullpq52B9+LDMne2KwSB/0z5ca/d79IjMka2jWU0NcPvt0tzcaJQB0vRG7K6tBZ56CnjySbkfHw/8938Djz3GAcaOHgV+/nOZ3g6Q+y+/LEGciIjC2+nT0vorJgYYMoRTRvobgzcRuWa1Sk24c+34sWP625tM0vZbC+KDBgFDhwat7zggQeirrxxrx0tKpC+vnthYCcJaEL/ySumvah+yy8rc/83MzKa11r17S1PjaO7fG43q6mTE7pUr5f/G229LeNR8+qkMnnb0qDweO1aCZXZ2SIobliwW6e/+P/8jHyFZWcC77wLXXx/qkkW/6mppIaS3lJUBbdoAI0bIMmwYkJIS6hITUaiUlsrMKLt2AV9+KbelpY7bdO4sAXzoUNvtVVexy1tzhTx4L168GE8//TRKS0tx9dVX4/nnn8fIkSN1ty0tLcXcuXOxa9cuHDlyBA8++CCef/55n/4egzeRH1RXN+07vnevtDHV07u3/NLLy5PbQYOCXt1bUaHff7y83Lv9O3ZsGqx795YvoDZtAlp0CjKLBZg1S5pOA8ArrwCTJgGzZ0sgB+Riy3PPyRRura1Zube2bpWLFsePy4+03/9e5jGPjw91ySKLUvI55SpQl5ZKLVVpqfuxI5zFxEgvIi2I5+XJDBLs5kIUfc6csYVr7Vab7tJeTIx8DlitMgiuXgvC5GSpX7EP41dfzYoGb4Q0eH/wwQeYPn06Fi9ejOuuuw6vvfYa3nzzTRw4cADddUZlOXHiBBYtWoTc3FwsWrQIo0ePZvAmChdWq4yi7lw7rlUN2ktIAHJybEE8L0/aZAc5wWj9x+2D+IkTMoCZc99rzi/eulitErRfekkeJyRIa4rYWBlIbcEC1hZ6o6pK+n2/8448vuYamXaMo/BLd5lz59wHaq2m2t1MB87atJELQxkZjkt6OnD2LLBjhyzaYIDO+w4f7nidND3df++ZiALv7FnHgP3ll9KSz1lMjLTWGjYMyM2V28GDbV2mLl+W30VFRUBxsdzu3Svfhc7i4iSw29eODxkiY+6QTUiD94gRI5CTk4NXX321cV2/fv1w66234sknn3S77w033IAhQ4YweBOFu4sXgS++kE6f2i8+vWrm9HTHX3tsB0khppTU0D7xhDweOVJqvwcODG25ItEHHwD33istT5KTZRC6u+6KztYCZrOEZU+B+swZCd/eat++aZjWW7xtgVNaavtI3rFDPqb1asy7d3f8aM7JkWkNiSj0zp1r2lxcb2pWg6FpyB4yxPdxSSwW6YpnH8aLilzPPpOVJUHcvna8a9fo/Oz3RsiCd319PZKSkvDhhx/itttua1z/0EMPobi4GBs3bnS7v7fBu66uDnV2l4qrqqrQrVs3Bm+iUFFKOk7v2GEL43v2yKe5Pa0dpPZrb8QItoOkkFi1Sv7b3nZb6/2x4A+nTgHTpwPa1/vttwOvvx60CRJ8ppTMnFBVpb9UVOgH6vPnvf8bBoPMH68XoO1rrdPTZQiNQGpokNki7D+a9++Xfwd7cXFSK6Z9LI8YIa2C2OeTKLDOn28asvVarhgMMtatFrBzcyX0BqouQykZR0cL4trtiRP623foYKsR1wJ5EMfmDamQBe/Tp0+jS5cu2LJlC6699trG9X/5y1+wfPlyHDp0yO3+3gbvBQsW4E9/+lOT9QzeRGHk+++B3bsda8X1vk1SUqQdpH0Y79w5+OUlomZpaACeeQb4wx/kWltmpvSl9zR1my+sVqm5dRWYq6pkqj93z2uLLzXS9uLiJCw7B2jnpVOn8P6xWV0tP/DtP5r1Bpts1066EdiH8XC9oEIUCS5ebNpc/Jtv9Lft08exJnvo0PBoMFheLvUq9mH8wAH9z9WEBGlNZl87PmgQkJQU7FIHVsiD99atW5Gfn9+4/s9//jP+9re/4auvvnK7P2u8iaKc1g5S+8W3c6d+O8gePRzbQQ4dyhE+iMLcrl3AnXfK4D0AMHcu8Pjj0pfZm0DsbqmublpL2xIGA5Caqr9o4dp56dAhOmuAlZJrovZN1Hft0u/zedVVjh/NgwdzCkUiPeXljiF71y4ZlFJP7962kJ2bKz95Imm6xtpaaUljXzu+Z4/+2LwxMXJRwb6Z+pAhMthtpIr6pubO2MebKEJZLHKp1L7q5cCBpr+w4+Plk9m+VrxXL7YRJgozNTUy//mSJYF5/dhY+UHqKjS7W+z3S0qKzhDtL2azDLhkf5308OGm25lM8sPZ/qO5Rw9+NAea1SoXoyoqZCkvd7y9dEmOTVKS9N1PSnJ93/6Wvb6ap7xcGvjZB21XM7RedZVjc/GcnMgK2d6yWoGvv27aVP3MGf3tu3RpOsVZz56R8VkS8sHVcnNzsXjx4sZ1/fv3x+TJkzm4GhF5Vlnp2A5y+3YZacRZhw6OVS/XXMNhyonCxOrVwN13y0i8gIQAb0OxuyUhITJ+iEWjixelkZL9dVK9wZc6dXJsnj58eHQGi5aqrdUPzd6sq6zUnxKqpYxG9yHdU3j39r7R6J/z2GqVJs4Wi/6tu+e8vXX1XHW1BMkvv9Sf6AUArrzSsbl4Tg5/ppSVOQbx4mIZIkhPaiowbx7w6KPBLKHvwmI6sSVLliA/Px+vv/463njjDezfvx9ZWVl49NFH8d133+EdbR4SAMXFxQCAWbNmoW/fvnj44YdhNBrRv39/r/4mgzdRFFNKRvOw/7W3ezdQX9902+xsxzA+cGDwO1sqJdVFZrN8O9vfervOYpFyp6TIN09Kiu2+ycTkQRHBbJagoP23peiilNRo2TdRLy6W427PYJAxNLUgnpsro7THxvq+hNNHX0ODBGBXYdlTkNZryu8rk0n64rdtK4t2PyVFunh8/71MH3X5suv7/iiHrwyGpoE8Jsb30Oz/FNN8PXs6NhfPyZFZC8iz6mppYWMfyPftk595zz8v032Gs5AGbwBYvHgxnnrqKZSWlmLAgAFYtGgRRo0aBQCYOXMmTpw4gcLCQltBdD5Js7KycMLV0HlOGLyJWpm6OulAZF8rrteuKynJ9g1oNLY8DHuzfSCqIew5B3LnYO58393ziYnh9UuWiCJaba38aNaC+PbtrkdBbg6DoXmBvaWLXu10VZV/3o9zaLa/73zrvM4fQ59YrfL+3IVzf91v7sCGzaEdu7i4lt26ei4hARgwwPYTgwMP+pfZDBw8KC1o0tNDXRr3Qh68g43Bm4hw7lzTucX98cvIH2Ji5Js6Pl4W7b67dWazXAbWRpbSG6XEH+XyV4hPTmaIJ6Imzpxx/GjWarK0Wk37JdDXLQMhKcn74Oy8TUpK6xprwGzWD+SXL8uxb2kY1m5jYvh1RMHD4E1EZLXK8Mrbt8svPYPB+/DrbTj29nX88cvKapXwbR/G7e/rrXP1vL+HiNYYDI6L9uvH0xKq7RITpfOp89K2ret1KSkcgYgoQJSy9dsN5WLf3LmhQWo39YJzWhq7URC1dt7m0DCeZZKIqIViYqRjYb9+oS6Jf8TE2EaY6tKlZa9ltUoVQ3ODu/N9rZpKqfDqdBcoKSnehXRX69u0aV3VXEResm9KTkQUTRi8iYhao5gYCX9t2sgExS2hlLQZvHRJArgWvrWqK/vHrpZQbKd1bKyokBGS7BdX6+rq5D1rFx2+/bZ5/2baJNLehHTnde3a+a9zJxEREQUFgzcREbWM/fC00a6uzruA7m6d2SzBX3t88mTzypKQIAG8fXtbGNcWT+uMRj/+oxAREZEnDN5ERETeMplkiNVOnZq3v1JSy+5taNd7XFFhe53SUll8lZTkfWi3f9y2rYxbQERERD5h8CYiIgoWbUC3xMTmz49itUq/+vJyWS5etN33tK6yUl5DG0b4u+98//tt2vhWy96hA9Cxo+zHYYaJiKiVYvAmIiKKJDExtmGVe/b0bd+GBgnf3oR053Xa1HyXLsniaxN5k0kCeMeO0mJAu+9qXWoqgzoREUUNBm8iIqLWIjZWaqHbt/d9X4tFmrn7GtrPn5fB9+rqZDA6bwekMxqbhnNXob1jR7kQwaBORERhisGbiIiIPIuLA664QhZf1dQA587ZlrNnHR87r6+pAerrpSm8t83h4+OlbJ5CuraubdvgTOmmlLQ0qKuT96Td2t9v6TqDQd6/0eh+8bSNp+djY3lxg4iomRi8iYiIKLCSk2Xp0cO77b//3vuQfu6cTO1mNvs22FxsrC2o2wf09u0dg7I/AnK0zG3vTcD35gKA0ShdCfSWtDTHxyZTqN81EZFfMHgTERFReElMBLp3l8UbtbXSpN1VUHdeV1kp4frMGVmCzWSS8Ol86+q+p3WAhHyz2Rb49RZfnzebHcutlO25YDEam4ZxVyHd3ToGeCIKMQZvIiIiimwJCUDXrrJ4o75egrpeSC8vl2b1voZfb0N0XFzkNNdWyhbG/Rnqa2ullUJVlW2prHR8fOmSlKG+3nZsWkKvlt3b4J6aKv/HDAZZYmJs950fN/c5Iop6DN5ERETUuhiNQGamLOSaweBYqx5MDQ0Svp0DuV5Id7e+ulpeT7vYcv588N+LN+wDuS+B3dW2MTFysSchwfOtv7cJxtgJGqWkO0dtbcuW5rxGfb10rTCZHC+8eXvf39tpXT14ISdsMXgTERERUXiJjZXa57S0lr2OFuBbEt4rK2199bXFavVv333tdaOBFkZ9DfBK+R6O6+pC/W7Dj7uAHh8vF0ZiYx0XvXWu1rdkXXP279cP6Ns31P+qfsHgTURERETRyV8B3hX7MO4cyp3ve3rsz+esVgmlWli1D63O6/zxnP1FA7NZFq27QLAYDLZw78tif1HAlyU+Xt6n/aCK7u4HYju9wRu19dFiwQJg/vxQl8IvGLyJiIiIiJqDfbQl+FksLQvztbVSw9mScNxam1lbLN4HeYtFWoHYL1Zr89YFaz9vB9mMAAzeRERERETUPNo0c/HxQEpKqEvT+sTFyZKcHOqSkAdBHP2AiIiIiIiIqPVh8CYiIiIiIiIKIAZvIiIiIiIiogBi8CYiIiIiIiIKIAZvIiIiIiIiogBi8CYiIiIiIiIKoKiZTkz9Z/L4qqqqEJeEiIiIiIiIWgMtf2p51JWoCd7V1dUAgG7duoW4JERERERERNSaVFdXIy0tzeXzBuUpmkcIq9WK06dPIyUlBQaDIdTFcamqqgrdunXDqVOnkJqaGurikA947CIXj11k4/GLXDx2kYvHLnLx2EU2Hr/Io5RCdXU1MjMzERPjuid31NR4x8TEoGvXrqEuhtdSU1N5MkUoHrvIxWMX2Xj8IhePXeTisYtcPHaRjccvsrir6dZwcDUiIiIiIiKiAGLwJiIiIiIiIgogBu8gM5lMmD9/PkwmU6iLQj7isYtcPHaRjccvcvHYRS4eu8jFYxfZePyiV9QMrkZEREREREQUjljjTURERERERBRADN5EREREREREAcTgTURERERERBRADN5EREREREREAcTgHQCLFy9Gz549kZCQgNzcXGzevNnt9hs3bkRubi4SEhJw5ZVXYsmSJUEqKWmefPJJDB8+HCkpKejUqRNuvfVWHDp0yO0+hYWFMBgMTZavvvoqSKUmAFiwYEGTY5Cenu52H55z4aNHjx6659F9992nuz3Pu9DZtGkTJk2ahMzMTBgMBnz00UcOzyulsGDBAmRmZiIxMRE33HAD9u/f7/F1V6xYgf79+8NkMqF///5YtWpVgN5B6+Xu2JnNZjzyyCMYOHAgkpOTkZmZiV/84hc4ffq029d8++23dc/F2traAL+b1sfTuTdz5swmxyEvL8/j6/LcCzxPx07vHDIYDHj66addvibPvcjF4O1nH3zwAWbPno3HHnsMRUVFGDlyJCZMmICTJ0/qbn/8+HHcfPPNGDlyJIqKivD73/8eDz74IFasWBHkkrduGzduxH333Yft27dj/fr1sFgsGDduHGpqajzue+jQIZSWljYuvXv3DkKJyd7VV1/tcAxKSkpcbstzLrzs3LnT4ditX78eAPDTn/7U7X4874KvpqYGgwcPxssvv6z7/FNPPYXnnnsOL7/8Mnbu3In09HTceOONqK6udvma27Ztw9SpUzF9+nTs2bMH06dPx5QpU7Bjx45AvY1Wyd2xu3z5Mnbv3o0//vGP2L17N1auXInDhw/jlltu8fi6qampDudhaWkpEhISAvEWWjVP5x4A3HTTTQ7H4eOPP3b7mjz3gsPTsXM+f9566y0YDAb8+Mc/dvu6PPcilCK/uuaaa9S9997rsC47O1vNmzdPd/vf/e53Kjs722Hdr3/9a5WXlxewMpJnZ8+eVQDUxo0bXW5TUFCgAKjy8vLgFYyamD9/vho8eLDX2/OcC28PPfSQ6tWrl7JarbrP87wLDwDUqlWrGh9brVaVnp6uFi5c2LiutrZWpaWlqSVLlrh8nSlTpqibbrrJYd348ePVtGnT/F5mEs7HTs8XX3yhAKhvvvnG5TbLli1TaWlp/i0ceaR3/GbMmKEmT57s0+vw3As+b869yZMnqzFjxrjdhude5GKNtx/V19dj165dGDdunMP6cePGYevWrbr7bNu2rcn248ePx5dffgmz2RywspJ7lZWVAID27dt73Hbo0KHIyMjA2LFjUVBQEOiikY4jR44gMzMTPXv2xLRp03Ds2DGX2/KcC1/19fV49913cdddd8FgMLjdluddeDl+/DjKysoczi2TyYTRo0e7/P4DXJ+P7vahwKusrITBYEDbtm3dbnfp0iVkZWWha9eumDhxIoqKioJTQGqisLAQnTp1Qp8+fXD33Xfj7NmzbrfnuRd+zpw5g7Vr1+JXv/qVx2157kUmBm8/On/+PBoaGtC5c2eH9Z07d0ZZWZnuPmVlZbrbWywWnD9/PmBlJdeUUpgzZw6uv/56DBgwwOV2GRkZeP3117FixQqsXLkSffv2xdixY7Fp06YglpZGjBiBd955B5999hneeOMNlJWV4dprr8WFCxd0t+c5F74++ugjVFRUYObMmS634XkXnrTvOF++/7T9fN2HAqu2thbz5s3Dz372M6SmprrcLjs7G2+//TZWr16N999/HwkJCbjuuutw5MiRIJaWAGDChAl47733sGHDBjz77LPYuXMnxowZg7q6Opf78NwLP8uXL0dKSgpuv/12t9vx3ItccaEuQDRyrqlRSrmtvdHbXm89Bcf999+PvXv34t///rfb7fr27Yu+ffs2Ps7Pz8epU6fwzDPPYNSoUYEuJv3HhAkTGu8PHDgQ+fn56NWrF5YvX445c+bo7sNzLjwtXboUEyZMQGZmpstteN6FN1+//5q7DwWG2WzGtGnTYLVasXjxYrfb5uXlOQzgdd111yEnJwcvvfQSXnzxxUAXlexMnTq18f6AAQMwbNgwZGVlYe3atW5DHM+98PLWW2/hzjvv9NhXm+de5GKNtx9dccUViI2NbXK18OzZs02uKmrS09N1t4+Li0OHDh0CVlbS98ADD2D16tUoKChA165dfd4/Ly+PVxxDLDk5GQMHDnR5HHjOhadvvvkGn3/+OWbNmuXzvjzvQk+bScCX7z9tP1/3ocAwm82YMmUKjh8/jvXr17ut7dYTExOD4cOH81wMAxkZGcjKynJ7LHjuhZfNmzfj0KFDzfoO5LkXORi8/choNCI3N7dxVF7N+vXrce211+ruk5+f32T7devWYdiwYYiPjw9YWcmRUgr3338/Vq5ciQ0bNqBnz57Nep2ioiJkZGT4uXTki7q6Ohw8eNDlceA5F56WLVuGTp064Uc/+pHP+/K8C72ePXsiPT3d4dyqr6/Hxo0bXX7/Aa7PR3f7kP9pofvIkSP4/PPPm3URUimF4uJinoth4MKFCzh16pTbY8FzL7wsXboUubm5GDx4sM/78tyLIKEa1S1a/f3vf1fx8fFq6dKl6sCBA2r27NkqOTlZnThxQiml1Lx589T06dMbtz927JhKSkpSv/3tb9WBAwfU0qVLVXx8vPrnP/8ZqrfQKv3mN79RaWlpqrCwUJWWljYuly9fbtzG+dgtWrRIrVq1Sh0+fFjt27dPzZs3TwFQK1asCMVbaLXmzp2rCgsL1bFjx9T27dvVxIkTVUpKCs+5CNLQ0KC6d++uHnnkkSbP8bwLH9XV1aqoqEgVFRUpAOq5555TRUVFjSNfL1y4UKWlpamVK1eqkpISdccdd6iMjAxVVVXV+BrTp093mOVjy5YtKjY2Vi1cuFAdPHhQLVy4UMXFxant27cH/f1FM3fHzmw2q1tuuUV17dpVFRcXO3wH1tXVNb6G87FbsGCB+vTTT9XRo0dVUVGR+uUvf6ni4uLUjh07QvEWo5q741ddXa3mzp2rtm7dqo4fP64KCgpUfn6+6tKlC8+9MODpc1MppSorK1VSUpJ69dVXdV+D5170YPAOgFdeeUVlZWUpo9GocnJyHKakmjFjhho9erTD9oWFhWro0KHKaDSqHj16uDzxKHAA6C7Lli1r3Mb52P31r39VvXr1UgkJCapdu3bq+uuvV2vXrg1+4Vu5qVOnqoyMDBUfH68yMzPV7bffrvbv39/4PM+58PfZZ58pAOrQoUNNnuN5Fz60qdyclxkzZiilZEqx+fPnq/T0dGUymdSoUaNUSUmJw2uMHj26cXvNhx9+qPr27avi4+NVdnY2L6IEgLtjd/z4cZffgQUFBY2v4XzsZs+erbp3766MRqPq2LGjGjdunNq6dWvw31wr4O74Xb58WY0bN0517NhRxcfHq+7du6sZM2aokydPOrwGz73Q8PS5qZRSr732mkpMTFQVFRW6r8FzL3oYlPrPqEJERERERERE5Hfs401EREREREQUQAzeRERERERERAHE4E1EREREREQUQAzeRERERERERAHE4E1EREREREQUQAzeRERERERERAHE4E1EREREREQUQAzeRERERERERAHE4E1EREREREQUQAzeRERERERERAHE4E1EREREREQUQAzeRERERERERAH0/9dWTGaSSVF0AAAAAElFTkSuQmCC\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 6), dpi=100)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(acc, 'r', label='train acc')\n",
    "plt.plot(val_acc, 'b', label='val acc')\n",
    "plt.title('Train and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(loss, 'r', label='train loss')\n",
    "plt.plot(val_loss, 'b', label='val loss')\n",
    "plt.title('Train and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "53/53 [==============================] - 0s 992us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.95238   0.96852   0.96038       413\n",
      "           1    0.98701   0.99130   0.98915       460\n",
      "           2    0.97818   0.97464   0.97641       276\n",
      "           3    0.98824   0.97297   0.98054       518\n",
      "\n",
      "    accuracy                        0.97720      1667\n",
      "   macro avg    0.97645   0.97686   0.97662      1667\n",
      "weighted avg    0.97735   0.97720   0.97724      1667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = ann_model.predict(x_test)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(classification_report(y_test, pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "training a CNN using user's profile photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_base = \"../new batch profile pics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all image pathnames from base\n",
    "# list to store files\n",
    "res = []\n",
    "res2 = []\n",
    "\n",
    "# Iterate directory\n",
    "for path in os.listdir(faces_base):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(faces_base, path)):\n",
    "        res.append(faces_base + path)\n",
    "        res2.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre trained CNN: InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: profile 'icc': 'RGB ': RGB color space not permitted on grayscale PNG\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: profile 'icc': 'RGB ': RGB color space not permitted on grayscale PNG\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "train_img_dict2 = {}\n",
    "test_img_dict2 = {}\n",
    "train_img2 = []\n",
    "test_img2 = []\n",
    "for i in range(len(res)):\n",
    "    pic = res[i]\n",
    "    id_name = re.match(r\"[^\\/\\\\]+(?=\\.png|\\.jpg)\", res2[i]).group(0)\n",
    "    try:\n",
    "        img = cv2.imread(pic)\n",
    "        if img is None:\n",
    "            print(\"none\")\n",
    "            continue\n",
    "        img = cv2.resize(img, (75, 75))\n",
    "        if int(id_name) in list(train_df['id']):\n",
    "            train_img_dict2[int(id_name)] = img \n",
    "            train_img2.append(img)\n",
    "        elif int(id_name) in list(test_df['id']):\n",
    "            test_img_dict2[int(id_name)] = img\n",
    "            test_img2.append(img)\n",
    "        #img.close()\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_df2 = pd.DataFrame(train_img_dict2.items(), columns = ['id', 'img'])  \n",
    "test_img_df2 = pd.DataFrame(test_img_dict2.items(), columns = ['id', 'img'])  \n",
    "train_df_with_img2 = pd.merge(train_img_df2, train_df, on='id')\n",
    "test_df_with_img2 = pd.merge(test_img_df2, test_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(input_shape = (75, 75, 3), include_top = False, weights = 'imagenet')\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(70, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(6, activation='softmax')(x)\n",
    "model = Model(base_model.input, x)\n",
    "# configure the model\n",
    "model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, x_val2, y_train2, y_val2 = train_test_split(np.array(train_img2) / 255.0, train_df_with_img2['account_type_multi'], test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "161/161 [==============================] - 35s 193ms/step - loss: 1.2554 - accuracy: 0.4530 - val_loss: 1.0318 - val_accuracy: 0.5152\n",
      "Epoch 2/20\n",
      "161/161 [==============================] - 30s 186ms/step - loss: 1.0592 - accuracy: 0.5195 - val_loss: 0.9994 - val_accuracy: 0.5272\n",
      "Epoch 3/20\n",
      "161/161 [==============================] - 31s 195ms/step - loss: 1.0125 - accuracy: 0.5410 - val_loss: 0.9858 - val_accuracy: 0.5343\n",
      "Epoch 4/20\n",
      "161/161 [==============================] - 37s 232ms/step - loss: 0.9721 - accuracy: 0.5673 - val_loss: 0.9675 - val_accuracy: 0.5519\n",
      "Epoch 5/20\n",
      "161/161 [==============================] - 33s 203ms/step - loss: 0.9377 - accuracy: 0.5901 - val_loss: 0.9596 - val_accuracy: 0.5527\n",
      "Epoch 6/20\n",
      "161/161 [==============================] - 35s 220ms/step - loss: 0.9056 - accuracy: 0.5981 - val_loss: 0.9721 - val_accuracy: 0.5541\n",
      "Epoch 7/20\n",
      "161/161 [==============================] - 34s 208ms/step - loss: 0.8772 - accuracy: 0.6178 - val_loss: 0.9658 - val_accuracy: 0.5576\n",
      "Epoch 8/20\n",
      "161/161 [==============================] - 36s 226ms/step - loss: 0.8493 - accuracy: 0.6314 - val_loss: 0.9704 - val_accuracy: 0.5618\n",
      "Epoch 9/20\n",
      "161/161 [==============================] - 35s 220ms/step - loss: 0.8265 - accuracy: 0.6409 - val_loss: 0.9652 - val_accuracy: 0.5696\n",
      "Epoch 10/20\n",
      "161/161 [==============================] - 32s 198ms/step - loss: 0.7806 - accuracy: 0.6598 - val_loss: 0.9945 - val_accuracy: 0.5746\n",
      "Epoch 11/20\n",
      "161/161 [==============================] - 41s 253ms/step - loss: 0.7568 - accuracy: 0.6754 - val_loss: 0.9829 - val_accuracy: 0.5823\n",
      "Epoch 12/20\n",
      "161/161 [==============================] - 34s 214ms/step - loss: 0.7301 - accuracy: 0.6850 - val_loss: 0.9982 - val_accuracy: 0.5760\n",
      "Epoch 13/20\n",
      "161/161 [==============================] - 35s 216ms/step - loss: 0.6930 - accuracy: 0.6992 - val_loss: 1.0056 - val_accuracy: 0.5675\n",
      "Epoch 14/20\n",
      "161/161 [==============================] - 35s 215ms/step - loss: 0.6765 - accuracy: 0.7070 - val_loss: 1.0452 - val_accuracy: 0.5717\n",
      "Epoch 15/20\n",
      "161/161 [==============================] - 34s 213ms/step - loss: 0.6504 - accuracy: 0.7255 - val_loss: 1.0537 - val_accuracy: 0.5597\n",
      "Epoch 16/20\n",
      "161/161 [==============================] - 48s 299ms/step - loss: 0.6257 - accuracy: 0.7366 - val_loss: 1.0610 - val_accuracy: 0.5746\n",
      "Epoch 17/20\n",
      "161/161 [==============================] - 40s 247ms/step - loss: 0.6205 - accuracy: 0.7333 - val_loss: 1.0618 - val_accuracy: 0.5739\n",
      "Epoch 18/20\n",
      "161/161 [==============================] - 44s 276ms/step - loss: 0.5817 - accuracy: 0.7500 - val_loss: 1.1475 - val_accuracy: 0.5675\n",
      "Epoch 19/20\n",
      "161/161 [==============================] - 43s 266ms/step - loss: 0.5591 - accuracy: 0.7545 - val_loss: 1.1142 - val_accuracy: 0.5597\n",
      "Epoch 20/20\n",
      "161/161 [==============================] - 42s 258ms/step - loss: 0.5439 - accuracy: 0.7651 - val_loss: 1.1108 - val_accuracy: 0.5781\n",
      "Total time taken for the program execution 735.5138609409332\n"
     ]
    }
   ],
   "source": [
    "#es_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15)\n",
    "start_time = time.time()\n",
    "inc_history = model.fit(x_train2, y_train2, epochs=20, batch_size=50, validation_data=(x_val2, y_val2))\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Total time taken for the program execution\", time_taken) # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train loss: 0.3346036374568939 / Train accuracy: 0.9026824831962585\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train2, y_train2, verbose=0)\n",
    "print(f'Train loss: {score[0]} / Train accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test loss: 1.0615887641906738 / Test accuracy: 0.605896532535553\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(np.array(test_img2) / 255.0, test_df_with_img2['account_type_multi'], verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "52/52 [==============================] - 22s 125ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(np.array(test_img2) / 255.0)\n",
    "pred = np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0    0.48387   0.47330   0.47853       412\n           1    0.51442   0.46522   0.48858       460\n           2    0.93506   0.78832   0.85545       274\n           3    0.62418   0.74031   0.67730       516\n\n    accuracy                        0.60590      1662\n   macro avg    0.63939   0.61679   0.62497      1662\nweighted avg    0.61027   0.60590   0.60516      1662\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df_with_img2['account_type_multi'], pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: profile 'icc': 'RGB ': RGB color space not permitted on grayscale PNG\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: profile 'icc': 'RGB ': RGB color space not permitted on grayscale PNG\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "train_img_dict = {}\n",
    "test_img_dict = {}\n",
    "train_img = []\n",
    "test_img = []\n",
    "for i in range(len(res)):\n",
    "    pic = res[i]\n",
    "    id_name = re.match(r\"[^\\/\\\\]+(?=\\.png|\\.jpg)\", res2[i]).group(0)\n",
    "    try:\n",
    "        img = cv2.imread(pic)\n",
    "        if img is None:\n",
    "            print(\"none\")\n",
    "            continue\n",
    "        if int(id_name) in list(train_df['id']):\n",
    "            train_img_dict[int(id_name)] = img \n",
    "            train_img.append(img)\n",
    "        elif int(id_name) in list(test_df['id']):\n",
    "            test_img_dict[int(id_name)] = img\n",
    "            test_img.append(img)\n",
    "        #img.close()\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9430"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_img_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_df = pd.DataFrame(train_img_dict.items(), columns = ['id', 'img'])  \n",
    "test_img_df = pd.DataFrame(test_img_dict.items(), columns = ['id', 'img'])  \n",
    "train_df_with_img = pd.merge(train_img_df, train_df, on='id')\n",
    "test_df_with_img = pd.merge(test_img_df, test_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = (3, 3)\n",
    "stride_size = (1, 1)\n",
    "filters = [4, 8, 16]\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Input(shape=(48, 48, 3)))\n",
    "for f in filters:\n",
    "    cnn_model.add(Conv2D(f, kernel_size, strides=stride_size, activation='relu'))\n",
    "    cnn_model.add(Dropout(0.1))\n",
    "    cnn_model.add(Conv2D(f, kernel_size, strides=stride_size, activation='relu'))\n",
    "    cnn_model.add(BatchNormalization(axis = -1))\n",
    "    cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(100, activation='relu'))\n",
    "cnn_model.add(Dropout(0.3))\n",
    "cnn_model.add(Dense(100, activation='relu'))\n",
    "cnn_model.add(Dropout(0.3))\n",
    "cnn_model.add(Dense(100, activation='relu'))\n",
    "cnn_model.add(Dense(6, activation='softmax'))\n",
    "# configure the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = (3, 3)\n",
    "stride_size = (1, 1)\n",
    "filters = [4, 10, 24]\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Input(shape=(48, 48, 3)))\n",
    "for f in filters:\n",
    "    cnn_model.add(Conv2D(f, kernel_size, strides=stride_size, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "    cnn_model.add(Conv2D(f, kernel_size, strides=stride_size, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "    cnn_model.add(BatchNormalization(axis = -1))\n",
    "    cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(100, activation='relu'))\n",
    "cnn_model.add(Dropout(0.1))\n",
    "\n",
    "cnn_model.add(Dense(100, activation='relu'))\n",
    "cnn_model.add(Dropout(0.1))\n",
    "cnn_model.add(Dense(100, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dense(6, activation='softmax'))\n",
    "# configure the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(np.array(train_img) / 255.0, train_df_with_img['account_type_multi'], test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "401/401 [==============================] - 17s 38ms/step - loss: 1.7331 - accuracy: 0.4354 - val_loss: 1.8124 - val_accuracy: 0.3880\n",
      "Epoch 2/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.5007 - accuracy: 0.4770 - val_loss: 1.4975 - val_accuracy: 0.4353\n",
      "Epoch 3/100\n",
      "401/401 [==============================] - 14s 36ms/step - loss: 1.3867 - accuracy: 0.4957 - val_loss: 1.3613 - val_accuracy: 0.4890\n",
      "Epoch 4/100\n",
      "401/401 [==============================] - 14s 36ms/step - loss: 1.3133 - accuracy: 0.4987 - val_loss: 1.3241 - val_accuracy: 0.4905\n",
      "Epoch 5/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.2707 - accuracy: 0.5154 - val_loss: 1.2847 - val_accuracy: 0.4883\n",
      "Epoch 6/100\n",
      "401/401 [==============================] - 14s 36ms/step - loss: 1.2424 - accuracy: 0.5188 - val_loss: 1.2548 - val_accuracy: 0.5088\n",
      "Epoch 7/100\n",
      "401/401 [==============================] - 14s 36ms/step - loss: 1.2212 - accuracy: 0.5279 - val_loss: 1.2828 - val_accuracy: 0.4488\n",
      "Epoch 8/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.2001 - accuracy: 0.5346 - val_loss: 1.4448 - val_accuracy: 0.4078\n",
      "Epoch 9/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.1866 - accuracy: 0.5364 - val_loss: 1.2377 - val_accuracy: 0.4898\n",
      "Epoch 10/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.1793 - accuracy: 0.5411 - val_loss: 1.2753 - val_accuracy: 0.4848\n",
      "Epoch 11/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.1614 - accuracy: 0.5429 - val_loss: 1.2598 - val_accuracy: 0.4742\n",
      "Epoch 12/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.1459 - accuracy: 0.5607 - val_loss: 1.2861 - val_accuracy: 0.4862\n",
      "Epoch 13/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.1459 - accuracy: 0.5566 - val_loss: 1.3325 - val_accuracy: 0.4332\n",
      "Epoch 14/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.1342 - accuracy: 0.5691 - val_loss: 1.2330 - val_accuracy: 0.5166\n",
      "Epoch 15/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.1194 - accuracy: 0.5797 - val_loss: 1.2363 - val_accuracy: 0.5095\n",
      "Epoch 16/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.1121 - accuracy: 0.5774 - val_loss: 1.2344 - val_accuracy: 0.4989\n",
      "Epoch 17/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.1119 - accuracy: 0.5774 - val_loss: 1.2458 - val_accuracy: 0.4975\n",
      "Epoch 18/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 1.0896 - accuracy: 0.5939 - val_loss: 1.3051 - val_accuracy: 0.4643\n",
      "Epoch 19/100\n",
      "401/401 [==============================] - 14s 34ms/step - loss: 1.0983 - accuracy: 0.5857 - val_loss: 1.3260 - val_accuracy: 0.4799\n",
      "Epoch 20/100\n",
      "401/401 [==============================] - 14s 34ms/step - loss: 1.0795 - accuracy: 0.6062 - val_loss: 1.2950 - val_accuracy: 0.5011\n",
      "Epoch 21/100\n",
      "401/401 [==============================] - 14s 36ms/step - loss: 1.0704 - accuracy: 0.6029 - val_loss: 1.3045 - val_accuracy: 0.4975\n",
      "Epoch 22/100\n",
      "401/401 [==============================] - 16s 39ms/step - loss: 1.0701 - accuracy: 0.6055 - val_loss: 1.2420 - val_accuracy: 0.5322\n",
      "Epoch 23/100\n",
      "401/401 [==============================] - 15s 37ms/step - loss: 1.0488 - accuracy: 0.6205 - val_loss: 1.2347 - val_accuracy: 0.5371\n",
      "Epoch 24/100\n",
      "401/401 [==============================] - 15s 37ms/step - loss: 1.0627 - accuracy: 0.6180 - val_loss: 1.3592 - val_accuracy: 0.4975\n",
      "Epoch 25/100\n",
      "401/401 [==============================] - 16s 39ms/step - loss: 1.0360 - accuracy: 0.6293 - val_loss: 1.2786 - val_accuracy: 0.5293\n",
      "Epoch 26/100\n",
      "401/401 [==============================] - 17s 41ms/step - loss: 1.0178 - accuracy: 0.6353 - val_loss: 1.2800 - val_accuracy: 0.5293\n",
      "Epoch 27/100\n",
      "401/401 [==============================] - 16s 40ms/step - loss: 1.0052 - accuracy: 0.6478 - val_loss: 1.3054 - val_accuracy: 0.5223\n",
      "Epoch 28/100\n",
      "401/401 [==============================] - 17s 43ms/step - loss: 0.9979 - accuracy: 0.6445 - val_loss: 1.5256 - val_accuracy: 0.4989\n",
      "Epoch 29/100\n",
      "401/401 [==============================] - 15s 38ms/step - loss: 0.9975 - accuracy: 0.6505 - val_loss: 1.3173 - val_accuracy: 0.5166\n",
      "Epoch 30/100\n",
      "401/401 [==============================] - 16s 39ms/step - loss: 0.9842 - accuracy: 0.6580 - val_loss: 1.3415 - val_accuracy: 0.5088\n",
      "Epoch 31/100\n",
      "401/401 [==============================] - 15s 37ms/step - loss: 0.9712 - accuracy: 0.6685 - val_loss: 1.4797 - val_accuracy: 0.5265\n",
      "Epoch 32/100\n",
      "401/401 [==============================] - 15s 37ms/step - loss: 0.9505 - accuracy: 0.6711 - val_loss: 1.3227 - val_accuracy: 0.5336\n",
      "Epoch 33/100\n",
      "401/401 [==============================] - 14s 36ms/step - loss: 0.9536 - accuracy: 0.6759 - val_loss: 1.3770 - val_accuracy: 0.5265\n",
      "Epoch 34/100\n",
      "401/401 [==============================] - 15s 37ms/step - loss: 0.9324 - accuracy: 0.6825 - val_loss: 1.3860 - val_accuracy: 0.5145\n",
      "Epoch 35/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 0.9163 - accuracy: 0.6967 - val_loss: 1.4192 - val_accuracy: 0.5463\n",
      "Epoch 36/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 0.9065 - accuracy: 0.7001 - val_loss: 1.4519 - val_accuracy: 0.5314\n",
      "Epoch 37/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 0.9144 - accuracy: 0.6967 - val_loss: 1.4608 - val_accuracy: 0.5314\n",
      "Epoch 38/100\n",
      "401/401 [==============================] - 15s 38ms/step - loss: 0.8919 - accuracy: 0.7104 - val_loss: 1.3783 - val_accuracy: 0.5463\n",
      "Epoch 39/100\n",
      "401/401 [==============================] - 16s 39ms/step - loss: 0.8854 - accuracy: 0.7168 - val_loss: 1.4634 - val_accuracy: 0.5251\n",
      "Epoch 40/100\n",
      "401/401 [==============================] - 14s 36ms/step - loss: 0.8953 - accuracy: 0.7103 - val_loss: 1.4096 - val_accuracy: 0.5399\n",
      "Epoch 41/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 0.8502 - accuracy: 0.7315 - val_loss: 1.4791 - val_accuracy: 0.5329\n",
      "Epoch 42/100\n",
      "401/401 [==============================] - 14s 34ms/step - loss: 0.8683 - accuracy: 0.7249 - val_loss: 1.4815 - val_accuracy: 0.5307\n",
      "Epoch 43/100\n",
      "401/401 [==============================] - 14s 36ms/step - loss: 0.8561 - accuracy: 0.7324 - val_loss: 1.4587 - val_accuracy: 0.5420\n",
      "Epoch 44/100\n",
      "401/401 [==============================] - 18s 45ms/step - loss: 0.8459 - accuracy: 0.7397 - val_loss: 1.5132 - val_accuracy: 0.5300\n",
      "Epoch 45/100\n",
      "401/401 [==============================] - 15s 37ms/step - loss: 0.8461 - accuracy: 0.7367 - val_loss: 1.5220 - val_accuracy: 0.5343\n",
      "Epoch 46/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 0.8486 - accuracy: 0.7405 - val_loss: 1.4241 - val_accuracy: 0.5498\n",
      "Epoch 47/100\n",
      "401/401 [==============================] - 14s 36ms/step - loss: 0.8190 - accuracy: 0.7470 - val_loss: 1.6051 - val_accuracy: 0.5201\n",
      "Epoch 48/100\n",
      "401/401 [==============================] - 15s 37ms/step - loss: 0.8144 - accuracy: 0.7538 - val_loss: 1.5669 - val_accuracy: 0.5159\n",
      "Epoch 49/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 0.8163 - accuracy: 0.7508 - val_loss: 1.4834 - val_accuracy: 0.5293\n",
      "Epoch 50/100\n",
      "401/401 [==============================] - 14s 35ms/step - loss: 0.7932 - accuracy: 0.7659 - val_loss: 1.5493 - val_accuracy: 0.5322\n",
      "Epoch 51/100\n",
      "401/401 [==============================] - 14s 34ms/step - loss: 0.8041 - accuracy: 0.7626 - val_loss: 1.4918 - val_accuracy: 0.5272\n",
      "Epoch 52/100\n",
      "401/401 [==============================] - 14s 36ms/step - loss: 0.8007 - accuracy: 0.7623 - val_loss: 1.5731 - val_accuracy: 0.5293\n",
      "Epoch 53/100\n",
      "401/401 [==============================] - 16s 39ms/step - loss: 0.7859 - accuracy: 0.7734 - val_loss: 1.6062 - val_accuracy: 0.5470\n",
      "Epoch 54/100\n",
      "401/401 [==============================] - 15s 38ms/step - loss: 0.7833 - accuracy: 0.7658 - val_loss: 1.6941 - val_accuracy: 0.5314\n",
      "Epoch 55/100\n",
      "401/401 [==============================] - 15s 38ms/step - loss: 0.7868 - accuracy: 0.7734 - val_loss: 1.7301 - val_accuracy: 0.5329\n",
      "Epoch 56/100\n",
      "401/401 [==============================] - 15s 37ms/step - loss: 0.7812 - accuracy: 0.7760 - val_loss: 1.6373 - val_accuracy: 0.5300\n",
      "Epoch 57/100\n",
      "401/401 [==============================] - 15s 38ms/step - loss: 0.7838 - accuracy: 0.7797 - val_loss: 1.6616 - val_accuracy: 0.5180\n",
      "Epoch 58/100\n",
      "401/401 [==============================] - 16s 40ms/step - loss: 0.7741 - accuracy: 0.7837 - val_loss: 1.6247 - val_accuracy: 0.5216\n",
      "Epoch 59/100\n",
      "401/401 [==============================] - 16s 40ms/step - loss: 0.7595 - accuracy: 0.7868 - val_loss: 1.9254 - val_accuracy: 0.4876\n",
      "Epoch 60/100\n",
      "401/401 [==============================] - 15s 37ms/step - loss: 0.7577 - accuracy: 0.7853 - val_loss: 1.6997 - val_accuracy: 0.5032\n",
      "Epoch 61/100\n",
      "401/401 [==============================] - 19s 48ms/step - loss: 0.7442 - accuracy: 0.7938 - val_loss: 1.6526 - val_accuracy: 0.5343\n"
     ]
    }
   ],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15)\n",
    "history = cnn_model.fit(x_train, y_train, epochs=100, batch_size=20, validation_data=(x_val, y_val), callbacks = es_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.653897225856781 / Train accuracy: 0.8326886892318726\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model.evaluate(x_train, y_train, verbose=0)\n",
    "print(f'Train loss: {score[0]} / Train accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.6942102909088135 / Test accuracy: 0.5391095280647278\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model.evaluate(np.array(test_img) / 255.0, test_df_with_img['account_type_multi'], verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN + CNN\n",
    "train a neural network that combines categorical/numerical attributes with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_df_with_img2.drop(to_drop, axis=1), train_df_with_img2['account_type_multi']\n",
    "x_test, y_test = test_df_with_img2.drop(to_drop, axis=1), test_df_with_img2['account_type_multi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_img, x_train_attr = np.stack(x_train['img']) / 255.0, x_train.drop('img', axis=1)\n",
    "x_val_img, x_val_attr = np.stack(x_val['img']) / 255.0, x_val.drop('img', axis=1)\n",
    "x_test_img, x_test_attr = np.stack(x_test['img']) / 255.0, x_test.drop('img', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ann():\n",
    "    ann_model = Sequential()\n",
    "    ann_model.add(Dense(64, activation = 'relu', input_dim = 222))\n",
    "    ann_model.add(Dropout(.1))\n",
    "    ann_model.add(Dense(128, activation='relu'))\n",
    "    return ann_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():    \n",
    "    base_model = InceptionV3(input_shape = (75, 75, 3), include_top = False, weights = 'imagenet')\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    x = Flatten()(base_model.output)\n",
    "    x = Dense(70, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(50, activation='softmax')(x)\n",
    "    cnn_model = Model(base_model.input, x)\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = create_ann()\n",
    "cnn_model = create_cnn()\n",
    "combined_input = concatenate([ann_model.output, cnn_model.output])\n",
    "x = Dense(50, activation=\"relu\")(combined_input)\n",
    "x = Dense(6, activation=\"softmax\")(x)\n",
    "combined_model = Model(inputs=[ann_model.input, cnn_model.input], outputs=x)\n",
    "combined_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "161/161 [==============================] - 38s 202ms/step - loss: 0.5380 - accuracy: 0.8196 - val_loss: 0.2438 - val_accuracy: 0.9223\n",
      "Epoch 2/20\n",
      "161/161 [==============================] - 39s 244ms/step - loss: 0.2178 - accuracy: 0.9329 - val_loss: 0.1329 - val_accuracy: 0.9583\n",
      "Epoch 3/20\n",
      "161/161 [==============================] - 45s 278ms/step - loss: 0.1412 - accuracy: 0.9592 - val_loss: 0.0962 - val_accuracy: 0.9654\n",
      "Epoch 4/20\n",
      "161/161 [==============================] - 36s 224ms/step - loss: 0.1096 - accuracy: 0.9692 - val_loss: 0.1078 - val_accuracy: 0.9604\n",
      "Epoch 5/20\n",
      "161/161 [==============================] - 37s 231ms/step - loss: 0.0967 - accuracy: 0.9735 - val_loss: 0.0969 - val_accuracy: 0.9675\n",
      "Epoch 6/20\n",
      "161/161 [==============================] - 39s 242ms/step - loss: 0.0884 - accuracy: 0.9752 - val_loss: 0.0848 - val_accuracy: 0.9731\n",
      "Epoch 7/20\n",
      "161/161 [==============================] - 39s 244ms/step - loss: 0.0787 - accuracy: 0.9784 - val_loss: 0.0723 - val_accuracy: 0.9802\n",
      "Epoch 8/20\n",
      "161/161 [==============================] - 36s 222ms/step - loss: 0.0700 - accuracy: 0.9813 - val_loss: 0.0882 - val_accuracy: 0.9731\n",
      "Epoch 9/20\n",
      "161/161 [==============================] - 38s 237ms/step - loss: 0.0594 - accuracy: 0.9825 - val_loss: 0.0684 - val_accuracy: 0.9760\n",
      "Epoch 10/20\n",
      "161/161 [==============================] - 35s 215ms/step - loss: 0.0608 - accuracy: 0.9827 - val_loss: 0.0657 - val_accuracy: 0.9809\n",
      "Epoch 11/20\n",
      "161/161 [==============================] - 35s 216ms/step - loss: 0.0586 - accuracy: 0.9805 - val_loss: 0.0662 - val_accuracy: 0.9781\n",
      "Epoch 12/20\n",
      "161/161 [==============================] - 35s 220ms/step - loss: 0.0529 - accuracy: 0.9848 - val_loss: 0.0620 - val_accuracy: 0.9830\n",
      "Epoch 13/20\n",
      "161/161 [==============================] - 32s 199ms/step - loss: 0.0509 - accuracy: 0.9859 - val_loss: 0.0645 - val_accuracy: 0.9809\n",
      "Epoch 14/20\n",
      "161/161 [==============================] - 34s 213ms/step - loss: 0.0484 - accuracy: 0.9858 - val_loss: 0.0596 - val_accuracy: 0.9845\n",
      "Epoch 15/20\n",
      "161/161 [==============================] - 36s 221ms/step - loss: 0.0437 - accuracy: 0.9869 - val_loss: 0.0592 - val_accuracy: 0.9852\n",
      "Epoch 16/20\n",
      "161/161 [==============================] - 32s 202ms/step - loss: 0.0411 - accuracy: 0.9871 - val_loss: 0.0634 - val_accuracy: 0.9802\n",
      "Epoch 17/20\n",
      "161/161 [==============================] - 30s 187ms/step - loss: 0.0434 - accuracy: 0.9881 - val_loss: 0.0616 - val_accuracy: 0.9809\n",
      "Epoch 18/20\n",
      "161/161 [==============================] - 36s 226ms/step - loss: 0.0368 - accuracy: 0.9893 - val_loss: 0.0601 - val_accuracy: 0.9823\n",
      "Epoch 19/20\n",
      "161/161 [==============================] - 36s 227ms/step - loss: 0.0384 - accuracy: 0.9876 - val_loss: 0.0622 - val_accuracy: 0.9809\n",
      "Epoch 20/20\n",
      "161/161 [==============================] - 32s 200ms/step - loss: 0.0413 - accuracy: 0.9876 - val_loss: 0.0690 - val_accuracy: 0.9816\n",
      "Total time taken for the program execution 722.1817548274994\n"
     ]
    }
   ],
   "source": [
    "#es_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15)\n",
    "start_time = time.time()\n",
    "combined_model.fit(\n",
    "\tx=[x_train_attr, x_train_img], y=y_train,\n",
    "\tvalidation_data=([x_val_attr, x_val_img], y_val),\n",
    "\tepochs=20, batch_size=50)\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Total time taken for the program execution\", time_taken) # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train loss: 0.026401977986097336 / Train accuracy: 0.9920149445533752\n"
     ]
    }
   ],
   "source": [
    "score = combined_model.evaluate([x_train_attr, x_train_img], y_train, verbose=0)\n",
    "print(f'Train loss: {score[0]} / Train accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test loss: 0.1009983941912651 / Test accuracy: 0.9753309488296509\n"
     ]
    }
   ],
   "source": [
    "score = combined_model.evaluate([x_test_attr, x_test_img], y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "52/52 [==============================] - 7s 103ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = combined_model.predict([x_test_attr, x_test_img])\n",
    "pred = np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0    0.94159   0.97816   0.95952       412\n           1    0.98911   0.98696   0.98803       460\n           2    0.98134   0.95985   0.97048       274\n           3    0.98817   0.97093   0.97947       516\n\n    accuracy                        0.97533      1662\n   macro avg    0.97505   0.97397   0.97438      1662\nweighted avg    0.97576   0.97533   0.97541      1662\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "77238a471535228e8cd55a3ca9e771a69c6c0bc66c44a56c972f9554a4042742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}