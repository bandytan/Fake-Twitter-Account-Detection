{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN + CNN + LSTM\n",
    "train a neural network that combines categorical/numerical attributes with images and text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import shap\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense, Dropout, BatchNormalization, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/twitter_data_train_multiclass.csv')\n",
    "test_df = pd.read_csv('data/twitter_data_test_multiclass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() \n",
    "train_df[['statuses_count', 'favourites_count', 'followers_count', 'friends_count', 'number_of_mentions', 'listed_count', 'number_of_tags']] = scaler.fit_transform(train_df[['statuses_count', 'favourites_count', 'followers_count', 'friends_count', 'number_of_mentions', 'listed_count', 'number_of_tags']])\n",
    "test_df[['statuses_count', 'favourites_count', 'followers_count', 'friends_count', 'number_of_mentions', 'listed_count', 'number_of_tags']] = scaler.transform(test_df[['statuses_count', 'favourites_count', 'followers_count', 'friends_count', 'number_of_mentions', 'listed_count', 'number_of_tags']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_base = \"data/new batch profile pics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all image pathnames from base\n",
    "# list to store files\n",
    "res = []\n",
    "res2 = []\n",
    "\n",
    "# Iterate directory\n",
    "for path in os.listdir(faces_base):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(faces_base, path)):\n",
    "        res.append(faces_base + path)\n",
    "        res2.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: profile 'icc': 'RGB ': RGB color space not permitted on grayscale PNG\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: profile 'icc': 'RGB ': RGB color space not permitted on grayscale PNG\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "train_img_dict2 = {}\n",
    "test_img_dict2 = {}\n",
    "train_img2 = []\n",
    "test_img2 = []\n",
    "for i in range(len(res)):\n",
    "    pic = res[i]\n",
    "    id_name = re.match(r\"[^\\/\\\\]+(?=\\.png|\\.jpg)\", res2[i]).group(0)\n",
    "    try:\n",
    "        img = cv2.imread(pic)\n",
    "        if img is None:\n",
    "            print(\"none\")\n",
    "            continue\n",
    "        img = cv2.resize(img, (75, 75))\n",
    "        if int(id_name) in list(train_df['id']):\n",
    "            train_img_dict2[int(id_name)] = img \n",
    "            train_img2.append(img)\n",
    "        elif int(id_name) in list(test_df['id']):\n",
    "            test_img_dict2[int(id_name)] = img\n",
    "            test_img2.append(img)\n",
    "        #img.close()\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['description_processed'] = train_df['description_processed'].apply(str)\n",
    "test_df['description_processed'] = test_df['description_processed'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 16349 unique tokens.\nFound 4366 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(train_df['description_processed'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "tokenizer_test = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer_test.fit_on_texts(test_df['description_processed'].values)\n",
    "word_index_test = tokenizer_test.word_index\n",
    "print('Found %s unique tokens.' % len(word_index_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_df2 = pd.DataFrame(train_img_dict2.items(), columns = ['id', 'img'])  \n",
    "test_img_df2 = pd.DataFrame(test_img_dict2.items(), columns = ['id', 'img'])  \n",
    "train_df_with_img2 = pd.merge(train_img_df2, train_df, on='id')\n",
    "test_df_with_img2 = pd.merge(test_img_df2, test_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of data tensor: (9430, 250)\nShape of data tensor: (1662, 250)\n"
     ]
    }
   ],
   "source": [
    "LSTM_X = tokenizer.texts_to_sequences(train_df_with_img2['description_processed'].values)\n",
    "LSTM_X_train = pad_sequences(LSTM_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', LSTM_X_train.shape)\n",
    "\n",
    "LSTM_x = tokenizer_test.texts_to_sequences(test_df_with_img2['description_processed'].values)\n",
    "LSTM_X_test = pad_sequences(LSTM_x, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', LSTM_X_test.shape)"
   ]
  },
  {
   "source": [
    "### ANN + CNN Model training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Unnamed: 0', 'screen_name', 'url', 'profile_image_url', 'description',\n",
    "           'id', 'name', 'account_type', 'tweets_list', 'tweets_list_processed',\n",
    "          'description_processed', 'protected', 'verified', 'account_type_multi', \n",
    "          'profile_use_background_image', 'profile_background_tile']\n",
    "\n",
    "x_train, y_train = train_df_with_img2.drop(to_drop, axis=1), train_df_with_img2['account_type_multi']\n",
    "x_test, y_test = test_df_with_img2.drop(to_drop, axis=1), test_df_with_img2['account_type_multi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_img, x_train_attr, x_train_text = np.stack(x_train['img']) / 255.0, x_train.drop('img', axis=1), LSTM_X_train\n",
    "x_test_img, x_test_attr, x_test_text = np.stack(x_test['img']) / 255.0, x_test.drop('img', axis=1), LSTM_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ann():\n",
    "    ann_model = Sequential()\n",
    "    ann_model.add(Dense(64, activation = 'relu', input_dim = 221))\n",
    "    ann_model.add(Dropout(.1))\n",
    "    ann_model.add(Dense(128, activation='relu'))\n",
    "    return ann_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():    \n",
    "    base_model = InceptionV3(input_shape = (75, 75, 3), include_top = False, weights = 'imagenet')\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    x = Flatten()(base_model.output)\n",
    "    x = Dense(70, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(50, activation='softmax')(x)\n",
    "    cnn_model = Model(base_model.input, x)\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-11-16 11:28:08.571282: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "ann_model = create_ann()\n",
    "cnn_model = create_cnn()\n",
    "combined_input = concatenate([ann_model.output, cnn_model.output])\n",
    "x = Dense(50, activation=\"relu\")(combined_input)\n",
    "x = Dense(4, activation=\"softmax\")(x)\n",
    "combined_model = Model(inputs=[ann_model.input, cnn_model.input], outputs=x)\n",
    "combined_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "161/161 [==============================] - 38s 215ms/step - loss: 0.6040 - accuracy: 0.7959 - val_loss: 0.2697 - val_accuracy: 0.9180\n",
      "Epoch 2/20\n",
      "161/161 [==============================] - 29s 179ms/step - loss: 0.2429 - accuracy: 0.9283 - val_loss: 0.1595 - val_accuracy: 0.9498\n",
      "Epoch 3/20\n",
      "161/161 [==============================] - 31s 190ms/step - loss: 0.1714 - accuracy: 0.9495 - val_loss: 0.1401 - val_accuracy: 0.9463\n",
      "Epoch 4/20\n",
      "161/161 [==============================] - 31s 192ms/step - loss: 0.1240 - accuracy: 0.9632 - val_loss: 0.1157 - val_accuracy: 0.9583\n",
      "Epoch 5/20\n",
      "161/161 [==============================] - 28s 172ms/step - loss: 0.1054 - accuracy: 0.9697 - val_loss: 0.1356 - val_accuracy: 0.9548\n",
      "Epoch 6/20\n",
      "161/161 [==============================] - 27s 171ms/step - loss: 0.0950 - accuracy: 0.9701 - val_loss: 0.0941 - val_accuracy: 0.9703\n",
      "Epoch 7/20\n",
      "161/161 [==============================] - 27s 168ms/step - loss: 0.0879 - accuracy: 0.9739 - val_loss: 0.0825 - val_accuracy: 0.9739\n",
      "Epoch 8/20\n",
      "161/161 [==============================] - 26s 162ms/step - loss: 0.0775 - accuracy: 0.9759 - val_loss: 0.0911 - val_accuracy: 0.9696\n",
      "Epoch 9/20\n",
      "161/161 [==============================] - 26s 160ms/step - loss: 0.0782 - accuracy: 0.9773 - val_loss: 0.0954 - val_accuracy: 0.9717\n",
      "Epoch 10/20\n",
      "161/161 [==============================] - 27s 169ms/step - loss: 0.0643 - accuracy: 0.9800 - val_loss: 0.0874 - val_accuracy: 0.9717\n",
      "Epoch 11/20\n",
      "161/161 [==============================] - 28s 173ms/step - loss: 0.0642 - accuracy: 0.9810 - val_loss: 0.0876 - val_accuracy: 0.9710\n",
      "Epoch 12/20\n",
      "161/161 [==============================] - 28s 175ms/step - loss: 0.0555 - accuracy: 0.9829 - val_loss: 0.0932 - val_accuracy: 0.9717\n",
      "Epoch 13/20\n",
      "161/161 [==============================] - 29s 183ms/step - loss: 0.0518 - accuracy: 0.9829 - val_loss: 0.1033 - val_accuracy: 0.9689\n",
      "Epoch 14/20\n",
      "161/161 [==============================] - 27s 166ms/step - loss: 0.0557 - accuracy: 0.9820 - val_loss: 0.0883 - val_accuracy: 0.9724\n",
      "Epoch 15/20\n",
      "161/161 [==============================] - 26s 159ms/step - loss: 0.0538 - accuracy: 0.9832 - val_loss: 0.0943 - val_accuracy: 0.9682\n",
      "Epoch 16/20\n",
      "161/161 [==============================] - 26s 161ms/step - loss: 0.0450 - accuracy: 0.9862 - val_loss: 0.0838 - val_accuracy: 0.9760\n",
      "Epoch 17/20\n",
      "161/161 [==============================] - 27s 167ms/step - loss: 0.0415 - accuracy: 0.9863 - val_loss: 0.0937 - val_accuracy: 0.9717\n",
      "Epoch 18/20\n",
      "161/161 [==============================] - 26s 159ms/step - loss: 0.0384 - accuracy: 0.9867 - val_loss: 0.0970 - val_accuracy: 0.9689\n",
      "Epoch 19/20\n",
      "161/161 [==============================] - 26s 159ms/step - loss: 0.0384 - accuracy: 0.9876 - val_loss: 0.0910 - val_accuracy: 0.9703\n",
      "Epoch 20/20\n",
      "161/161 [==============================] - 26s 164ms/step - loss: 0.0350 - accuracy: 0.9888 - val_loss: 0.0879 - val_accuracy: 0.9753\n",
      "Total time taken for the program execution 559.6284792423248\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "combined_model.fit(\n",
    "\tx=[x_train_attr, x_train_img], y=y_train,\n",
    "\tvalidation_split=0.15,\n",
    "\tepochs=20, batch_size=50)\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Total time taken for the program execution\", time_taken) # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train loss: 0.03053157776594162 / Train accuracy: 0.9915164113044739\n",
      "Test loss: 0.12443032115697861 / Test accuracy: 0.9741275310516357\n"
     ]
    }
   ],
   "source": [
    "score = combined_model.evaluate([x_train_attr, x_train_img], y_train, verbose=0)\n",
    "print(f'Train loss: {score[0]} / Train accuracy: {score[1]}')\n",
    "score = combined_model.evaluate([x_test_attr, x_test_img], y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "52/52 [==============================] - 7s 91ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.94976   0.96359   0.95663       412\n",
      "           1    0.97430   0.98913   0.98166       460\n",
      "           2    0.97059   0.96350   0.96703       274\n",
      "           3    0.99604   0.97481   0.98531       516\n",
      "\n",
      "    accuracy                        0.97413      1662\n",
      "   macro avg    0.97267   0.97276   0.97266      1662\n",
      "weighted avg    0.97436   0.97413   0.97418      1662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = combined_model.predict([x_test_attr, x_test_img])\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(classification_report(y_test, pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN + CNN + LSTM Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():    \n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Embedding(50000, 128, input_length=250))\n",
    "    lstm_model.add(SpatialDropout1D(0.7))\n",
    "    lstm_model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = create_ann()\n",
    "cnn_model = create_cnn()\n",
    "lstm_model = create_lstm()\n",
    "combined_input = concatenate([ann_model.output, cnn_model.output, lstm_model.output])\n",
    "x = Dense(50, activation=\"relu\")(combined_input)\n",
    "x = Dense(4, activation=\"softmax\")(x)\n",
    "combined_model = Model(inputs=[ann_model.input, cnn_model.input, lstm_model.input], outputs=x)\n",
    "combined_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "161/161 [==============================] - 102s 572ms/step - loss: 0.5202 - accuracy: 0.8221 - val_loss: 0.2717 - val_accuracy: 0.9095\n",
      "Epoch 2/20\n",
      "161/161 [==============================] - 88s 548ms/step - loss: 0.2274 - accuracy: 0.9336 - val_loss: 0.1925 - val_accuracy: 0.9413\n",
      "Epoch 3/20\n",
      "161/161 [==============================] - 84s 520ms/step - loss: 0.1418 - accuracy: 0.9546 - val_loss: 0.1373 - val_accuracy: 0.9590\n",
      "Epoch 4/20\n",
      "161/161 [==============================] - 84s 520ms/step - loss: 0.1182 - accuracy: 0.9641 - val_loss: 0.1118 - val_accuracy: 0.9611\n",
      "Epoch 5/20\n",
      "161/161 [==============================] - 89s 550ms/step - loss: 0.0906 - accuracy: 0.9726 - val_loss: 0.1085 - val_accuracy: 0.9689\n",
      "Epoch 6/20\n",
      "161/161 [==============================] - 86s 536ms/step - loss: 0.0920 - accuracy: 0.9740 - val_loss: 0.0887 - val_accuracy: 0.9724\n",
      "Epoch 7/20\n",
      "161/161 [==============================] - 89s 551ms/step - loss: 0.0785 - accuracy: 0.9777 - val_loss: 0.0998 - val_accuracy: 0.9640\n",
      "Epoch 8/20\n",
      "161/161 [==============================] - 86s 534ms/step - loss: 0.0680 - accuracy: 0.9804 - val_loss: 0.0830 - val_accuracy: 0.9731\n",
      "Epoch 9/20\n",
      "161/161 [==============================] - 91s 568ms/step - loss: 0.0577 - accuracy: 0.9809 - val_loss: 0.0995 - val_accuracy: 0.9689\n",
      "Epoch 10/20\n",
      "161/161 [==============================] - 87s 543ms/step - loss: 0.0461 - accuracy: 0.9838 - val_loss: 0.0852 - val_accuracy: 0.9731\n",
      "Epoch 11/20\n",
      "161/161 [==============================] - 91s 567ms/step - loss: 0.0527 - accuracy: 0.9829 - val_loss: 0.1040 - val_accuracy: 0.9675\n",
      "Epoch 12/20\n",
      "161/161 [==============================] - 99s 617ms/step - loss: 0.0421 - accuracy: 0.9864 - val_loss: 0.1043 - val_accuracy: 0.9724\n",
      "Epoch 13/20\n",
      "161/161 [==============================] - 93s 578ms/step - loss: 0.0348 - accuracy: 0.9883 - val_loss: 0.1010 - val_accuracy: 0.9682\n",
      "Epoch 14/20\n",
      "161/161 [==============================] - 89s 551ms/step - loss: 0.0387 - accuracy: 0.9889 - val_loss: 0.1007 - val_accuracy: 0.9731\n",
      "Epoch 15/20\n",
      "161/161 [==============================] - 93s 580ms/step - loss: 0.0366 - accuracy: 0.9873 - val_loss: 0.0951 - val_accuracy: 0.9724\n",
      "Epoch 16/20\n",
      "161/161 [==============================] - 86s 535ms/step - loss: 0.0297 - accuracy: 0.9886 - val_loss: 0.0936 - val_accuracy: 0.9739\n",
      "Epoch 17/20\n",
      "161/161 [==============================] - 88s 547ms/step - loss: 0.0268 - accuracy: 0.9905 - val_loss: 0.1196 - val_accuracy: 0.9689\n",
      "Epoch 18/20\n",
      "161/161 [==============================] - 85s 528ms/step - loss: 0.0248 - accuracy: 0.9923 - val_loss: 0.1050 - val_accuracy: 0.9668\n",
      "Epoch 19/20\n",
      "161/161 [==============================] - 86s 534ms/step - loss: 0.0194 - accuracy: 0.9931 - val_loss: 0.0902 - val_accuracy: 0.9731\n",
      "Epoch 20/20\n",
      "161/161 [==============================] - 80s 497ms/step - loss: 0.0228 - accuracy: 0.9924 - val_loss: 0.1272 - val_accuracy: 0.9640\n",
      "Total time taken for the program execution 1779.1880779266357\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "combined_model.fit(\n",
    "\tx=[x_train_attr, x_train_img, x_train_text], y=y_train,\n",
    "\tvalidation_split=0.15,\n",
    "\tepochs=20, batch_size=50)\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Total time taken for the program execution\", time_taken) # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train loss: 0.03258036822080612 / Train accuracy: 0.9908801913261414\n",
      "Test loss: 0.29367831349372864 / Test accuracy: 0.9368231296539307\n"
     ]
    }
   ],
   "source": [
    "score = combined_model.evaluate([x_train_attr, x_train_img, x_train_text], y_train, verbose=0)\n",
    "print(f'Train loss: {score[0]} / Train accuracy: {score[1]}')\n",
    "score = combined_model.evaluate([x_test_attr, x_test_img, x_test_text], y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "52/52 [==============================] - 11s 143ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.87617   0.91019   0.89286       412\n",
      "           1    0.96781   0.98043   0.97408       460\n",
      "           2    0.91575   0.91241   0.91408       274\n",
      "           3    0.97172   0.93217   0.95153       516\n",
      "\n",
      "    accuracy                        0.93682      1662\n",
      "   macro avg    0.93286   0.93380   0.93314      1662\n",
      "weighted avg    0.93772   0.93682   0.93705      1662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = combined_model.predict([x_test_attr, x_test_img, x_test_text])\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(classification_report(y_test, pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}