{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66f02fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae51145",
   "metadata": {},
   "source": [
    "<h1>Import Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfb27ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/Users/ivankoh/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/1D/NUS Y3S1/BT4012/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ebfa370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv(base + \"combined_twitter_data_with_tweets_corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d60d3810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'id', 'name', 'screen_name',\n",
       "       'statuses_count', 'followers_count', 'friends_count',\n",
       "       'favourites_count', 'listed_count', 'url', 'lang', 'time_zone',\n",
       "       'location', 'default_profile', 'default_profile_image', 'geo_enabled',\n",
       "       'profile_image_url', 'profile_banner_url',\n",
       "       'profile_use_background_image', 'profile_background_image_url_https',\n",
       "       'profile_text_color', 'profile_image_url_https',\n",
       "       'profile_sidebar_border_color', 'profile_background_tile',\n",
       "       'profile_sidebar_fill_color', 'profile_background_image_url',\n",
       "       'profile_background_color', 'profile_link_color', 'utc_offset',\n",
       "       'protected', 'verified', 'description', 'created_at', 'updated',\n",
       "       'account_type', 'tweets_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da878038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dq/33rpc0cx4f54t0vx2cdcgczr0000gn/T/ipykernel_92529/3882822358.py:20: DtypeWarning: Columns (8,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_tweets = pd.read_csv(fn, encoding='ISO-8859-1')\n",
      "/var/folders/dq/33rpc0cx4f54t0vx2cdcgczr0000gn/T/ipykernel_92529/3882822358.py:22: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_tweets = pd.concat([df_tweets, pd.read_csv(fn, encoding='ISO-8859-1') ], axis=0)\n",
      "/var/folders/dq/33rpc0cx4f54t0vx2cdcgczr0000gn/T/ipykernel_92529/3882822358.py:22: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_tweets = pd.concat([df_tweets, pd.read_csv(fn, encoding='ISO-8859-1') ], axis=0)\n",
      "/var/folders/dq/33rpc0cx4f54t0vx2cdcgczr0000gn/T/ipykernel_92529/3882822358.py:22: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_tweets = pd.concat([df_tweets, pd.read_csv(fn, encoding='ISO-8859-1') ], axis=0)\n",
      "/var/folders/dq/33rpc0cx4f54t0vx2cdcgczr0000gn/T/ipykernel_92529/3882822358.py:22: DtypeWarning: Columns (7,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_tweets = pd.concat([df_tweets, pd.read_csv(fn, encoding='ISO-8859-1') ], axis=0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     df_tweets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(fn, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     df_tweets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_tweets, \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m ], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(inspect\u001b[38;5;241m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1789\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1786\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1787\u001b[0m         new_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index)\n\u001b[0;32m-> 1789\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msqueeze \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/core/frame.py:663\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    657\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    658\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    659\u001b[0m     )\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 663\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/core/internals/construction.py:494\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    492\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/core/internals/construction.py:155\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    152\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/core/internals/managers.py:2171\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[0;34m(arrays, axes, consolidate)\u001b[0m\n\u001b[1;32m   2169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, axes, e)\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/core/internals/managers.py:1848\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1848\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1849\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1850\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs \u001b[38;5;241m=\u001b[39m _consolidate_with_refs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/core/internals/managers.py:2296\u001b[0m, in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   2294\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[0;32m-> 2296\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2297\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[1;32m   2298\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2299\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pandas/core/internals/managers.py:2348\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[1;32m   2341\u001b[0m new_values: ArrayLike\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   2344\u001b[0m     \u001b[38;5;66;03m# error: List comprehension has incompatible type List[Union[ndarray,\u001b[39;00m\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;66;03m# ExtensionArray]]; expected List[Union[complex, generic,\u001b[39;00m\n\u001b[1;32m   2346\u001b[0m     \u001b[38;5;66;03m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[39;00m\n\u001b[1;32m   2347\u001b[0m     \u001b[38;5;66;03m# Sequence[Sequence[Any]], SupportsArray]]\u001b[39;00m\n\u001b[0;32m-> 2348\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2350\u001b[0m     bvals \u001b[38;5;241m=\u001b[39m [blk\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks]\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/numpy/core/shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    281\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get tweets df\n",
    "# save tweets dataset into local \n",
    "filenames_tweets = [\n",
    "    \"all tweets 2017/tweets_fake_followers.csv\",\n",
    "    \"all tweets 2017/tweets_genuine_accounts.csv\",\n",
    "    \"all tweets 2017/tweets_social_spambots_1.csv\",\n",
    "    \"all tweets 2017/tweets_social_spambots_2.csv\",\n",
    "    \"all tweets 2017/tweets_social_spambots_3.csv\",\n",
    "    \"all tweets 2017/tweets_traditional_spambots_1.csv\",\n",
    "\n",
    "    \"tweets 2015/tweets_E13.csv\",\n",
    "    \"tweets 2015/tweets_FSF.csv\",\n",
    "    \"tweets 2015/tweets_INT.csv\",\n",
    "    \"tweets 2015/tweets_TFP.csv\",\n",
    "    \"tweets 2015/tweets_TWT.csv\"\n",
    "]\n",
    "filenames_tweets = map(lambda x: base+x, filenames_tweets)\n",
    "for i,fn in enumerate(filenames_tweets):\n",
    "    if i == 0:\n",
    "        df_tweets = pd.read_csv(fn, encoding='ISO-8859-1')\n",
    "    else:\n",
    "        df_tweets = pd.concat([df_tweets, pd.read_csv(fn, encoding='ISO-8859-1') ], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets.dropna(subset = [\"user_id\"])  \n",
    "df_tweets[\"user_id\"] = df_tweets[\"user_id\"].apply(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3c55f",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b324af6f",
   "metadata": {},
   "source": [
    "### Remove columns that are redundant\n",
    "Data is redundant in helping us with our problem statement when:\n",
    "- The data is metadata\n",
    "- There are too many unique categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa513ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list = ['Unnamed: 0.1', 'Unnamed: 0', 'lang', 'time_zone', 'location', 'profile_banner_url', 'profile_background_image_url_https',\n",
    "       'profile_text_color', 'profile_image_url_https', 'profile_sidebar_border_color', 'profile_sidebar_fill_color',\n",
    "       'profile_background_image_url', 'profile_background_color', 'profile_link_color', 'utc_offset', 'created_at', 'updated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0009e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = df_users.drop(remove_list, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dd05b0",
   "metadata": {},
   "source": [
    "### Replace NaN values with zeros for binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users['default_profile'] = df_users['default_profile'].fillna(0)\n",
    "df_users['default_profile_image'] = df_users['default_profile_image'].fillna(0)\n",
    "df_users['geo_enabled'] = df_users['geo_enabled'].fillna(0)\n",
    "df_users['default_profile_image'] = df_users['default_profile_image'].fillna(0)\n",
    "df_users['profile_use_background_image'] = df_users['profile_use_background_image'].fillna(0)\n",
    "df_users['profile_background_tile'] = df_users['profile_background_tile'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6a871",
   "metadata": {},
   "source": [
    "<h2>Train Test Split (85-15)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the target variable - real or fake account type - binary classification problem\n",
    "df_users = df_users[(df_users['account_type'] == \"real\") | (df_users['account_type'] == \"fake\")]\n",
    "print(df_users['account_type'].value_counts())\n",
    "df_users['account_type'] = df_users['account_type'].apply(lambda x: 0 if x==\"fake\" else 1)\n",
    "\n",
    "train, test = train_test_split(df_users, test_size=0.15, random_state=69, stratify=df_users['account_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d646c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train size:\", len(train))\n",
    "print(\"test size\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7303ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['account_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa59a12",
   "metadata": {},
   "source": [
    "<h2>Date Formatting</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98154f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes around 10 min to run\n",
    "df_tweets['created_at_formatted'] = pd.to_datetime(df_tweets['timestamp'], infer_datetime_format=True, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38598aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['created_at_date'] = df_tweets['created_at_formatted'].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4398af",
   "metadata": {},
   "source": [
    "<h2>Tweet features</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f36ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_freq(df_users, df_tweets):\n",
    "    \n",
    "    # user tweet frequency = total number of tweets / number of user active days \n",
    "    # shows how often the user tweets among the days that a user tweets at least once. User activity is defined by whether the user tweets in a given day\n",
    "    # 1 = user tweets only once per active day \n",
    "    # >1 = user tweets more than once a day on average, in the days that the user is active \n",
    "\n",
    "    df_tweets_per_day = df_tweets.groupby(by=[\"user_id\"]).agg(tweet_count=('text', 'count'),\n",
    "                                                              date_count=('created_at_date', lambda x: x.nunique()))\n",
    "\n",
    "    dict_tweets_average = {user_id: df_tweets_per_day.loc[user_id]['tweet_count'] / df_tweets_per_day.loc[user_id]['date_count'] for user_id in df_tweets_per_day.index}\n",
    "    #create new column for user tweet frequency \n",
    "    df_users['tweet_frequency'] = df_users['id'].map(dict_tweets_average)\n",
    "    df_users['tweet_frequency'] = df_users['tweet_frequency'].fillna(0)\n",
    "    return df_users\n",
    "\n",
    "def tweet_tags_mention(df_users, df_tweets):\n",
    "    # average number of tags per post = total number of tags used per tweet \n",
    "    # average number of mentions per post = total number of mentions per tweet \n",
    "\n",
    "    df_tweets['text'] = df_tweets['text'].apply(str) #convert all text to string\n",
    "    df_tweets['number_of_tags'] = df_tweets['text'].apply(lambda x: x.count(\"#\"))\n",
    "    df_tweets['number_of_mentions'] = df_tweets['text'].apply(lambda x: x.count(\"@\"))\n",
    "    tags_dict = df_tweets.groupby(by=[\"user_id\"])['number_of_tags'].sum().to_dict()\n",
    "    mentions_dict = df_tweets.groupby(by=[\"user_id\"])['number_of_mentions'].sum().to_dict() \n",
    "\n",
    "    #create new column for number of tags\n",
    "    df_users['number_of_tags'] = df_users['id'].map(tags_dict)\n",
    "    #create new column for number of mentions\n",
    "    df_users['number_of_mentions'] = df_users['id'].map(mentions_dict)\n",
    "    \n",
    "    df_users['number_of_mentions'] = df_users['number_of_mentions'].fillna(0)\n",
    "    df_users['number_of_tags'] = df_users['number_of_tags'].fillna(0)\n",
    "    return df_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return 0 if weekend, 1 if weekday \n",
    "def is_weekday(dt):\n",
    "    return 0 if dt.weekday() > 4 else 1\n",
    "\n",
    "# return day of week \n",
    "def get_weekday(dt):\n",
    "    return dt.weekday()\n",
    "\n",
    "def get_weekend_weekday_frequency(df_tweets, df_users):\n",
    "    df_tweets['weekday'] = df_tweets['created_at_formatted'].apply(lambda x: is_weekday(x))\n",
    "    df_tweets_weekday_weekend = df_tweets.groupby(by=[\"user_id\", \"weekday\"]).agg(tweet_count=('text', 'count'),\n",
    "                                                          date_count=('created_at_date', lambda x: x.nunique()))\n",
    "    dict_tweets_weekend = {user_id: df_tweets_weekday_weekend.loc[(user_id, weekday)]['tweet_count'] / df_tweets_weekday_weekend.loc[(user_id, weekday)]['date_count'] for (user_id, weekday) in df_tweets_weekday_weekend.index if weekday == 0}\n",
    "    df_users['tweet_weekend_frequency'] = df_users['id'].map(dict_tweets_weekend)        \n",
    "    dict_tweets_weekday = {user_id: df_tweets_weekday_weekend.loc[(user_id, weekday)]['tweet_count'] / df_tweets_weekday_weekend.loc[(user_id, weekday)]['date_count'] for (user_id, weekday) in df_tweets_weekday_weekend.index if weekday == 1}\n",
    "    df_users['tweet_weekday_frequency'] = df_users['id'].map(dict_tweets_weekday)\n",
    "    \n",
    "    df_users['tweet_weekend_frequency'] = df_users['tweet_weekend_frequency'].fillna(0)\n",
    "    df_users['tweet_weekday_frequency'] = df_users['tweet_weekday_frequency'].fillna(0)\n",
    "    return df_users      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d1618",
   "metadata": {},
   "source": [
    "<h2>Followers To Following Ratio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_followers_following_ratio(df):\n",
    "    #followers divide by following (high means popular, low means more following)\n",
    "    df['following_to_followers_ratio'] = df['friends_count'] / df['followers_count']\n",
    "    df['following_to_followers_ratio'] = df['following_to_followers_ratio'].fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857742c6",
   "metadata": {},
   "source": [
    "<h2>Name Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_features(df):\n",
    "    #get length of username and screen name\n",
    "    df['username_length'] = df['name'].apply(lambda x: len(str(x)))\n",
    "    df['screen_name_length'] = df['screen_name'].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    #anything that is not a-z or 0-9 will be blocked, outputs length\n",
    "    df['username_spec_char_count'] = df['name'].apply(lambda x: len(re.findall(r'[^A-Za-z0-9]+', str(x))))\n",
    "    df['screen_name_spec_char_count'] = df['screen_name'].apply(lambda x: len(re.findall(r'[^A-Za-z0-9]+', str(x))))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ded964",
   "metadata": {},
   "source": [
    "<h2>Has URL Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26a4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_url_feature(df):\n",
    "    #1 if has url, 0 if no url\n",
    "    df['has_url'] = df['url'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b463fa9a",
   "metadata": {},
   "source": [
    "<h2>Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff62399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(df):\n",
    "    def process_tweets_list(corpus):\n",
    "        \n",
    "        corpus_processed = []\n",
    "        for tweet_list in corpus:\n",
    "            tweet_list = str(tweet_list)\n",
    "            row_processed = \"\"\n",
    "            \n",
    "            #replace RT and @\n",
    "            row_processed = tweet_list.replace(\"RT\", \"\" ) \n",
    "            row_processed = row_processed.replace(\"@\", \"\" )\n",
    "            \n",
    "            row_processed = re.sub(r'http\\S+', \"\", row_processed) #remove any URLs in tweets\n",
    "            row_processed = re.sub(r'[^\\x00-\\x7f]', \"\", row_processed) #remove Non-ASCII characters\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed if not row_processed == 'nan' else \"\") # handle NA\n",
    "            \n",
    "\n",
    "        return corpus_processed\n",
    "    \n",
    "    def process_description(corpus):\n",
    "        \n",
    "        corpus_processed = []\n",
    "        for row in corpus:\n",
    "            row = str(row)\n",
    "            row_processed = re.sub(r'[^\\x00-\\x7f]', \"\", row) #remove Non-ASCII characters\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed if not row_processed == 'nan' else \"\") # handle NA\n",
    "            \n",
    "        return corpus_processed\n",
    "    \n",
    "    df[\"tweets_list_processed\"] = process_tweets_list(df[\"tweets_list\"])\n",
    "    df[\"description_processed\"] = process_description(df[\"description\"])\n",
    "    \n",
    "    return df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac20b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmatizeTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nlp_features(df):\n",
    "    \n",
    "    #tweets\n",
    "    vect_tweets = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "    \n",
    "    tweets_processed = pd.Series(df[\"tweets_list_processed\"])\n",
    "    tfidf_fit_tweets = vect_tweets.fit(tweets_processed)\n",
    "    tweets_tfidf_array = tfidf_fit_tweets.transform(tweets_processed).toarray()\n",
    "    tweets_tfidf_df = pd.DataFrame(tweets_tfidf_array)\n",
    "    tweets_tfidf_df.columns = list(map(lambda x: \"tweets_\" + str(x), tweets_tfidf_df.columns))\n",
    "    df = pd.concat([df.reset_index(drop=True),tweets_tfidf_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    #description\n",
    "    vect_description = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "    \n",
    "    description_processed = pd.Series(df[\"description_processed\"])\n",
    "    tfidf_fit_description = vect_description.fit(description_processed)\n",
    "    description_tfidf_array = tfidf_fit_description.transform(description_processed).toarray()\n",
    "    description_tfidf_df = pd.DataFrame(description_tfidf_array)\n",
    "    description_tfidf_df.columns = list(map(lambda x: \"description_\" + str(x), description_tfidf_df.columns))\n",
    "    df = pd.concat([df.reset_index(drop=True),description_tfidf_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    return (df, tfidf_fit_tweets, tfidf_fit_description)\n",
    "\n",
    "def nlp_transform_test(df, tfidf_fit_tweets, tfidf_fit_description):\n",
    "    tweets_tfidf_array = tfidf_fit_tweets.transform(df['tweets_list_processed']).toarray()\n",
    "    tweets_tfidf_df = pd.DataFrame(tweets_tfidf_array)\n",
    "    tweets_tfidf_df.columns = list(map(lambda x : \"tweets_\" + str(x), tweets_tfidf_df.columns))\n",
    "    df = pd.concat([df.reset_index(drop=True),tweets_tfidf_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    description_tfidf_array = tfidf_fit_description.transform(df['description_processed']).toarray()\n",
    "    description_tfidf_df = pd.DataFrame(description_tfidf_array)\n",
    "    description_tfidf_df.columns = list(map(lambda x : \"description_\" + str(x), description_tfidf_df.columns))\n",
    "    df = pd.concat([df.reset_index(drop=True),description_tfidf_df.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4ac4b",
   "metadata": {},
   "source": [
    "<h2>Combine all Feature Generating Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff95beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tweet_freq(train, df_tweets)\n",
    "result = tweet_tags_mention(result, df_tweets)\n",
    "result = get_weekend_weekday_frequency(df_tweets, result)\n",
    "result = create_followers_following_ratio(result)\n",
    "result = name_features(result)\n",
    "result = has_url_feature(result)\n",
    "result = clean_texts(result)\n",
    "result, tfidf_fit_tweets, tfidf_fit_description = generate_nlp_features(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615aeb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6ad70",
   "metadata": {},
   "source": [
    "## Apply same feature engineering on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae570c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tweet_freq(test, df_tweets)\n",
    "test = tweet_tags_mention(test, df_tweets)\n",
    "test = get_weekend_weekday_frequency(df_tweets, test)\n",
    "test = create_followers_following_ratio(test)\n",
    "test = name_features(test)\n",
    "test = has_url_feature(test)\n",
    "test = clean_texts(test)\n",
    "test = nlp_transform_test(test, tfidf_fit_tweets, tfidf_fit_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test.columns))\n",
    "print(len(result.columns))\n",
    "\n",
    "for i in result.columns:\n",
    "    if i not in test.columns:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d477bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tweets_99.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e4e13",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(base + \"twitter_data_train.csv\", index=False)\n",
    "test.to_csv(base + \"twitter_data_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd5870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (bt4012)",
   "language": "python",
   "name": "bt4012"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
