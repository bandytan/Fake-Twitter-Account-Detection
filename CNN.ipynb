{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-11-16 11:11:13.364911: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/twitter_data_train_multiclass.csv\")\n",
    "test_df = pd.read_csv(\"data/twitter_data_test_multiclass.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_base = \"data/new batch profile pics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all image pathnames from base\n",
    "# list to store files\n",
    "res = []\n",
    "res2 = []\n",
    "\n",
    "# Iterate directory\n",
    "for path in os.listdir(faces_base):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(faces_base, path)):\n",
    "        res.append(faces_base + path)\n",
    "        res2.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre trained CNN: InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: profile 'icc': 'RGB ': RGB color space not permitted on grayscale PNG\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: profile 'icc': 'RGB ': RGB color space not permitted on grayscale PNG\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "train_img_dict2 = {}\n",
    "test_img_dict2 = {}\n",
    "train_img2 = []\n",
    "test_img2 = []\n",
    "for i in range(len(res)):\n",
    "    pic = res[i]\n",
    "    id_name = re.match(r\"[^\\/\\\\]+(?=\\.png|\\.jpg)\", res2[i]).group(0)\n",
    "    try:\n",
    "        img = cv2.imread(pic)\n",
    "        if img is None:\n",
    "            print(\"none\")\n",
    "            continue\n",
    "        img = cv2.resize(img, (75, 75))\n",
    "        if int(id_name) in list(train_df['id']):\n",
    "            train_img_dict2[int(id_name)] = img \n",
    "            train_img2.append(img)\n",
    "        elif int(id_name) in list(test_df['id']):\n",
    "            test_img_dict2[int(id_name)] = img\n",
    "            test_img2.append(img)\n",
    "        #img.close()\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_df2 = pd.DataFrame(train_img_dict2.items(), columns = ['id', 'img'])  \n",
    "test_img_df2 = pd.DataFrame(test_img_dict2.items(), columns = ['id', 'img'])  \n",
    "train_df_with_img2 = pd.merge(train_img_df2, train_df, on='id')\n",
    "test_df_with_img2 = pd.merge(test_img_df2, test_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-11-16 11:12:25.023570: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "base_model = InceptionV3(input_shape = (75, 75, 3), include_top = False, weights = 'imagenet')\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(70, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(4, activation='softmax')(x)\n",
    "model = Model(base_model.input, x)\n",
    "# configure the model\n",
    "model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, x_val2, y_train2, y_val2 = train_test_split(np.array(train_img2) / 255.0, train_df_with_img2['account_type_multi'], test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "161/161 [==============================] - 43s 237ms/step - loss: 1.1449 - accuracy: 0.4746 - val_loss: 1.0201 - val_accuracy: 0.5442\n",
      "Epoch 2/20\n",
      "161/161 [==============================] - 36s 223ms/step - loss: 1.0231 - accuracy: 0.5329 - val_loss: 1.0120 - val_accuracy: 0.5456\n",
      "Epoch 3/20\n",
      "161/161 [==============================] - 29s 178ms/step - loss: 0.9816 - accuracy: 0.5586 - val_loss: 0.9832 - val_accuracy: 0.5675\n",
      "Epoch 4/20\n",
      "161/161 [==============================] - 29s 181ms/step - loss: 0.9335 - accuracy: 0.5891 - val_loss: 0.9671 - val_accuracy: 0.5823\n",
      "Epoch 5/20\n",
      "161/161 [==============================] - 29s 179ms/step - loss: 0.8843 - accuracy: 0.6026 - val_loss: 0.9889 - val_accuracy: 0.5682\n",
      "Epoch 6/20\n",
      "161/161 [==============================] - 28s 172ms/step - loss: 0.8600 - accuracy: 0.6178 - val_loss: 0.9736 - val_accuracy: 0.5760\n",
      "Epoch 7/20\n",
      "161/161 [==============================] - 25s 155ms/step - loss: 0.8188 - accuracy: 0.6343 - val_loss: 0.9749 - val_accuracy: 0.5880\n",
      "Epoch 8/20\n",
      "161/161 [==============================] - 24s 152ms/step - loss: 0.7846 - accuracy: 0.6517 - val_loss: 0.9883 - val_accuracy: 0.5852\n",
      "Epoch 9/20\n",
      "161/161 [==============================] - 28s 173ms/step - loss: 0.7573 - accuracy: 0.6710 - val_loss: 0.9763 - val_accuracy: 0.5922\n",
      "Epoch 10/20\n",
      "161/161 [==============================] - 27s 167ms/step - loss: 0.7168 - accuracy: 0.6831 - val_loss: 1.0002 - val_accuracy: 0.5852\n",
      "Epoch 11/20\n",
      "161/161 [==============================] - 29s 178ms/step - loss: 0.6885 - accuracy: 0.6998 - val_loss: 1.0111 - val_accuracy: 0.5816\n",
      "Epoch 12/20\n",
      "161/161 [==============================] - 25s 155ms/step - loss: 0.6505 - accuracy: 0.7170 - val_loss: 1.0137 - val_accuracy: 0.5880\n",
      "Epoch 13/20\n",
      "161/161 [==============================] - 25s 157ms/step - loss: 0.6407 - accuracy: 0.7233 - val_loss: 1.0455 - val_accuracy: 0.5873\n",
      "Epoch 14/20\n",
      "161/161 [==============================] - 25s 156ms/step - loss: 0.6020 - accuracy: 0.7427 - val_loss: 1.0535 - val_accuracy: 0.5880\n",
      "Epoch 15/20\n",
      "161/161 [==============================] - 25s 157ms/step - loss: 0.5732 - accuracy: 0.7481 - val_loss: 1.0567 - val_accuracy: 0.5866\n",
      "Epoch 16/20\n",
      "161/161 [==============================] - 26s 161ms/step - loss: 0.5497 - accuracy: 0.7649 - val_loss: 1.0854 - val_accuracy: 0.5866\n",
      "Epoch 17/20\n",
      "161/161 [==============================] - 25s 157ms/step - loss: 0.5194 - accuracy: 0.7748 - val_loss: 1.0844 - val_accuracy: 0.6014\n",
      "Epoch 18/20\n",
      "161/161 [==============================] - 26s 159ms/step - loss: 0.5116 - accuracy: 0.7808 - val_loss: 1.1294 - val_accuracy: 0.5845\n",
      "Epoch 19/20\n",
      "161/161 [==============================] - 27s 168ms/step - loss: 0.4966 - accuracy: 0.7853 - val_loss: 1.1280 - val_accuracy: 0.5922\n",
      "Epoch 20/20\n",
      "161/161 [==============================] - 26s 164ms/step - loss: 0.4837 - accuracy: 0.7961 - val_loss: 1.1681 - val_accuracy: 0.5965\n",
      "Total time taken for the program execution 558.3341519832611\n"
     ]
    }
   ],
   "source": [
    "#es_callback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15)\n",
    "start_time = time.time()\n",
    "inc_history = model.fit(x_train2, y_train2, epochs=20, batch_size=50, validation_data=(x_val2, y_val2))\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Total time taken for the program execution\", time_taken) # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train loss: 0.26304110884666443 / Train accuracy: 0.9310043454170227\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train2, y_train2, verbose=0)\n",
    "print(f'Train loss: {score[0]} / Train accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test loss: 1.1885908842086792 / Test accuracy: 0.5866426229476929\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(np.array(test_img2) / 255.0, test_df_with_img2['account_type_multi'], verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "52/52 [==============================] - 11s 145ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.43421   0.40049   0.41667       412\n",
      "           1    0.47119   0.49783   0.48414       460\n",
      "           2    0.94714   0.78467   0.85828       274\n",
      "           3    0.64323   0.70930   0.67465       516\n",
      "\n",
      "    accuracy                        0.58664      1662\n",
      "   macro avg    0.62394   0.59807   0.60844      1662\n",
      "weighted avg    0.59390   0.58664   0.58825      1662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(np.array(test_img2) / 255.0)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(classification_report(test_df_with_img2['account_type_multi'], pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: profile 'icc': 'RGB ': RGB color space not permitted on grayscale PNG\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: profile 'icc': 'RGB ': RGB color space not permitted on grayscale PNG\n",
      "none\n",
      "none\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "train_img_dict = {}\n",
    "test_img_dict = {}\n",
    "train_img = []\n",
    "test_img = []\n",
    "for i in range(len(res)):\n",
    "    pic = res[i]\n",
    "    id_name = re.match(r\"[^\\/\\\\]+(?=\\.png|\\.jpg)\", res2[i]).group(0)\n",
    "    try:\n",
    "        img = cv2.imread(pic)\n",
    "        if img is None:\n",
    "            print(\"none\")\n",
    "            continue\n",
    "        if int(id_name) in list(train_df['id']):\n",
    "            train_img_dict[int(id_name)] = img \n",
    "            train_img.append(img)\n",
    "        elif int(id_name) in list(test_df['id']):\n",
    "            test_img_dict[int(id_name)] = img\n",
    "            test_img.append(img)\n",
    "        #img.close()\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_df = pd.DataFrame(train_img_dict.items(), columns = ['id', 'img'])  \n",
    "test_img_df = pd.DataFrame(test_img_dict.items(), columns = ['id', 'img'])  \n",
    "train_df_with_img = pd.merge(train_img_df, train_df, on='id')\n",
    "test_df_with_img = pd.merge(test_img_df, test_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = (3, 3)\n",
    "stride_size = (1, 1)\n",
    "filters = [4, 10, 24]\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Input(shape=(48, 48, 3)))\n",
    "for f in filters:\n",
    "    cnn_model.add(Conv2D(f, kernel_size, strides=stride_size, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "    cnn_model.add(Conv2D(f, kernel_size, strides=stride_size, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "    cnn_model.add(BatchNormalization(axis = -1))\n",
    "    cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(100, activation='relu'))\n",
    "cnn_model.add(Dropout(0.1))\n",
    "\n",
    "cnn_model.add(Dense(100, activation='relu'))\n",
    "cnn_model.add(Dropout(0.1))\n",
    "cnn_model.add(Dense(100, activation='relu'))\n",
    "\n",
    "cnn_model.add(Dense(4, activation='softmax'))\n",
    "# configure the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(np.array(train_img) / 255.0, train_df_with_img['account_type_multi'], test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "161/161 [==============================] - 11s 64ms/step - loss: 1.6779 - accuracy: 0.4564 - val_loss: 1.8703 - val_accuracy: 0.2608\n",
      "Epoch 2/20\n",
      "161/161 [==============================] - 10s 60ms/step - loss: 1.4668 - accuracy: 0.5038 - val_loss: 1.7900 - val_accuracy: 0.3484\n",
      "Epoch 3/20\n",
      "161/161 [==============================] - 10s 65ms/step - loss: 1.3539 - accuracy: 0.5159 - val_loss: 1.4378 - val_accuracy: 0.4473\n",
      "Epoch 4/20\n",
      "161/161 [==============================] - 10s 61ms/step - loss: 1.2733 - accuracy: 0.5289 - val_loss: 1.2847 - val_accuracy: 0.5004\n",
      "Epoch 5/20\n",
      "161/161 [==============================] - 10s 64ms/step - loss: 1.2265 - accuracy: 0.5300 - val_loss: 1.2691 - val_accuracy: 0.4869\n",
      "Epoch 6/20\n",
      "161/161 [==============================] - 13s 81ms/step - loss: 1.1950 - accuracy: 0.5416 - val_loss: 1.2429 - val_accuracy: 0.5018\n",
      "Epoch 7/20\n",
      "161/161 [==============================] - 13s 83ms/step - loss: 1.1627 - accuracy: 0.5492 - val_loss: 1.2691 - val_accuracy: 0.5039\n",
      "Epoch 8/20\n",
      "161/161 [==============================] - 13s 83ms/step - loss: 1.1431 - accuracy: 0.5513 - val_loss: 1.2322 - val_accuracy: 0.4954\n",
      "Epoch 9/20\n",
      "161/161 [==============================] - 13s 80ms/step - loss: 1.1254 - accuracy: 0.5583 - val_loss: 1.3319 - val_accuracy: 0.4382\n",
      "Epoch 10/20\n",
      "161/161 [==============================] - 11s 67ms/step - loss: 1.0953 - accuracy: 0.5707 - val_loss: 1.3938 - val_accuracy: 0.4014\n",
      "Epoch 11/20\n",
      "161/161 [==============================] - 11s 70ms/step - loss: 1.0807 - accuracy: 0.5723 - val_loss: 1.1651 - val_accuracy: 0.5392\n",
      "Epoch 12/20\n",
      "161/161 [==============================] - 11s 68ms/step - loss: 1.0663 - accuracy: 0.5839 - val_loss: 1.2266 - val_accuracy: 0.5081\n",
      "Epoch 13/20\n",
      "161/161 [==============================] - 11s 66ms/step - loss: 1.0606 - accuracy: 0.5929 - val_loss: 1.2149 - val_accuracy: 0.5011\n",
      "Epoch 14/20\n",
      "161/161 [==============================] - 11s 68ms/step - loss: 1.0464 - accuracy: 0.6019 - val_loss: 1.2703 - val_accuracy: 0.4827\n",
      "Epoch 15/20\n",
      "161/161 [==============================] - 11s 71ms/step - loss: 1.0376 - accuracy: 0.6055 - val_loss: 1.1942 - val_accuracy: 0.5102\n",
      "Epoch 16/20\n",
      "161/161 [==============================] - 11s 67ms/step - loss: 1.0121 - accuracy: 0.6227 - val_loss: 1.1600 - val_accuracy: 0.5428\n",
      "Epoch 17/20\n",
      "161/161 [==============================] - 11s 69ms/step - loss: 1.0016 - accuracy: 0.6283 - val_loss: 1.2412 - val_accuracy: 0.5032\n",
      "Epoch 18/20\n",
      "161/161 [==============================] - 11s 66ms/step - loss: 0.9880 - accuracy: 0.6314 - val_loss: 1.2240 - val_accuracy: 0.5279\n",
      "Epoch 19/20\n",
      "161/161 [==============================] - 10s 63ms/step - loss: 0.9712 - accuracy: 0.6408 - val_loss: 1.2541 - val_accuracy: 0.5314\n",
      "Epoch 20/20\n",
      "161/161 [==============================] - 10s 63ms/step - loss: 0.9435 - accuracy: 0.6614 - val_loss: 1.2928 - val_accuracy: 0.5011\n"
     ]
    }
   ],
   "source": [
    "history = cnn_model.fit(x_train, y_train, epochs=20, batch_size=50, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train loss: 1.0152924060821533 / Train accuracy: 0.6313163042068481\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model.evaluate(x_train, y_train, verbose=0)\n",
    "print(f'Train loss: {score[0]} / Train accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test loss: 1.283149242401123 / Test accuracy: 0.5354993939399719\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model.evaluate(np.array(test_img) / 255.0, test_df_with_img['account_type_multi'], verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "52/52 [==============================] - 1s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.36973   0.46845   0.41328       412\n",
      "           1    0.44712   0.52391   0.48248       460\n",
      "           2    0.95045   0.77007   0.85081       274\n",
      "           3    0.64644   0.47481   0.54749       516\n",
      "\n",
      "    accuracy                        0.53550      1662\n",
      "   macro avg    0.60344   0.55931   0.57351      1662\n",
      "weighted avg    0.57280   0.53550   0.54623      1662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = cnn_model.predict(np.array(test_img) / 255.0)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(classification_report(test_df_with_img['account_type_multi'], pred, digits=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "77238a471535228e8cd55a3ca9e771a69c6c0bc66c44a56c972f9554a4042742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}