{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN + CNN + LSTM\n",
    "train a neural network that combines categorical/numerical attributes with images and text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, Dense, Dropout, BatchNormalization, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/twitter_data_train_multiclass.csv')\n",
    "test_df = pd.read_csv('data/twitter_data_test_multiclass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler() \n",
    "train_df[['statuses_count', 'favourites_count', 'followers_count', 'friends_count', 'number_of_mentions', 'listed_count', 'number_of_tags']] = scaler.fit_transform(train_df[['statuses_count', 'favourites_count', 'followers_count', 'friends_count', 'number_of_mentions', 'listed_count', 'number_of_tags']])\n",
    "test_df[['statuses_count', 'favourites_count', 'followers_count', 'friends_count', 'number_of_mentions', 'listed_count', 'number_of_tags']] = scaler.transform(test_df[['statuses_count', 'favourites_count', 'followers_count', 'friends_count', 'number_of_mentions', 'listed_count', 'number_of_tags']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_base = \"../new batch profile pics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all image pathnames from base\n",
    "# list to store files\n",
    "res = []\n",
    "res2 = []\n",
    "\n",
    "# Iterate directory\n",
    "for path in os.listdir(faces_base):\n",
    "    # check if current path is a file\n",
    "    if os.path.isfile(os.path.join(faces_base, path)):\n",
    "        res.append(faces_base + path)\n",
    "        res2.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_dict2 = {}\n",
    "test_img_dict2 = {}\n",
    "train_img2 = []\n",
    "test_img2 = []\n",
    "for i in range(len(res)):\n",
    "    pic = res[i]\n",
    "    id_name = re.match(r\"[^\\/\\\\]+(?=\\.png|\\.jpg)\", res2[i]).group(0)\n",
    "    try:\n",
    "        img = cv2.imread(pic)\n",
    "        if img is None:\n",
    "            print(\"none\")\n",
    "            continue\n",
    "        img = cv2.resize(img, (75, 75))\n",
    "        if int(id_name) in list(train_df['id']):\n",
    "            train_img_dict2[int(id_name)] = img \n",
    "            train_img2.append(img)\n",
    "        elif int(id_name) in list(test_df['id']):\n",
    "            test_img_dict2[int(id_name)] = img\n",
    "            test_img2.append(img)\n",
    "        #img.close()\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\radellng\\AppData\\Local\\Temp\\ipykernel_18628\\2202239605.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_LSTM['description_processed'] = train_LSTM['description_processed'].apply(str)\n",
      "C:\\Users\\radellng\\AppData\\Local\\Temp\\ipykernel_18628\\2202239605.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_LSTM['description_processed'] = test_LSTM['description_processed'].apply(str)\n"
     ]
    }
   ],
   "source": [
    "train_LSTM = train_df[['description_processed', 'account_type_multi']]\n",
    "test_LSTM = test_df[['description_processed','account_type_multi']]\n",
    "\n",
    "train_LSTM['description_processed'] = train_LSTM['description_processed'].apply(str)\n",
    "test_LSTM['description_processed'] = test_LSTM['description_processed'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_df2 = pd.DataFrame(train_img_dict2.items(), columns = ['id', 'img'])  \n",
    "test_img_df2 = pd.DataFrame(test_img_dict2.items(), columns = ['id', 'img'])  \n",
    "train_df_with_img2 = pd.merge(train_img_df2, train_df, on='id')\n",
    "test_df_with_img2 = pd.merge(test_img_df2, test_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (9446, 250)\n",
      "Shape of data tensor: (1667, 250)\n"
     ]
    }
   ],
   "source": [
    "LSTM_X = tokenizer.texts_to_sequences(train_LSTM['description_processed'].values)\n",
    "LSTM_X_train = pad_sequences(LSTM_X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', LSTM_X_train.shape)\n",
    "\n",
    "LSTM_x = tokenizer_test.texts_to_sequences(test_LSTM['description_processed'].values)\n",
    "LSTM_X_test = pad_sequences(LSTM_x, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', LSTM_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['Unnamed: 0', 'screen_name', 'url', 'profile_image_url', 'description',\n",
    "           'id', 'name', 'account_type', 'tweets_list', 'tweets_list_processed',\n",
    "          'description_processed', 'protected', 'verified', 'account_type_multi', \n",
    "          'profile_use_background_image', 'profile_background_tile']\n",
    "\n",
    "x_train, y_train = train_df.drop(to_drop, axis=1), train_df['account_type_multi']\n",
    "x_test, y_test = test_df.drop(to_drop, axis=1), test_df['account_type_multi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_df_with_img2.drop(to_drop, axis=1), train_df_with_img2['account_type_multi']\n",
    "x_test, y_test = test_df_with_img2.drop(to_drop, axis=1), test_df_with_img2['account_type_multi']\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_img, x_train_attr = np.stack(x_train['img']) / 255.0, x_train.drop('img', axis=1)\n",
    "x_val_img, x_val_attr = np.stack(x_val['img']) / 255.0, x_val.drop('img', axis=1)\n",
    "x_test_img, x_test_attr = np.stack(x_test['img']) / 255.0, x_test.drop('img', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16349 unique tokens.\n",
      "Found 4366 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(train_LSTM['description_processed'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "tokenizer_test = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer_test.fit_on_texts(test_LSTM['description_processed'].values)\n",
    "word_index_test = tokenizer_test.word_index\n",
    "print('Found %s unique tokens.' % len(word_index_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ann():\n",
    "    ann_model = Sequential()\n",
    "    ann_model.add(Dense(64, activation = 'relu', input_dim = 222))\n",
    "    ann_model.add(Dropout(.1))\n",
    "    ann_model.add(Dense(128, activation='relu'))\n",
    "    return ann_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():    \n",
    "    base_model = InceptionV3(input_shape = (75, 75, 3), include_top = False, weights = 'imagenet')\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    x = Flatten()(base_model.output)\n",
    "    x = Dense(70, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(50, activation='softmax')(x)\n",
    "    cnn_model = Model(base_model.input, x)\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm():    \n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Embedding(50000, 128, input_length=250))\n",
    "    lstm_model.add(SpatialDropout1D(0.7))\n",
    "    lstm_model.add(LSTM(64, dropout=0.7, recurrent_dropout=0.7))\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = create_ann()\n",
    "cnn_model = create_cnn()\n",
    "combined_input = concatenate([ann_model.output, cnn_model.output])\n",
    "x = Dense(50, activation=\"relu\")(combined_input)\n",
    "x = Dense(6, activation=\"softmax\")(x)\n",
    "combined_model = Model(inputs=[ann_model.input, cnn_model.input], outputs=x)\n",
    "combined_model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "combined_model.fit(\n",
    "\tx=[x_train_attr, x_train_img], y=y_train,\n",
    "\tvalidation_data=([x_val_attr, x_val_img], y_val),\n",
    "\tepochs=20, batch_size=50)\n",
    "time_taken = time.time() - start_time\n",
    "print(\"Total time taken for the program execution\", time_taken) # seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = combined_model.evaluate([x_train_attr, x_train_img], y_train, verbose=0)\n",
    "print(f'Train loss: {score[0]} / Train accuracy: {score[1]}')\n",
    "score = combined_model.evaluate([x_test_attr, x_test_img], y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = combined_model.predict([x_test_attr, x_test_img])\n",
    "pred = np.argmax(pred,axis=1)\n",
    "print(classification_report(y_test, pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM on Account's description Text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
