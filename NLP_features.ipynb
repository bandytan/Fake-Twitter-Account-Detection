{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4709403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\radellng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\radellng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\radellng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\radellng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aba8a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/combined_twitter_data_with_tweets_corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b7bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_followers_following_ratio(df):\n",
    "    #followers divide by following (high means popular, low means more following)\n",
    "    df['following_to_followers_ratio'] = df['friends_count'] / df['followers_count']\n",
    "    return df\n",
    "\n",
    "def has_url_feature(df):\n",
    "    #1 if has url, 0 if no url\n",
    "    df['has_url'] = df['url'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "    return df\n",
    "\n",
    "#2685 has url, 8427 has no url (is na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f3b4fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'name', 'screen_name',\n",
       "       'statuses_count', 'followers_count', 'friends_count',\n",
       "       'favourites_count', 'listed_count', 'url', 'lang', 'time_zone',\n",
       "       'location', 'default_profile', 'default_profile_image', 'geo_enabled',\n",
       "       'profile_image_url', 'profile_banner_url',\n",
       "       'profile_use_background_image', 'profile_background_image_url_https',\n",
       "       'profile_text_color', 'profile_image_url_https',\n",
       "       'profile_sidebar_border_color', 'profile_background_tile',\n",
       "       'profile_sidebar_fill_color', 'profile_background_image_url',\n",
       "       'profile_background_color', 'profile_link_color', 'utc_offset',\n",
       "       'protected', 'verified', 'description', 'created_at', 'updated',\n",
       "       'account_type', 'tweets_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1aeaacae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_features(df):\n",
    "    #get length of username and screen name\n",
    "    df['username_length'] = df['name'].apply(lambda x: len(str(x)))\n",
    "    df['screen_name_length'] = df['screen_name'].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    #anything that is not a-z or 0-9 will be blocked, outputs length\n",
    "    df['username_spec_char_count'] = df['name'].apply(lambda x: len(re.findall(r'[^A-Za-z0-9]+', str(x))))\n",
    "    df['screen_name_spec_char_count'] = df['screen_name'].apply(lambda x: len(re.findall(r'[^A-Za-z0-9]+', str(x))))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04b03a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>...</th>\n",
       "      <th>verified</th>\n",
       "      <th>description</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated</th>\n",
       "      <th>account_type</th>\n",
       "      <th>tweets_list</th>\n",
       "      <th>username_length</th>\n",
       "      <th>screen_name_length</th>\n",
       "      <th>username_spec_char_count</th>\n",
       "      <th>screen_name_spec_char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22903</td>\n",
       "      <td>effeffe</td>\n",
       "      <td>effeffe</td>\n",
       "      <td>164</td>\n",
       "      <td>132</td>\n",
       "      <td>194</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L'uomo ha creato dio a sua immagine e somiglia...</td>\n",
       "      <td>Sun Nov 26 15:19:32 +0000 2006</td>\n",
       "      <td>14/2/2015 11:32</td>\n",
       "      <td>real</td>\n",
       "      <td>['@TheFakeProject cerca followers reali!!! #Im...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>286543</td>\n",
       "      <td>Alessio Bragadini</td>\n",
       "      <td>abragad</td>\n",
       "      <td>6892</td>\n",
       "      <td>930</td>\n",
       "      <td>535</td>\n",
       "      <td>478</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web and social media developer from Italy</td>\n",
       "      <td>Wed Dec 27 14:55:17 +0000 2006</td>\n",
       "      <td>14/2/2015 11:32</td>\n",
       "      <td>real</td>\n",
       "      <td>[\"RT @hotdogsladies: The real problem with ema...</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>438023</td>\n",
       "      <td>fullcaffeine</td>\n",
       "      <td>fullcaffeine</td>\n",
       "      <td>2885</td>\n",
       "      <td>173</td>\n",
       "      <td>444</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tue Jan 02 09:01:50 +0000 2007</td>\n",
       "      <td>14/2/2015 11:32</td>\n",
       "      <td>real</td>\n",
       "      <td>['Amare il #giornalismo : @Internazionale @la_...</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>586003</td>\n",
       "      <td>Maurizio Tesconi</td>\n",
       "      <td>myself2048</td>\n",
       "      <td>216</td>\n",
       "      <td>97</td>\n",
       "      <td>234</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CNR Researcher. Web developer. Social media ge...</td>\n",
       "      <td>Fri Jan 05 16:20:42 +0000 2007</td>\n",
       "      <td>14/2/2015 11:32</td>\n",
       "      <td>real</td>\n",
       "      <td>['RT @TheFakeProject: Dear followers, phase 2 ...</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>628563</td>\n",
       "      <td>Massimo Moretti</td>\n",
       "      <td>MaxMoretti</td>\n",
       "      <td>505</td>\n",
       "      <td>154</td>\n",
       "      <td>314</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The truth is out there!</td>\n",
       "      <td>Fri Jan 12 11:06:17 +0000 2007</td>\n",
       "      <td>14/2/2015 11:32</td>\n",
       "      <td>real</td>\n",
       "      <td>['@Swype  when the Italian dictionary will be ...</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11107</th>\n",
       "      <td>11107</td>\n",
       "      <td>16273</td>\n",
       "      <td>385095166</td>\n",
       "      <td>Hull Util Jobs</td>\n",
       "      <td>tmj_UKH_UTIL</td>\n",
       "      <td>2</td>\n",
       "      <td>332</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Follow this account for geo-targeted Utilities...</td>\n",
       "      <td>Tue Oct 04 21:25:35 +0000 2011</td>\n",
       "      <td>2016-03-15 13:49:14</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11108</th>\n",
       "      <td>11108</td>\n",
       "      <td>16274</td>\n",
       "      <td>385096188</td>\n",
       "      <td>Glasgow Util Jobs</td>\n",
       "      <td>tmj_UKG_UTIL</td>\n",
       "      <td>1</td>\n",
       "      <td>327</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Follow this account for geo-targeted Utilities...</td>\n",
       "      <td>Tue Oct 04 21:28:08 +0000 2011</td>\n",
       "      <td>2016-03-15 13:49:14</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11109</th>\n",
       "      <td>11109</td>\n",
       "      <td>16275</td>\n",
       "      <td>398274058</td>\n",
       "      <td>Sandwich Uk Mgmt</td>\n",
       "      <td>tmj_SND_mgmt</td>\n",
       "      <td>2</td>\n",
       "      <td>276</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Follow this account for geo-targeted Business/...</td>\n",
       "      <td>Tue Oct 25 20:41:55 +0000 2011</td>\n",
       "      <td>2016-03-15 13:49:14</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11110</th>\n",
       "      <td>11110</td>\n",
       "      <td>16292</td>\n",
       "      <td>631519728</td>\n",
       "      <td>PR Business/Mgmt</td>\n",
       "      <td>tmj_ptr_mgmt</td>\n",
       "      <td>36</td>\n",
       "      <td>527</td>\n",
       "      <td>278</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Follow this account for geo-targeted Business/...</td>\n",
       "      <td>Mon Jul 09 21:35:55 +0000 2012</td>\n",
       "      <td>2016-03-15 13:49:15</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111</th>\n",
       "      <td>11111</td>\n",
       "      <td>16314</td>\n",
       "      <td>846108068</td>\n",
       "      <td>NJ Business/Mgmt</td>\n",
       "      <td>tmj_nj_mgmt</td>\n",
       "      <td>218</td>\n",
       "      <td>332</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Follow this account for geo-targeted Business/...</td>\n",
       "      <td>Tue Sep 25 19:35:53 +0000 2012</td>\n",
       "      <td>2016-03-15 13:49:15</td>\n",
       "      <td>fake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11112 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1         id               name   screen_name  \\\n",
       "0               0             1      22903            effeffe       effeffe   \n",
       "1               1             3     286543  Alessio Bragadini       abragad   \n",
       "2               2             4     438023       fullcaffeine  fullcaffeine   \n",
       "3               3             5     586003   Maurizio Tesconi    myself2048   \n",
       "4               4             6     628563    Massimo Moretti    MaxMoretti   \n",
       "...           ...           ...        ...                ...           ...   \n",
       "11107       11107         16273  385095166     Hull Util Jobs  tmj_UKH_UTIL   \n",
       "11108       11108         16274  385096188  Glasgow Util Jobs  tmj_UKG_UTIL   \n",
       "11109       11109         16275  398274058   Sandwich Uk Mgmt  tmj_SND_mgmt   \n",
       "11110       11110         16292  631519728   PR Business/Mgmt  tmj_ptr_mgmt   \n",
       "11111       11111         16314  846108068   NJ Business/Mgmt   tmj_nj_mgmt   \n",
       "\n",
       "       statuses_count  followers_count  friends_count  favourites_count  \\\n",
       "0                 164              132            194                12   \n",
       "1                6892              930            535               478   \n",
       "2                2885              173            444                41   \n",
       "3                 216               97            234                 2   \n",
       "4                 505              154            314                 0   \n",
       "...               ...              ...            ...               ...   \n",
       "11107               2              332            310                 0   \n",
       "11108               1              327            308                 0   \n",
       "11109               2              276            265                 0   \n",
       "11110              36              527            278                 0   \n",
       "11111             218              332            245                 0   \n",
       "\n",
       "       listed_count  ... verified  \\\n",
       "0                 4  ...      NaN   \n",
       "1                28  ...      NaN   \n",
       "2                 2  ...      NaN   \n",
       "3                 0  ...      NaN   \n",
       "4                 3  ...      NaN   \n",
       "...             ...  ...      ...   \n",
       "11107             0  ...      NaN   \n",
       "11108             4  ...      NaN   \n",
       "11109             1  ...      NaN   \n",
       "11110            21  ...      NaN   \n",
       "11111            31  ...      NaN   \n",
       "\n",
       "                                             description  \\\n",
       "0      L'uomo ha creato dio a sua immagine e somiglia...   \n",
       "1              Web and social media developer from Italy   \n",
       "2                                                    NaN   \n",
       "3      CNR Researcher. Web developer. Social media ge...   \n",
       "4                                The truth is out there!   \n",
       "...                                                  ...   \n",
       "11107  Follow this account for geo-targeted Utilities...   \n",
       "11108  Follow this account for geo-targeted Utilities...   \n",
       "11109  Follow this account for geo-targeted Business/...   \n",
       "11110  Follow this account for geo-targeted Business/...   \n",
       "11111  Follow this account for geo-targeted Business/...   \n",
       "\n",
       "                           created_at              updated  account_type  \\\n",
       "0      Sun Nov 26 15:19:32 +0000 2006      14/2/2015 11:32          real   \n",
       "1      Wed Dec 27 14:55:17 +0000 2006      14/2/2015 11:32          real   \n",
       "2      Tue Jan 02 09:01:50 +0000 2007      14/2/2015 11:32          real   \n",
       "3      Fri Jan 05 16:20:42 +0000 2007      14/2/2015 11:32          real   \n",
       "4      Fri Jan 12 11:06:17 +0000 2007      14/2/2015 11:32          real   \n",
       "...                               ...                  ...           ...   \n",
       "11107  Tue Oct 04 21:25:35 +0000 2011  2016-03-15 13:49:14          fake   \n",
       "11108  Tue Oct 04 21:28:08 +0000 2011  2016-03-15 13:49:14          fake   \n",
       "11109  Tue Oct 25 20:41:55 +0000 2011  2016-03-15 13:49:14          fake   \n",
       "11110  Mon Jul 09 21:35:55 +0000 2012  2016-03-15 13:49:15          fake   \n",
       "11111  Tue Sep 25 19:35:53 +0000 2012  2016-03-15 13:49:15          fake   \n",
       "\n",
       "                                             tweets_list  username_length  \\\n",
       "0      ['@TheFakeProject cerca followers reali!!! #Im...                7   \n",
       "1      [\"RT @hotdogsladies: The real problem with ema...               17   \n",
       "2      ['Amare il #giornalismo : @Internazionale @la_...               12   \n",
       "3      ['RT @TheFakeProject: Dear followers, phase 2 ...               16   \n",
       "4      ['@Swype  when the Italian dictionary will be ...               15   \n",
       "...                                                  ...              ...   \n",
       "11107                                                NaN               14   \n",
       "11108                                                NaN               17   \n",
       "11109                                                NaN               16   \n",
       "11110                                                NaN               16   \n",
       "11111                                                NaN               16   \n",
       "\n",
       "      screen_name_length username_spec_char_count  screen_name_spec_char_count  \n",
       "0                      7                        0                            0  \n",
       "1                      7                        1                            0  \n",
       "2                     12                        0                            0  \n",
       "3                     10                        1                            0  \n",
       "4                     10                        1                            0  \n",
       "...                  ...                      ...                          ...  \n",
       "11107                 12                        2                            2  \n",
       "11108                 12                        2                            2  \n",
       "11109                 12                        2                            2  \n",
       "11110                 12                        2                            2  \n",
       "11111                 11                        2                            2  \n",
       "\n",
       "[11112 rows x 41 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d1250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_texts(df):\n",
    "    def process_tweets_list(corpus):\n",
    "        \n",
    "        corpus_processed = []\n",
    "        for tweet_list in corpus:\n",
    "            tweet_list = str(tweet_list)\n",
    "            row_processed = \"\"\n",
    "            \n",
    "            #replace RT and @\n",
    "            row_processed = tweet_list.replace(\"RT\", \"\" ) \n",
    "            row_processed = row_processed.replace(\"@\", \"\" )\n",
    "            \n",
    "            row_processed = re.sub(r'http\\S+', \"\", row_processed) #remove any URLs in tweets\n",
    "            row_processed = re.sub(r'[^\\x00-\\x7f]', \"\", row_processed) #remove Non-ASCII characters\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed if not row_processed == 'nan' else \"\") # handle NA\n",
    "            \n",
    "\n",
    "        return corpus_processed\n",
    "    \n",
    "    def process_description(corpus):\n",
    "        \n",
    "        corpus_processed = []\n",
    "        for row in corpus:\n",
    "            row = str(row)\n",
    "            row_processed = re.sub(r'[^\\x00-\\x7f]', \"\", row) #remove Non-ASCII characters\n",
    "            row_processed = re.sub(r'[^\\w\\s]', '', row_processed) # remove punctuation\n",
    "            corpus_processed.append(row_processed if not row_processed == 'nan' else \"\") # handle NA\n",
    "            \n",
    "        return corpus_processed\n",
    "    \n",
    "    df[\"tweets_list_processed\"] = process_tweets_list(df[\"tweets_list\"])\n",
    "    df[\"description_processed\"] = process_description(df[\"description\"])\n",
    "    \n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "162c945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmatizeTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, text):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31f4a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nlp_features(df):\n",
    "    \n",
    "    #tweets\n",
    "    vect_tweets = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "    \n",
    "    tweets_processed = pd.Series(df[\"tweets_list_processed\"])\n",
    "    tfidf_fit_tweets = vect_tweets.fit(tweets_processed)\n",
    "    tweets_tfidf_array = tfidf_fit_tweets.transform(tweets_processed).toarray()\n",
    "    tweets_tfidf_df = pd.DataFrame(tweets_tfidf_array)\n",
    "    tweets_tfidf_df.columns = list(map(lambda x: \"tweets_\" + str(x), tweets_tfidf_df.columns))\n",
    "    df = pd.merge(df, tweets_tfidf_df, left_index=True, right_index=True)\n",
    "    \n",
    "    #description\n",
    "    vect_description = TfidfVectorizer( \n",
    "        tokenizer=LemmatizeTokenizer(),\n",
    "        lowercase=True,\n",
    "        analyzer='word', \n",
    "        ngram_range=(1,3), # unigram, bigram and trigram \n",
    "        max_features=100, # vocabulary that only consider the top max_features ordered by term frequency across the corpus\n",
    "        min_df=10, # minimum word frequency required to be in model\n",
    "        stop_words=stopwords.words('english') # remove stopwords\n",
    "        )\n",
    "    \n",
    "    description_processed = pd.Series(df[\"description_processed\"])\n",
    "    tfidf_fit_description = vect_description.fit(description_processed)\n",
    "    description_tfidf_array = tfidf_fit_description.transform(description_processed).toarray()\n",
    "    description_tfidf_df = pd.DataFrame(description_tfidf_array)\n",
    "    description_tfidf_df.columns = list(map(lambda x: \"description_\" + str(x), description_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_tfidf_df, left_index=True, right_index=True)\n",
    "    \n",
    "    return (df, tfidf_fit_tweets, tfidf_fit_description)\n",
    "\n",
    "def nlp_transform_test(df, tfidf_fit_tweets, tfidf_fit_description):\n",
    "    tweets_tfidf_array = tfidf_fit_tweets.transform(df['tweets_list_processed']).toarray()\n",
    "    tweets_tfidf_df = pd.DataFrame(tweets_tfidf_array)\n",
    "    tweets_tfidf_df.columns = list(map(lambda x : \"tweets_\" + str(x), tweets_tfidf_df.columns))\n",
    "    df = pd.merge(df, tweets_tfidf_df , left_index=True, right_index=True)\n",
    "    \n",
    "    description_tfidf_array = tfidf_fit_description.transform(df['description_processed']).toarray()\n",
    "    description_tfidf_df = pd.DataFrame(description_tfidf_array)\n",
    "    description_tfidf_df.columns = list(map(lambda x : \"description_\" + str(x), description_tfidf_df.columns))\n",
    "    df = pd.merge(df, description_tfidf_df , left_index=True, right_index=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95f62c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = clean_texts(df)\n",
    "result, tfidf_fit_tweets, tfidf_fit_description = generate_nlp_features(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bf54431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=100, min_df=10, ngram_range=(1, 3),\n",
      "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
      "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
      "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
      "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
      "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
      "                            'itself', ...],\n",
      "                tokenizer=<__main__.LemmatizeTokenizer object at 0x0000019129E2F880>)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_fit_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "525ff32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nlp_transform_test(result, tfidf_fit_tweets, tfidf_fit_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "927f8ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tweets_61_x'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.columns[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be37fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
